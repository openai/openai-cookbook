


Research











CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewCustomer storiesSafety standardsPricingSafetyCompanyAboutCareersBlogCharterSearchMenu Mobile Navigation CloseSite NavigationResearchProductSafetyCompanySearch Submit Pioneering research on the path to AGIWe believe our research will eventually lead to artificial general intelligence, a system that can solve human-level problems. Building safe and beneficial AGI is our mission.Quick linksView research indexLearn about safetySafely aligning powerful AI systems is one of the most important unsolved problems for our mission. Techniques like learning from human feedback are helping us get closer, and we are actively researching new techniques to help us fill the gaps.Josh AchiamResearcher at OpenAIFocus areasWe build our generative models using a technology called deep learning, which leverages large amounts of data to train an AI system to perform a task.TextOur text models are advanced language processing tools that can generate, classify, and summarize text with high levels of coherence and accuracy.Aligning language models to follow instructionsWeâve trained language models that are much better at following user intentions than GPT-3.Summarizing books with human feedbackWe've trained a model to summarize entire books with human feedback.Language models are few-shot learnersWe trained GPT-3, an autoregressive language model with 175 billion parameters.ImageOur research on generative modeling for images has led to representation models like CLIP, which makes a map between text and images that an AI can read, and DALL-E, a tool for creating vivid images from text descriptions.Hierarchical text-conditional image generation with CLIP latentsWe show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity.DALLÂ·E: Creating images from textWeâve trained a neural network called DALLÂ·E that creates images from text captions for a wide range of concepts expressible in natural language.CLIP: Connecting text and imagesWeâre introducing a neural network called CLIP which efficiently learns visual concepts from natural language supervision.AudioOur research on applying AI to audio processing and audio generation has led to developments in automatic speech recognition and original musical compositions.Introducing WhisperWeâve trained and are open-sourcing a neural net that approaches human level robustness and accuracy on English speech recognition.JukeboxWeâre introducing Jukebox, a neural net that generates music as raw audio in a variety of genres and artist styles.MuseNetWeâve created MuseNet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments.Past highlightsOur current AI research builds upon a wealth of previous projects and advances.View all researchImage GPTJun 17, 2020June 17, 2020Solving Rubikâs Cube with a robot handOct 15, 2019October 15, 2019Emergent tool use from multi-agent interactionSep 17, 2019September 17, 2019Featured rolesWe are constantly seeking talented individuals to join our team. Explore featured roles or view all open roles.View all careersResearch EngineerSan Francisco, California, United States â All teams (roles across multiple teams)Apply nowResearch ScientistSan Francisco, California, United States â All teams (roles across multiple teams)Apply nowResearchOverviewIndexProductOverviewCustomer storiesSafety standardsPricingSafetyOverviewCompanyAboutCareersBlogCharterOpenAI Â© 2015âââ2023Terms & policiesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
