{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538f25ce",
   "metadata": {},
   "source": [
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ OpenAIì˜  **gpt-oss (openâ€‘weight)** ëª¨ë¸ì„ **í•œêµ­ ë‰´ìŠ¤ ë¬¸ì²´ + ìµœì‹  ëŒ€í™”ì²´**ë¡œ ì„¸ë°€ íŠœë‹í•˜ëŠ” ë°©ë²•ì„\n",
    "í•œêµ­ì–´/ì˜ì–´ **ì´ì¤‘ ì–¸ì–´**ë¡œ ì œê³µí•©ë‹ˆë‹¤.  \n",
    "This notebook shows how to fineâ€‘tune OpenAI's **gpt-oss (openâ€‘weight)** models for **Korean news style + modern chat tone**, in **Korean & English**.\n",
    "\n",
    "---\n",
    "\n",
    "### MXFP4 workflow clarifications Â· MXFP4 ì›Œí¬í”Œë¡œ ì •ë¦¬\n",
    "\n",
    "**EN:**  \n",
    "- Training or fine-tuning **directly in MXFP4 is not supported** by public frameworks today.  \n",
    "- Recommended path: train in **BF16** (or **QLoRA 4â€‘bit nf4**) â†’ **merge LoRA** â†’ **postâ€‘training quantize to MXFP4** â†’ `save_pretrained()` for deployment.  \n",
    "- If you need an MXFP4 artifact, you must **reâ€‘quantize from BF16** after merging adapters. (Export utilities are evolving; if your toolchain already supports MXFP4 serialization, thatâ€™s ideal.)\n",
    "\n",
    "**KR:**  \n",
    "- í˜„ì¬ ê³µê°œ í”„ë ˆì„ì›Œí¬ì—ì„œëŠ” **MXFP4ë¡œ ì§ì ‘ í•™ìŠµ/íŒŒì¸íŠœë‹**ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
    "- ê¶Œì¥ ê²½ë¡œ: **BF16**(ë˜ëŠ” **QLoRA 4â€‘bit nf4**)ë¡œ í•™ìŠµ â†’ **LoRA ë³‘í•©** â†’ **ì‚¬í›„(MXFP4) ì–‘ìí™”** â†’ ë°°í¬ìš©ìœ¼ë¡œ `save_pretrained()` ì €ì¥.  \n",
    "- MXFP4 ì•„í‹°íŒ©íŠ¸ê°€ í•„ìš”í•˜ë©´, ì–´ëŒ‘í„° ë³‘í•© í›„ **BF16 â†’ MXFP4 ì¬ì–‘ìí™”**ê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì§ë ¬í™” ìœ í‹¸ì€ ì§„í™” ì¤‘ì´ë©°, íˆ´ì²´ì¸ì—ì„œ MXFP4 ì €ì¥ì„ ì§€ì›í•˜ë©´ ê°€ì¥ ì¢‹ìŠµë‹ˆë‹¤.)\n",
    "\n",
    "---\n",
    "\n",
    "### LoRA targets (MoE) Â· LoRA íƒ€ê¹ƒ(MoE í¬í•¨)\n",
    "\n",
    "**EN:**  \n",
    "- Minimal config (fast, low VRAM): target attention only, e.g. `[\"q_proj\",\"v_proj\"]`.  \n",
    "- MoEâ€‘aware config (better domain adaptation, more VRAM/time): include **expert projection layers** in addition to attention.  \n",
    "\n",
    "```python\n",
    "from peft import LoraConfig\n",
    "\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # baseline\n",
    "MOE_TARGET_PARAMETERS = [\n",
    "    # example expert layers; adjust indices to your model depth\n",
    "    \"mlp.experts.gate_up_proj\",\n",
    "    \"mlp.experts.down_proj\",\n",
    "]\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",              # cover all linear layers\n",
    "    target_parameters=MOE_TARGET_PARAMETERS,  # add expert projections\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "```\n",
    "\n",
    "- Start with attentionâ€‘only; if KR domain fit is insufficient, enable MoE targets and reâ€‘eval.\n",
    "\n",
    "**KR:**  \n",
    "- ìµœì†Œ êµ¬ì„±(ë¹ ë¥´ê³  VRAM ì ˆì•½): `[\"q_proj\",\"v_proj\"]` ë“± **ì–´í…ì…˜ë§Œ** ì ìš©.  \n",
    "- **MoE ì¸ì§€ êµ¬ì„±**(ë„ë©”ì¸ ì í•©ì„±â†‘, ìì› ì†Œëª¨â†‘): ì–´í…ì…˜ì— **ì „ë¬¸ê°€(Expert) íˆ¬ì˜ ë ˆì´ì–´**ë¥¼ ì¶”ê°€ë¡œ í¬í•¨.  \n",
    "- ë¨¼ì € ì–´í…ì…˜ë§Œìœ¼ë¡œ ì‹œë„í•œ ë’¤, í•œêµ­ì–´ ë„ë©”ì¸ ì í•©ì„±ì´ ë¶€ì¡±í•˜ë©´ MoE íƒ€ê¹ƒì„ ì¼œê³  ì¬í‰ê°€í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c12ff",
   "metadata": {},
   "source": [
    "## Contents Â· ëª©ì°¨\n",
    "0) Goals & Scope Â· ëª©í‘œ & ë²”ìœ„  \n",
    "1) Environment check Â· í™˜ê²½ ì ê²€  \n",
    "2) ì„¤ì •ê°’ Â· Config  \n",
    "3) íŒ¨í‚¤ì§€ ì„¤ì¹˜ Â· Install Deps  \n",
    "4) ë°ì´í„° ì†Œì‹±(í•œêµ­í˜•) Â· KRâ€‘Context Data Sourcing  \n",
    "5) ìƒ˜í”Œ ë°ì´í„° ìƒì„± Â· Create Sample Data  \n",
    "6) ì „ì²˜ë¦¬(PIPA) & ìŠ¤íƒ€ì¼ ë¼ë²¨ Â· PII Scrubbing & Style Tags  \n",
    "7) ë°ì´í„° ë¡œë”©/í¬ë§·íŒ… Â· Load & Format  \n",
    "8) ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ Â· Load Model & Tokenizer  \n",
    "9) Fineâ€‘Tuning (LoRA/QLoRA) Â· ì„¸ë°€ íŠœë‹  \n",
    "   9a) Data curation & splits  \n",
    "   9b) Hyperparameters (r/alpha/dropout)  \n",
    "   9c) Merge adapters (BF16)  \n",
    "   9d) Save merged BF16 (`save_pretrained`)  \n",
    "   9e) Export & Quantize (BF16 â†’ MXFP4) Â· ë‚´ë³´ë‚´ê¸° & ì–‘ìí™”  \n",
    "10) í‰ê°€(ë‰´ìŠ¤/ëŒ€í™”) Â· Evaluation (News/Chat)  \n",
    "11) Inference Prompt Templates Â· ì¶”ë¡  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿  \n",
    "12) ìµœì‹ ì„± ìœ ì§€ Â· Freshness Strategy  \n",
    "13) ì•ˆì „/ì»´í”Œë¼ì´ì–¸ìŠ¤ Â· Safety & Compliance  \n",
    "14) ë¬¸ì œí•´ê²° & ë‹¤ìŒ ë‹¨ê³„ Â· Troubleshooting & Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8655d2",
   "metadata": {},
   "source": [
    "### âš™ï¸ Training vs Quantization â€” Whatâ€™s supported\n",
    "- **Do:** Train with BF16/FP16 or QLoRA; export merged weights.\n",
    "- **Then:** Quantize to **MXFP4** for inference using provided conversion scripts/utilities.\n",
    "- **Donâ€™t:** Attempt to run an endâ€‘toâ€‘end â€œtrain in MXFP4â€ pipeline â€” not supported today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24a3d9",
   "metadata": {},
   "source": [
    "> **PII & Compliance Reminder:** For KR data, follow your enterprise policy (mask RRN/phone/account IDs, remove emails) **before** training & logging. Keep train/val/test splits stratified by source and style tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e883f5",
   "metadata": {},
   "source": [
    "### ğŸ§ª MoE adapters (optional)\n",
    "You can target MoE layers with adapters, but treat this as **advanced/experimental**. Start with attention projections first and validate KR benchmarks before expanding scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179543e6",
   "metadata": {},
   "source": [
    "> **Note:** Keep `transformers`, `peft`, `accelerate`, and `trl` at versions known to support BF16/4â€‘bit LoRA.  \n",
    "If you pin `safetensors`, remember that **native MXFP4 serialization is not yet standardized**; loaders may upcast internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e743f0",
   "metadata": {},
   "source": [
    "### ğŸ” Support Matrix â€” At a glance\n",
    "- **Fineâ€‘tuning precision:** BF16/FP16 âœ… Â· QLoRA 4â€‘bit âœ… Â· **MXFP4 FT âŒ**\n",
    "- **Quantization target:** MXFP4 âœ… (postâ€‘training)\n",
    "- **API FT (hosted) for OSS models:** âŒ\n",
    "- **Openâ€‘source FT (Transformers/TRL/PEFT):** âœ…\n",
    "- **LoRA targets:** `q_proj`, `k_proj`, `v_proj`, `o_proj` âœ…; MoE expert adapters **experimental** âš ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dec1f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d489c2",
   "metadata": {},
   "source": [
    "## 0) Goals & Scope Â· ëª©í‘œ & ë²”ìœ„\n",
    "- **KR**: í•œêµ­ì–´ ì¼ë°˜ ë‰´ìŠ¤ + ì¼ìƒ/ìƒë‹´ ëŒ€í™”ì²´ì— ìµœì í™”. `style=news_headline|news_lead|news_body|kakao_casual|kakao_formal` ì œì–´.\n",
    "- **EN**: Optimize for Korean news writing and modern chat tone; control output via style tags above.\n",
    "- **Stack**: `transformers`, `trl(SFTTrainer)`, `peft(LoRA/QLoRA)`, `datasets`.\n",
    "- **Hardware**: Single/few GPUs (BF16 preferred). CPU/Mac for lightweight tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97218d",
   "metadata": {},
   "source": [
    "## 1) Environment check Â· í™˜ê²½ ì ê²€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5babb2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\n",
      "OS/Platform: Linux-6.8.0-60-generic-x86_64-with-glibc2.35\n",
      "CUDA_VISIBLE_DEVICES: \n",
      "Torch: 2.7.1+cu126 CUDA: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OS/Platform:\", platform.platform())\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\"))\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"Torch not installed or GPU not detected:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25688688",
   "metadata": {},
   "source": [
    "## 2) ì„¤ì •ê°’ Â· Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15817f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# === Model & Training Params ===\n",
    "BASE_URL = \"http://localhost:8000/v1\"     # vLLM OpenAI-compatible endpoint\n",
    "API_KEY  = \"dummy-key\"                     # vLLM ignores; SDK requires a value\n",
    "MODEL    = \"openai/gpt-oss-120b\"           # must match the model vLLM loaded\n",
    "OUTPUT_DIR = \"ft-oss-kr-news-chat-bilingual\"\n",
    "\n",
    "# Data mix (news : chat)\n",
    "MIX_NEWS = 0.6\n",
    "MIX_CHAT = 0.4\n",
    "\n",
    "# LoRA\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # adjust per model\n",
    "\n",
    "# Training\n",
    "EPOCHS = 1\n",
    "PER_DEVICE_BS = 2\n",
    "GRAD_ACCUM = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "BF16 = True\n",
    "LOG_STEPS = 20\n",
    "SAVE_STEPS = 200\n",
    "SAVE_TOTAL_LIMIT = 2\n",
    "\n",
    "print(\"Config ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f258eb",
   "metadata": {},
   "source": [
    "## 3) íŒ¨í‚¤ì§€ ì„¤ì¹˜ Â· Install Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b75968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.55.3\n",
      "accelerate: 1.10.0\n",
      "datasets: 4.0.0\n",
      "peft: not installed\n",
      "trl: 0.21.0\n",
      "bitsandbytes: not installed\n",
      "sentencepiece: 0.2.1\n",
      "vllm: 0.10.1\n",
      "llama_cpp: 0.3.16\n",
      "pip: 25.2\n",
      "Install cells are commented. Un-comment in your environment.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install transformers accelerate datasets peft trl bitsandbytes sentencepiece\n",
    "# (optional) serving/runtimes\n",
    "# %pip install vllm\n",
    "# %pip install llama-cpp-python\n",
    "\n",
    "import importlib, pip\n",
    "\n",
    "for dep in [\"transformers\",\"accelerate\",\"datasets\",\"peft\",\"trl\",\n",
    "            \"bitsandbytes\",\"sentencepiece\",\"vllm\",\"llama_cpp\"]:\n",
    "    try:\n",
    "        print(f\"{dep}: {importlib.import_module(dep).__version__}\")\n",
    "    except Exception:\n",
    "        print(f\"{dep}: not installed\")\n",
    "\n",
    "print(f\"pip: {pip.__version__}\")\n",
    "\n",
    "print(\"Install cells are commented. Un-comment in your environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8647fd",
   "metadata": {},
   "source": [
    "## 4) ë°ì´í„° ì†Œì‹±(í•œêµ­í˜•) Â· KRâ€‘Context Data Sourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22cbd6",
   "metadata": {},
   "source": [
    "**KR**  \n",
    "- ê³µê°œ ë²¤ì¹˜ë§ˆí¬(ì£¼ì œ ë¶„ë¥˜/ìš”ì•½/QA) + **í—ˆìš©ëœ ë‰´ìŠ¤ APIì˜ ë©”íƒ€ë°ì´í„°(ì œëª©/ìš”ì•½/ì„¹ì…˜)** ì¤‘ì‹¬ìœ¼ë¡œ ìŠ¤íƒ€ì¼ ë³´ì •.\n",
    "- ê¸°ì‚¬ **ì›ë¬¸ ëŒ€ëŸ‰ ì¬í•™ìŠµì€ ì €ì‘ê¶Œ/ì•½ê´€ ì´ìŠˆ** â†’ ë©”íƒ€ë°ì´í„°Â·ê³µê°œ ì½”í¼ìŠ¤ ìœ„ì£¼.\n",
    "- ëŒ€í™”ì²´ëŠ” í•©ë²• ê³µê°œ ì½”í¼ìŠ¤(ë°˜ë§/ì¡´ëŒ“ë§/ì´ëª¨í‹°ì½˜/ì¶•ì•½ì–´ ë¼ë²¨ í¬í•¨) ìš°ì„ .\n",
    "- PIPA: ì£¼ë¯¼ë²ˆí˜¸/ì—°ë½ì²˜/ì´ë©”ì¼/ê³„ì¢Œ ë“± ê°œì¸ì •ë³´ëŠ” **í›ˆë ¨ ì „/ë¡œê·¸ ì „** ìŠ¤í¬ëŸ¬ë¹™.\n",
    "\n",
    "**EN**  \n",
    "- Prefer public KR benchmarks (topic classification / summarization / QA) and **allowed news API metadata** for style calibration.\n",
    "- Avoid mass training on news full texts due to license/ToS constraints; use metadata + open corpora.\n",
    "- For chat, use lawful open corpora with tone/emoji/informalâ€‘formal annotations.\n",
    "- Scrub PII (phone, RRNs, emails, accounts) before training/logging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b918411",
   "metadata": {},
   "source": [
    "## 5) ìƒ˜í”Œ ë°ì´í„° ìƒì„± Â· Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18db10a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: data/news.jsonl, data/chat.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib\n",
    "pathlib.Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "news_samples = [\n",
    "  {\"style\":\"news_lead\",\"topic\":\"ê²½ì œ\",\"title\":\"ë°˜ë„ì²´ ìˆ˜ì¶œ í˜¸ì¡°â€¦ 7ì›” ìˆ˜ì¶œì•¡ 20% ì¦ê°€\",\"summary\":\"ìˆ˜ì¶œ ê°œì„ ì„¸ê°€ ì´ì–´ì§€ë©° ê²½ê¸° íšŒë³µ ê¸°ëŒ€ê°€ ì»¤ì¡Œë‹¤.\"},\n",
    "  {\"style\":\"news_headline\",\"topic\":\"ì •ì¹˜\",\"title\":\"êµ­íšŒ, ë°ì´í„° ì‚°ì—… ìœ¡ì„±ë²• ë³¸íšŒì˜ í†µê³¼\",\"summary\":\"ë°ì´í„° í™œìš© ì´‰ì§„ê³¼ ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ê°•í™”í•˜ëŠ” ë‚´ìš©.\"},\n",
    "  {\n",
    "    \"style\": \"news_lead\",\n",
    "    \"topic\": \"ê²½ì œ\",\n",
    "    \"title\": \"ì¹´ì¹´ì˜¤í˜ì´ ë³´ì•ˆ ì ê²€â€¦ ê³ ê°ë¬¸ì˜: help+vip@corp.co.kr\",\n",
    "    \"summary\": \"ê³ ê°ì„¼í„° 010-1234-5678ë¡œ ë¬¸ì˜ í­ì£¼. ê³„ì¢Œ 110-123-456789 ê´€ë ¨ ê²°ì œ ì˜¤ë¥˜ ë…¼ë€.\"\n",
    "  },\n",
    "  {\n",
    "    \"style\": \"news_headline\",\n",
    "    \"topic\": \"ì‚¬íšŒ\",\n",
    "    \"title\": \"ê°œì¸ì •ë³´ ìœ ì¶œ ì˜í˜¹â€¦ ì£¼ë¯¼ë²ˆí˜¸ 901010-1234567 ìœ í†µ ì£¼ì¥\",\n",
    "    \"summary\": \"ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123ì—ì„œ ìë£Œ í™•ë³´â€¦ ë‹´ë‹¹ì john.doe+news@example.com\"\n",
    "  }\n",
    "]\n",
    "\n",
    "chat_samples = [\n",
    "  {\"style\":\"kakao_casual\",\"dialog\":[\"ì£¼ë§ì— ë¹„ ì˜¨ëŒ€?\",\"ì‘ ì¼ìš”ì¼ì— ê½¤ ì˜¨ë‹¤ë”ë¼ â˜”\",\"í— ìš°ì‚° ì±™ê²¨ì•¼ê² ë‹¤\"]},\n",
    "  {\"style\":\"kakao_formal\",\"dialog\":[\"ì•ˆë…•í•˜ì„¸ìš”. ë°°ì†¡ ì¼ì • í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\",\"ë‚´ì¼ ì¤‘ ë„ì°© ì˜ˆì •ì…ë‹ˆë‹¤.\",\"ì•ˆë‚´ ê°ì‚¬í•©ë‹ˆë‹¤.\"]},\n",
    "  {\n",
    "    \"style\": \"kakao_formal\",\n",
    "    \"dialog\": [\n",
    "      \"ë°°ì†¡ í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ì£¼ë¬¸ë²ˆí˜¸ ORD-2025-0001 ì…ë‹ˆë‹¤.\",\n",
    "      \"ì—°ë½ì²˜ëŠ” 010-2222-3333 ì…ë‹ˆë‹¤. (ìœ ë‹ˆì½”ë“œ í•˜ì´í”ˆ)\",\n",
    "      \"ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ëŠ” ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "with open(\"data/news.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for ex in news_samples: f.write(json.dumps(ex, ensure_ascii=False)+\"\\n\")\n",
    "with open(\"data/chat.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for ex in chat_samples: f.write(json.dumps(ex, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "print(\"Created: data/news.jsonl, data/chat.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1eaa27",
   "metadata": {},
   "source": [
    "## 6) ì „ì²˜ë¦¬(PIPA) & ìŠ¤íƒ€ì¼ ë¼ë²¨ Â· PII Scrubbing & Style Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430c1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/news.jsonl -> data/news_clean.jsonl | rows: 4, redacted_rows: 2, hits: {'[EMAIL]': 2, '[ACCOUNT]': 1, '[RRN]': 1, '[CITY]': 1}\n",
      "data/chat.jsonl -> data/chat_clean.jsonl | rows: 3, redacted_rows: 1, hits: {'[PHONE]': 1}\n"
     ]
    }
   ],
   "source": [
    "# Step 6 â€” PII scrubbing + style tags (no Harmony here)\n",
    "import json, re, unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Normalization helpers ---\n",
    "HYPHENS = dict.fromkeys(map(ord, \"â€-â€’â€“â€”â€•ï¹˜ï¹£ï¼\"), ord(\"-\"))  # map unicode hyphens â†’ ASCII\n",
    "def normalize(s: str) -> str:\n",
    "    if not isinstance(s, str): return s\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.translate(HYPHENS)\n",
    "    return s\n",
    "\n",
    "# --- PII patterns (illustrative; tune for production) ---\n",
    "RE_EMAIL = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "# KR mobile numbers with spaces/hyphens: 010-1234-5678, 010 1234 5678, etc.\n",
    "RE_PHONE = re.compile(r\"\\b01[016789][-\\s]?\\d{3,4}[-\\s]?\\d{4}\\b\")\n",
    "# Korean RRN (ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸) basic pattern\n",
    "RE_RRN = re.compile(r\"\\b\\d{6}-\\d{7}\\b\")\n",
    "# Bank-ish account numbers: strictly digits in groups (avoid codes with letters)\n",
    "RE_ACCOUNT = re.compile(r\"\\b\\d{2,3}-\\d{2,4}-\\d{3,6}\\b\")\n",
    "# Very simple postal address cue (city names) â€“ conservative, just redact the token (optional)\n",
    "RE_CITY = re.compile(r\"(ì„œìš¸íŠ¹ë³„ì‹œ|ë¶€ì‚°ê´‘ì—­ì‹œ|ëŒ€êµ¬ê´‘ì—­ì‹œ|ì¸ì²œê´‘ì—­ì‹œ|ê´‘ì£¼ê´‘ì—­ì‹œ|ëŒ€ì „ê´‘ì—­ì‹œ|ìš¸ì‚°ê´‘ì—­ì‹œ|ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ|ê²½ê¸°ë„|ê°•ì›ë„|ì¶©ì²­ë¶ë„|ì¶©ì²­ë‚¨ë„|ì „ë¼ë¶ë„|ì „ë¼ë‚¨ë„|ê²½ìƒë¶ë„|ê²½ìƒë‚¨ë„|ì œì£¼íŠ¹ë³„ìì¹˜ë„)\")\n",
    "\n",
    "# Allowlist: things that look like PII but arenâ€™t (e.g., bill/order codes w/ letters)\n",
    "def looks_like_code(s: str) -> bool:\n",
    "    return bool(re.search(r\"[A-Za-z]\", s))  # if letters present, treat as code, not account/phone\n",
    "\n",
    "# Order of application matters (longest/most specific first sometimes helps)\n",
    "SCRUBBERS = [\n",
    "    (\"[RRN]\", RE_RRN),\n",
    "    (\"[EMAIL]\", RE_EMAIL),\n",
    "    (\"[PHONE]\", RE_PHONE),\n",
    "    (\"[ACCOUNT]\", RE_ACCOUNT),\n",
    "    (\"[CITY]\", RE_CITY),  # optional; comment out if you don't want to redact city tokens\n",
    "]\n",
    "\n",
    "def scrub_text(text: str) -> tuple[str, dict]:\n",
    "    \"\"\"Return (scrubbed_text, hits_dict). Avoid false positives with basic allowlisting.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return text, {}\n",
    "    orig = text\n",
    "    text = normalize(text)\n",
    "    hits = {}\n",
    "\n",
    "    # Guard account-like and phone-like strings that contain letters (likely codes)\n",
    "    guarded = set()\n",
    "    for m in RE_ACCOUNT.finditer(text):\n",
    "        if looks_like_code(m.group(0)):\n",
    "            guarded.add(m.span())\n",
    "    for m in RE_PHONE.finditer(text):\n",
    "        if looks_like_code(m.group(0)):\n",
    "            guarded.add(m.span())\n",
    "\n",
    "    # Apply scrubs\n",
    "    for label, pattern in SCRUBBERS:\n",
    "        out = []\n",
    "        last = 0\n",
    "        count = 0\n",
    "        for m in pattern.finditer(text):\n",
    "            span = m.span()\n",
    "            if pattern in (RE_ACCOUNT, RE_PHONE) and span in guarded:\n",
    "                continue\n",
    "            out.append(text[last:span[0]])\n",
    "            out.append(label)\n",
    "            last = span[1]\n",
    "            count += 1\n",
    "        out.append(text[last:])\n",
    "        text = \"\".join(out)\n",
    "        if count:\n",
    "            hits[label] = hits.get(label, 0) + count\n",
    "\n",
    "    return text, hits if text != orig else {}\n",
    "\n",
    "def scrub_record(rec: dict, kind: str) -> tuple[dict, dict]:\n",
    "    \"\"\"Scrub fields in a news/chat record; return (new_rec, hits).\"\"\"\n",
    "    rec = dict(rec)  # shallow copy\n",
    "    total_hits = {}\n",
    "\n",
    "    def scrub_field(key):\n",
    "        val = rec.get(key)\n",
    "        new, hits = scrub_text(val) if isinstance(val, str) else (val, {})\n",
    "        rec[key] = new\n",
    "        for k, v in hits.items():\n",
    "            total_hits[k] = total_hits.get(k, 0) + v\n",
    "\n",
    "    if kind == \"news\":\n",
    "        for key in (\"title\", \"summary\", \"topic\"):\n",
    "            scrub_field(key)\n",
    "    elif kind == \"chat\":\n",
    "        scrub_field(\"style\")\n",
    "        if isinstance(rec.get(\"dialog\"), list):\n",
    "            cleaned_dialog = []\n",
    "            for turn in rec[\"dialog\"]:\n",
    "                new, hits = scrub_text(turn) if isinstance(turn, str) else (turn, {})\n",
    "                cleaned_dialog.append(new)\n",
    "                for k, v in hits.items():\n",
    "                    total_hits[k] = total_hits.get(k, 0) + v\n",
    "            rec[\"dialog\"] = cleaned_dialog\n",
    "\n",
    "    return rec, total_hits\n",
    "\n",
    "# --- Style tagger (lightweight labels for later routing/metrics) ---\n",
    "def build_style_tags(rec: dict, kind: str) -> list[str]:\n",
    "    tags = []\n",
    "    if kind == \"news\":\n",
    "        tags.append(\"domain:\" + (rec.get(\"topic\") or \"unknown\"))\n",
    "        tags.append(\"style:\" + (rec.get(\"style\") or \"news\"))\n",
    "        tags.append(\"tone:formal\")\n",
    "        tags.append(\"medium:news\")\n",
    "    elif kind == \"chat\":\n",
    "        style = (rec.get(\"style\") or \"\").lower()\n",
    "        tags.append(\"style:\" + (style or \"chat\"))\n",
    "        tags.append(\"tone:\" + (\"formal\" if \"formal\" in style else \"casual\"))\n",
    "        tags.append(\"medium:kakao\")\n",
    "    return [t.replace(\" \", \"_\") for t in tags]\n",
    "\n",
    "# --- Process files ---\n",
    "def process_file(src: str, dst: str, kind: str):\n",
    "    total = 0\n",
    "    redacted = 0\n",
    "    counters = {}\n",
    "    with open(src, encoding=\"utf-8\") as fin, open(dst, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            if not line.strip(): continue\n",
    "            rec = json.loads(line)\n",
    "            total += 1\n",
    "            cleaned, hits = scrub_record(rec, kind)\n",
    "            cleaned[\"style_tags\"] = build_style_tags(cleaned, kind)\n",
    "            cleaned[\"_pii_hits\"] = hits  # keep for inspection; drop later if you want\n",
    "            if hits: redacted += 1\n",
    "            for k, v in hits.items():\n",
    "                counters[k] = counters.get(k, 0) + v\n",
    "            fout.write(json.dumps(cleaned, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"{src} -> {dst} | rows: {total}, redacted_rows: {redacted}, hits: {counters}\")\n",
    "\n",
    "process_file(\"data/news.jsonl\", \"data/news_clean.jsonl\", kind=\"news\")\n",
    "process_file(\"data/chat.jsonl\", \"data/chat_clean.jsonl\", kind=\"chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac01dca",
   "metadata": {},
   "source": [
    "## 7) ë°ì´í„° ë¡œë”©/í¬ë§·íŒ… Â· Load & Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd825e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: data/news_harmony.jsonl data/chat_harmony.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f769d524f424ed5a11781a157cfa796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating news split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2e4dc971884747a719d500caf52722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating chat split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 3, 'validation': 4}\n"
     ]
    }
   ],
   "source": [
    "# Step 7 â€” Harmony conversion + dataset loading & tokenization\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DATA = Path(\"data\")\n",
    "assert (DATA / \"news_clean.jsonl\").exists(), \"Run Step 6 first\"\n",
    "assert (DATA / \"chat_clean.jsonl\").exists(), \"Run Step 6 first\"\n",
    "\n",
    "# ---------- 7A) Convert cleaned â†’ Harmony messages ----------\n",
    "\n",
    "def news_to_messages(rec):\n",
    "    # system style from Step 6 tags; default to KR news tone\n",
    "    system = \"í•œêµ­ ë‰´ìŠ¤ ë¬¸ì²´ë¡œ ê°„ê²°í•˜ê³  ì‚¬ì‹¤ ìœ„ì£¼ë¡œ ì‘ì„±.\"\n",
    "    # user asks for a headline+lead from topic; assistant is the expected formatted answer\n",
    "    user = f\"ì£¼ì œ: {rec.get('topic','ì•Œìˆ˜ì—†ìŒ')}. ê¸°ì‚¬ ì œëª©ê³¼ ìš”ì•½ì„ ìƒì„±í•´ì¤˜.\"\n",
    "    assistant = f\"{rec.get('title','')} â€” {rec.get('summary','')}\"\n",
    "    return [{\"role\":\"system\",\"content\":system},\n",
    "            {\"role\":\"user\",\"content\":user},\n",
    "            {\"role\":\"assistant\",\"content\":assistant}]\n",
    "\n",
    "def chat_to_messages(rec):\n",
    "    # Keep style hint (casual/formal) in system\n",
    "    style = (rec.get(\"style\") or \"\").lower()\n",
    "    system = f\"ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ìŠ¤íƒ€ì¼. style={style or 'chat'}\"\n",
    "    dialog = rec.get(\"dialog\") or []\n",
    "    msgs = [{\"role\":\"system\",\"content\":system}]\n",
    "    # Alternate user/assistant turns; if odd length, last user stays without assistant label\n",
    "    roles = [\"user\",\"assistant\"]\n",
    "    for i, turn in enumerate(dialog[:6]):  # cap tiny demos to avoid runaway\n",
    "        msgs.append({\"role\": roles[i % 2], \"content\": str(turn)})\n",
    "    # Ensure there is at least one assistant turn for SFT\n",
    "    if not any(m[\"role\"]==\"assistant\" for m in msgs):\n",
    "        msgs.append({\"role\":\"assistant\",\"content\":\"ë„¤, í™•ì¸í–ˆìŠµë‹ˆë‹¤.\"})\n",
    "    return msgs\n",
    "\n",
    "def write_harmony(src, dst, kind):\n",
    "    convert = news_to_messages if kind==\"news\" else chat_to_messages\n",
    "    with open(src, encoding=\"utf-8\") as fin, open(dst, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            if not line.strip(): continue\n",
    "            rec = json.loads(line)\n",
    "            msgs = convert(rec)\n",
    "            fout.write(json.dumps({\"messages\": msgs}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_harmony(DATA/\"news_clean.jsonl\", DATA/\"news_harmony.jsonl\", \"news\")\n",
    "write_harmony(DATA/\"chat_clean.jsonl\", DATA/\"chat_harmony.jsonl\", \"chat\")\n",
    "print(\"Created:\", DATA/\"news_harmony.jsonl\", DATA/\"chat_harmony.jsonl\")\n",
    "\n",
    "# ---------- 7B) Load Harmony JSONL with ğŸ¤— Datasets ----------\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"news\": str(DATA/\"news_harmony.jsonl\"),\n",
    "                \"chat\": str(DATA/\"chat_harmony.jsonl\")}\n",
    ")\n",
    "\n",
    "# Mix train split using your Step-2 mix ratios\n",
    "news = raw[\"news\"]\n",
    "chat = raw[\"chat\"]\n",
    "\n",
    "def take_portion(ds, frac):\n",
    "    n = max(1, int(round(len(ds) * frac)))\n",
    "    return ds.select(range(n)) if n < len(ds) else ds\n",
    "\n",
    "news_part = take_portion(news, MIX_NEWS if 'MIX_NEWS' in globals() else 0.5)\n",
    "chat_part = take_portion(chat, MIX_CHAT if 'MIX_CHAT' in globals() else 0.5)\n",
    "train_ds = concatenate_datasets([news_part, chat_part]).shuffle(seed=42)\n",
    "\n",
    "# Tiny validation built from remaining examples (if any)\n",
    "remaining_news = news.select(range(len(news_part), len(news))) if len(news) > len(news_part) else news_part\n",
    "remaining_chat = chat.select(range(len(chat_part), len(chat))) if len(chat) > len(chat_part) else chat_part\n",
    "val_candidates = concatenate_datasets([remaining_news, remaining_chat])\n",
    "val_ds = val_candidates.shuffle(seed=43).select(range(min(64, len(val_candidates)))) if len(val_candidates) else train_ds.select(range(min(32, len(train_ds))))\n",
    "\n",
    "dataset = {\"train\": train_ds, \"validation\": val_ds}\n",
    "print({k: len(v) for k, v in dataset.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c9122",
   "metadata": {},
   "source": [
    "## 8) ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ Â· Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db67b6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfc411479e145e4b5b161df311d4b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebea3ddd62e340cc83e2a484a04e3e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330fd60c5e1248998f0f5bc8c394b2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027cffb8b0f94cecbd92dd8514ddbbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ad8283ea01465595cfb7d4c89279eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2a106e66474c68b3a7cc746218b4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeba45c1c0c4083817e302e23e316a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68baa8de3457430fa7cee836ca3257db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization done. train: 3 val: 4 example lens: [200006, 17360, 200008, 3575, 553, 17554, 162016, 11, 261, 4410, 6439, 2359] ...\n"
     ]
    }
   ],
   "source": [
    "# ---------- 7C) Tokenizer + Harmony template fallback ----------\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL,\n",
    "    use_fast=True,          # required if only tokenizer.json exists\n",
    "    trust_remote_code=True,\n",
    "    force_download=True     # ensures a fresh pull\n",
    ")\n",
    "\n",
    "if not getattr(tokenizer, \"chat_template\", None):\n",
    "    # Minimal Harmony-style fallback (server already knows Harmony; this is ONLY for training tokenization)\n",
    "    tokenizer.chat_template = \"\"\"{% for m in messages -%}\n",
    "{%- if m['role'] == 'system' -%}<|system|>\n",
    "{{ m['content'] }}<|end|>\n",
    "{%- elif m['role'] == 'user' -%}<|user|>\n",
    "{{ m['content'] }}<|end|>\n",
    "{%- elif m['role'] == 'assistant' -%}<|assistant|>\n",
    "{{ m['content'] }}<|end|>\n",
    "{%- endif -%}\n",
    "{%- endfor -%}\"\"\"\n",
    "\n",
    "# Ensure pad/eos are sane\n",
    "tokenizer.pad_token = tokenizer.eos_token or tokenizer.pad_token\n",
    "\n",
    "# ---------- 7D) Tokenize with assistant-only labels ----------\n",
    "ASST_TOKEN = None\n",
    "END_TOKEN = None\n",
    "try:\n",
    "    ASST_TOKEN = tokenizer.convert_tokens_to_ids(\"<|assistant|>\")\n",
    "    END_TOKEN = tokenizer.convert_tokens_to_ids(\"<|end|>\")\n",
    "except Exception:\n",
    "    # If the base vocab lacks these tokens, it's okay; masking fallback below will still work heuristically\n",
    "    pass\n",
    "\n",
    "MAX_LEN = 2048  # you can raise this if you have room\n",
    "\n",
    "def tokenize_with_labels(example):\n",
    "    # 1) Render with chat template (includes assistant answer)\n",
    "    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "    # 2) Tokenize\n",
    "    enc = tokenizer(text, truncation=True, max_length=MAX_LEN)\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    labels = [-100] * len(input_ids)\n",
    "\n",
    "    # 3) Label only assistant content\n",
    "    if ASST_TOKEN is not None and END_TOKEN is not None:\n",
    "        start = None\n",
    "        for i, tid in enumerate(input_ids):\n",
    "            if tid == ASST_TOKEN:\n",
    "                start = i + 1  # learn after the tag\n",
    "            elif start is not None and tid == END_TOKEN:\n",
    "                start = None\n",
    "            elif start is not None:\n",
    "                labels[i] = input_ids[i]\n",
    "    else:\n",
    "        # Heuristic fallback: learn on the last third of tokens (crude but avoids total silence)\n",
    "        start = int(len(input_ids) * 0.66)\n",
    "        for i in range(start, len(input_ids)):\n",
    "            labels[i] = input_ids[i]\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": enc[\"attention_mask\"], \"labels\": labels}\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(tokenize_with_labels, remove_columns=[\"messages\"])\n",
    "tokenized_val   = dataset[\"validation\"].map(tokenize_with_labels, remove_columns=[\"messages\"])\n",
    "\n",
    "print(\"Tokenization done.\",\n",
    "      \"train:\", len(tokenized_train),\n",
    "      \"val:\", len(tokenized_val),\n",
    "      \"example lens:\", tokenized_train[0][\"input_ids\"][:12], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67dd4ef",
   "metadata": {},
   "source": [
    "## 9) Fineâ€‘Tuning (LoRA/QLoRA) Â· ì„¸ë°€ íŠœë‹\n",
    "### 9a) Data curation & splits\n",
    "_(See Section 7/8 for dataset prep; move relevant snippets here if needed.)_\n",
    "### 9b) Hyperparameters (r/alpha/dropout)\n",
    "```python\n",
    "# Example LoRA hyperparameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "```\n",
    "\n",
    "### 9c) Merge adapters (BF16)\n",
    "```python\n",
    "# Example merge step (after training)\n",
    "# model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "# merged_model = model.merge_and_unload()\n",
    "```\n",
    "\n",
    "### 9d) Save merged BF16 (`save_pretrained`)\n",
    "```python\n",
    "# merged_model.save_pretrained(OUTPUT_DIR)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9157315",
   "metadata": {},
   "source": [
    "### 9e) Export & Quantize (BF16 â†’ MXFP4) Â· ë‚´ë³´ë‚´ê¸° & ì–‘ìí™”\n",
    "\n",
    "**EN (neutral, framework-agnostic):**  \n",
    "Public libraries currently do **not** support training/fineâ€‘tuning *directly* in MXFP4. The common pipeline is:\n",
    "1) **Train/SFT** in **BF16** (or **QLoRA 4â€‘bit nf4**).  \n",
    "2) **Merge LoRA adapters** into the base model (BF16).  \n",
    "3) **Save** the merged BF16 checkpoint with `save_pretrained()`.  \n",
    "4) **Postâ€‘training quantize** the merged BF16 tensors to **MXFP4** using a **vendor/toolchainâ€‘provided packer**.  \n",
    "5) **Save/export** the MXFP4 artifact (same shape as Hugging Face `save_pretrained()` output) for deployment/serving.\n",
    "\n",
    "> Notes:  \n",
    "> - If your serving stack supports **LoRA at inference**, you may skip merging and quantization and ship: **base (MXFP4 or BF16) + LoRA adapters**.  \n",
    "> - If your runtime requires **merged MXFP4**, you must run a **BF16 â†’ MXFP4** quantization step after merging adapters.  \n",
    "> - Keep **tokenizer/config** files aligned across BF16 and MXFP4 exports.\n",
    "\n",
    "**KR (ì¤‘ë¦½ì , ë„êµ¬ ë¹„ì˜ì¡´):**  \n",
    "í˜„ì¬ ê³µê°œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” MXFP4ì—ì„œ **ì§ì ‘ í•™ìŠµ/íŒŒì¸íŠœë‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**. ì¼ë°˜ì ì¸ íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:  \n",
    "1) **BF16**(ë˜ëŠ” **QLoRA 4â€‘bit nf4**)ë¡œ **í•™ìŠµ/íŒŒì¸íŠœë‹**  \n",
    "2) **LoRA ì–´ëŒ‘í„° ë³‘í•©**(BF16 ê¸°ì¤€)  \n",
    "3) `save_pretrained()`ë¡œ **ë³‘í•©ëœ BF16 ì²´í¬í¬ì¸íŠ¸ ì €ì¥**  \n",
    "4) ë²¤ë”/íˆ´ì²´ì¸ì—ì„œ ì œê³µí•˜ëŠ” **ì–‘ìí™” ë„êµ¬**ë¡œ **BF16 â†’ MXFP4 ì‚¬í›„ ì–‘ìí™”**  \n",
    "5) ë°°í¬/ì„œë¹™ìš© **MXFP4 ì•„í‹°íŒ©íŠ¸ ì €ì¥/ë‚´ë³´ë‚´ê¸°** (Hugging Face `save_pretrained()` êµ¬ì¡°ì™€ ë™ì¼)\n",
    "\n",
    "> ì°¸ê³ :  \n",
    "> - **ì„œë¹™ì—ì„œ LoRAë¥¼ ì§€ì›**í•œë‹¤ë©´, ë³‘í•©Â·ì–‘ìí™”ë¥¼ ìƒëµí•˜ê³  **ê¸°ì €( MXFP4 ë˜ëŠ” BF16 ) + LoRA ì–´ëŒ‘í„°**ë¡œ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "> - **ë³‘í•©ëœ MXFP4**ê°€ í•„ìš”í•œ ëŸ°íƒ€ì„ì˜ ê²½ìš°, ì–´ëŒ‘í„° ë³‘í•© í›„ **BF16 â†’ MXFP4 ì¬ì–‘ìí™”** ë‹¨ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤.  \n",
    "> - **tokenizer/config** íŒŒì¼ì€ BF16ê³¼ MXFP4 ì•„í‹°íŒ©íŠ¸ ê°„ì— ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48a5cbc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fineâ€‘tuning skeleton ready. Unâ€‘comment on your machine.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES\n",
    ")\n",
    "\n",
    "# base_model = get_peft_model(base_model, lora_cfg)\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=BF16,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT\n",
    ")\n",
    "\n",
    "# trainer = SFTTrainer(model=base_model, args=sft_args, train_dataset=combined, tokenizer=tokenizer)\n",
    "# trainer.train()\n",
    "# trainer.save_model(OUTPUT_DIR)\n",
    "print(\"Fineâ€‘tuning skeleton ready. Unâ€‘comment on your machine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490798f2",
   "metadata": {},
   "source": [
    "## 10) í‰ê°€(ë‰´ìŠ¤/ëŒ€í™”) Â· Evaluation (News/Chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdafe4",
   "metadata": {},
   "source": [
    "**KR ì§€í‘œ Â· KR Metrics**  \n",
    "- ë‰´ìŠ¤ì„±: ì£¼ì œ ë¶„ë¥˜ ì í•©ë„(F1), ìš”ì•½ í’ˆì§ˆ(ROUGEâ€‘1/2/L), ë…í•´ QA(EM/F1).  \n",
    "- ëŒ€í™”ì„±: ìì—°ì„±/ë§¥ë½ ìœ ì§€, ê²½ì–´/ë°˜ë§ ì „í™˜ ì •í™•ë„, ì´ëª¨í‹°ì½˜/ì¶•ì•½ì–´ ì ì ˆì„±.\n",
    "\n",
    "**EN Notes**  \n",
    "- Use public KR benchmarks (e.g., topic classification, KorQuADâ€‘like QA) where licenses permit.\n",
    "- Mix automatic metrics (F1/ROUGE) with human eval for tone & politeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "971b8dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval stubs ready.\n"
     ]
    }
   ],
   "source": [
    "# Example helpers (stub)\n",
    "def simple_accuracy(preds, labels):\n",
    "    return sum(int(p==g) for p,g in zip(preds, labels)) / max(1, len(labels))\n",
    "\n",
    "# For ROUGE:\n",
    "# import evaluate\n",
    "# rouge = evaluate.load(\"rouge\")\n",
    "# result = rouge.compute(predictions=pred_texts, references=ref_texts)\n",
    "# print(result)\n",
    "\n",
    "print(\"Eval stubs ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5594e",
   "metadata": {},
   "source": [
    "## 11) Inference Prompt Templates Â· ì¶”ë¡  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f690452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-21\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "ë„ˆëŠ” í•œêµ­ ê³ ê°ì„ ë•ëŠ” ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.\n",
      "\n",
      "<|end|><|start|>user<|message|>êµ­ë‚´ PIPA ê·œì •ì„ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ë‚´ ë¬¸ì„œ ìš”ì•½ê¸°ë¥¼ êµ¬ì„±í•˜ë ¤ë©´ ì–´ë–¤ ì•„í‚¤í…ì²˜ê°€ ì¢‹ì„ê¹Œ?<|end|><|start|>assistant\n"
     ]
    }
   ],
   "source": [
    "from openai_harmony import Message, ChatFormatter\n",
    "\n",
    "# Example prompt construction using Harmony\n",
    "messages = [\n",
    "    Message(role=\"system\", content=\"ë„ˆëŠ” í•œêµ­ ê³ ê°ì„ ë•ëŠ” ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.\"),\n",
    "    Message(role=\"user\", content=\"êµ­ë‚´ PIPA ê·œì •ì„ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ë‚´ ë¬¸ì„œ ìš”ì•½ê¸°ë¥¼ êµ¬ì„±í•˜ë ¤ë©´ ì–´ë–¤ ì•„í‚¤í…ì²˜ê°€ ì¢‹ì„ê¹Œ?\")\n",
    "]\n",
    "\n",
    "prompt = ChatFormatter.to_chat_prompt(messages)\n",
    "print(prompt)  # For preview; pass to tokenizer when running inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216d049",
   "metadata": {},
   "source": [
    "## 12) ìµœì‹ ì„± ìœ ì§€ Â· Freshness Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452decd1",
   "metadata": {},
   "source": [
    "- **ì£¼ê°„ ë³´ì • SFT**: í—ˆìš©ëœ ë‰´ìŠ¤ API **ë©”íƒ€ë°ì´í„°(ì œëª©/ìš”ì•½/ì„¹ì…˜)** ìƒ˜í”Œë§ â†’ ìŠ¤íƒ€ì¼ ë³´ì •.  \n",
    "- **ëŒ€í™”ì²´ ì—…ë°ì´íŠ¸**: ìµœì‹  ì¶•ì•½ì–´/ì‹ ì¡°ì–´/ì´ëª¨í‹°ì½˜ ì‚¬ì „ ë°˜ì˜(ì˜ˆ: ã„±ã„±, ã…‡ã…‹, ã…‹ã…‹, ã„¹ã…‡).  \n",
    "- **íšŒê·€ í‰ê°€**: ë™ì¼ ì§€í‘œë¡œ before/after ë¹„êµ â†’ í˜¼í•©ë¹„/ì˜¨ë„/íŒ¨ë„í‹° íŠœë‹.\n",
    "\n",
    "- Weekly calibration SFT using **allowed news API metadata** for style;  \n",
    "- Update slang/emoji lexicons;  \n",
    "- Regression evals to track drift and adjust data mix/decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b9f2a",
   "metadata": {},
   "source": [
    "## 13) ì•ˆì „/ì»´í”Œë¼ì´ì–¸ìŠ¤ Â· Safety & Compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad24ef",
   "metadata": {},
   "source": [
    "- ë°ì´í„° ì¶œì²˜/ë¼ì´ì„ ìŠ¤ í™•ì¸(ë²¤ì¹˜ë§ˆí¬, API, ë‚´ë¶€ ë°ì´í„°) Â· Verify dataset/API licenses.\n",
    "- ê°œì¸ì •ë³´ ìŠ¤í¬ëŸ¬ë¹™(í›ˆë ¨/ë¡œê·¸/í‰ê°€ ì „) Â· Scrub PII before training/logging/eval.\n",
    "- ì €ì‘ê¶Œ/ì•½ê´€ ì¤€ìˆ˜(ê¸°ì‚¬ **ì›ë¬¸ ëŒ€ëŸ‰ ì¬í•™ìŠµ ê¸ˆì§€**) Â· Avoid mass training on full news articles.\n",
    "- ì¶œë ¥ ê²€ì¦(ìŠ¤í‚¤ë§ˆ/ê¸ˆì¹™ì–´/ë¯¼ê°ë„ ê·œì¹™) Â· Output validation & forbiddenâ€‘term filters.\n",
    "- ë²„ì „/í‰ê°€ ë¦¬í¬íŠ¸ ê´€ë¦¬ Â· Version datasets/models and keep eval reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb8464b",
   "metadata": {},
   "source": [
    "## 14) ë¬¸ì œí•´ê²° & ë‹¤ìŒ ë‹¨ê³„ Â· Troubleshooting & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee17077",
   "metadata": {},
   "source": [
    "- í˜¼í•© ë¹„ìœ¨ íŠœë‹: (ë‰´ìŠ¤:ëŒ€í™”) 6:4 â†’ 7:3 ë˜ëŠ” 5:5ë¡œ ì¡°ì •  \n",
    "- LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„°: r=8~16, Î±=16~32, dropout=0.05~0.1  \n",
    "- ì„œë¹„ìŠ¤í™”: vLLM/llama.cpp ì„œë¹™ + í† í”½/ìŠ¤íƒ€ì¼ ë¼ìš°íŒ…  \n",
    "- RAG ê²°í•©: ìµœì‹  ì‚¬ì‹¤ì„± ë³´ê°•ì„ ìœ„í•´ ë‰´ìŠ¤/ë¬¸ì„œ ì¸ë±ìŠ¤ ê²°í•©  \n",
    "- A/B í…ŒìŠ¤íŠ¸: í†¤/ê¸¸ì´/ì´ëª¨í‹°ì½˜ ì‚¬ìš©ëŸ‰ ë“± ì‚¬ìš©ì ë§Œì¡±ë„ ì¸¡ì •\n",
    "\n",
    "- Tune mix ratios, run A/B tests, consider vLLM serving, and pair with RAG for factuality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
