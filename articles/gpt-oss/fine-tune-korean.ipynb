{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538f25ce",
   "metadata": {},
   "source": [
    "\n",
    "Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÄ OpenAIÏùò  **gpt-oss (open‚Äëweight)** Î™®Îç∏ÏùÑ **ÌïúÍµ≠ Îâ¥Ïä§ Î¨∏Ï≤¥ + ÏµúÏã† ÎåÄÌôîÏ≤¥**Î°ú ÏÑ∏Î∞Ä ÌäúÎãùÌïòÎäî Î∞©Î≤ïÏùÑ\n",
    "ÌïúÍµ≠Ïñ¥/ÏòÅÏñ¥ **Ïù¥Ï§ë Ïñ∏Ïñ¥**Î°ú Ï†úÍ≥µÌï©ÎãàÎã§.  \n",
    "This notebook shows how to fine‚Äëtune OpenAI's **gpt-oss (open‚Äëweight)** models for **Korean news style + modern chat tone**, in **Korean & English**.\n",
    "\n",
    "---\n",
    "\n",
    "### MXFP4 workflow clarifications ¬∑ MXFP4 ÏõåÌÅ¨ÌîåÎ°ú Ï†ïÎ¶¨\n",
    "\n",
    "**EN:**  \n",
    "- Training or fine-tuning **directly in MXFP4 is not supported** by public frameworks today.  \n",
    "- Recommended path: train in **BF16** (or **QLoRA 4‚Äëbit nf4**) ‚Üí **merge LoRA** ‚Üí **post‚Äëtraining quantize to MXFP4** ‚Üí `save_pretrained()` for deployment.  \n",
    "- If you need an MXFP4 artifact, you must **re‚Äëquantize from BF16** after merging adapters. (Export utilities are evolving; if your toolchain already supports MXFP4 serialization, that‚Äôs ideal.)\n",
    "\n",
    "**KR:**  \n",
    "- ÌòÑÏû¨ Í≥µÍ∞ú ÌîÑÎ†àÏûÑÏõåÌÅ¨ÏóêÏÑúÎäî **MXFP4Î°ú ÏßÅÏ†ë ÌïôÏäµ/ÌååÏù∏ÌäúÎãù**Ïù¥ ÏßÄÏõêÎêòÏßÄ ÏïäÏäµÎãàÎã§.  \n",
    "- Í∂åÏû• Í≤ΩÎ°ú: **BF16**(ÎòêÎäî **QLoRA 4‚Äëbit nf4**)Î°ú ÌïôÏäµ ‚Üí **LoRA Î≥ëÌï©** ‚Üí **ÏÇ¨ÌõÑ(MXFP4) ÏñëÏûêÌôî** ‚Üí Î∞∞Ìè¨Ïö©ÏúºÎ°ú `save_pretrained()` Ï†ÄÏû•.  \n",
    "- MXFP4 ÏïÑÌã∞Ìå©Ìä∏Í∞Ä ÌïÑÏöîÌïòÎ©¥, Ïñ¥ÎåëÌÑ∞ Î≥ëÌï© ÌõÑ **BF16 ‚Üí MXFP4 Ïû¨ÏñëÏûêÌôî**Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§. (ÏßÅÎ†¨Ìôî Ïú†Ìã∏ÏùÄ ÏßÑÌôî Ï§ëÏù¥Î©∞, Ìà¥Ï≤¥Ïù∏ÏóêÏÑú MXFP4 Ï†ÄÏû•ÏùÑ ÏßÄÏõêÌïòÎ©¥ Í∞ÄÏû• Ï¢ãÏäµÎãàÎã§.)\n",
    "\n",
    "---\n",
    "\n",
    "### LoRA targets (MoE) ¬∑ LoRA ÌÉÄÍπÉ(MoE Ìè¨Ìï®)\n",
    "\n",
    "**EN:**  \n",
    "- Minimal config (fast, low VRAM): target attention only, e.g. `[\"q_proj\",\"v_proj\"]`.  \n",
    "- MoE‚Äëaware config (better domain adaptation, more VRAM/time): include **expert projection layers** in addition to attention.  \n",
    "\n",
    "```python\n",
    "from peft import LoraConfig\n",
    "\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # baseline\n",
    "MOE_TARGET_PARAMETERS = [\n",
    "    # example expert layers; adjust indices to your model depth\n",
    "    \"mlp.experts.gate_up_proj\",\n",
    "    \"mlp.experts.down_proj\",\n",
    "]\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",              # cover all linear layers\n",
    "    target_parameters=MOE_TARGET_PARAMETERS,  # add expert projections\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "```\n",
    "\n",
    "- Start with attention‚Äëonly; if KR domain fit is insufficient, enable MoE targets and re‚Äëeval.\n",
    "\n",
    "**KR:**  \n",
    "- ÏµúÏÜå Íµ¨ÏÑ±(Îπ†Î•¥Í≥† VRAM Ï†àÏïΩ): `[\"q_proj\",\"v_proj\"]` Îì± **Ïñ¥ÌÖêÏÖòÎßå** Ï†ÅÏö©.  \n",
    "- **MoE Ïù∏ÏßÄ Íµ¨ÏÑ±**(ÎèÑÎ©îÏù∏ Ï†ÅÌï©ÏÑ±‚Üë, ÏûêÏõê ÏÜåÎ™®‚Üë): Ïñ¥ÌÖêÏÖòÏóê **Ï†ÑÎ¨∏Í∞Ä(Expert) Ìà¨ÏòÅ Î†àÏù¥Ïñ¥**Î•º Ï∂îÍ∞ÄÎ°ú Ìè¨Ìï®.  \n",
    "- Î®ºÏ†Ä Ïñ¥ÌÖêÏÖòÎßåÏúºÎ°ú ÏãúÎèÑÌïú Îí§, ÌïúÍµ≠Ïñ¥ ÎèÑÎ©îÏù∏ Ï†ÅÌï©ÏÑ±Ïù¥ Î∂ÄÏ°±ÌïòÎ©¥ MoE ÌÉÄÍπÉÏùÑ ÏºúÍ≥† Ïû¨ÌèâÍ∞ÄÌïòÏÑ∏Ïöî."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c12ff",
   "metadata": {},
   "source": [
    "## Contents ¬∑ Î™©Ï∞®\n",
    "0) Goals & Scope ¬∑ Î™©Ìëú & Î≤îÏúÑ  \n",
    "1) Environment check ¬∑ ÌôòÍ≤Ω Ï†êÍ≤Ä  \n",
    "2) ÏÑ§Ï†ïÍ∞í ¬∑ Config  \n",
    "3) Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ¬∑ Install Deps  \n",
    "4) Îç∞Ïù¥ÌÑ∞ ÏÜåÏã±(ÌïúÍµ≠Ìòï) ¬∑ KR‚ÄëContext Data Sourcing  \n",
    "5) ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ¬∑ Create Sample Data  \n",
    "6) Ï†ÑÏ≤òÎ¶¨(PIPA) & Ïä§ÌÉÄÏùº ÎùºÎ≤® ¬∑ PII Scrubbing & Style Tags  \n",
    "7) Îç∞Ïù¥ÌÑ∞ Î°úÎî©/Ìè¨Îß∑ÌåÖ ¬∑ Load & Format  \n",
    "8) Î™®Îç∏/ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ¬∑ Load Model & Tokenizer  \n",
    "9) Fine‚ÄëTuning (LoRA/QLoRA) ¬∑ ÏÑ∏Î∞Ä ÌäúÎãù  \n",
    "   9a) Data curation & splits  \n",
    "   9b) Hyperparameters (r/alpha/dropout)  \n",
    "   9c) Merge adapters (BF16)  \n",
    "   9d) Save merged BF16 (`save_pretrained`)  \n",
    "   9e) Export & Quantize (BF16 ‚Üí MXFP4) ¬∑ ÎÇ¥Î≥¥ÎÇ¥Í∏∞ & ÏñëÏûêÌôî  \n",
    "10) ÌèâÍ∞Ä(Îâ¥Ïä§/ÎåÄÌôî) ¬∑ Evaluation (News/Chat)  \n",
    "11) Inference Prompt Templates ¬∑ Ï∂îÎ°† ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø  \n",
    "12) ÏµúÏã†ÏÑ± Ïú†ÏßÄ ¬∑ Freshness Strategy  \n",
    "13) ÏïàÏ†Ñ/Ïª¥ÌîåÎùºÏù¥Ïñ∏Ïä§ ¬∑ Safety & Compliance  \n",
    "14) Î¨∏Ï†úÌï¥Í≤∞ & Îã§Ïùå Îã®Í≥Ñ ¬∑ Troubleshooting & Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8655d2",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Training vs Quantization ‚Äî What‚Äôs supported\n",
    "- **Do:** Train with BF16/FP16 or QLoRA; export merged weights.\n",
    "- **Then:** Quantize to **MXFP4** for inference using provided conversion scripts/utilities.\n",
    "- **Don‚Äôt:** Attempt to run an end‚Äëto‚Äëend ‚Äútrain in MXFP4‚Äù pipeline ‚Äî not supported today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24a3d9",
   "metadata": {},
   "source": [
    "> **PII & Compliance Reminder:** For KR data, follow your enterprise policy (mask RRN/phone/account IDs, remove emails) **before** training & logging. Keep train/val/test splits stratified by source and style tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e883f5",
   "metadata": {},
   "source": [
    "### üß™ MoE adapters (optional)\n",
    "You can target MoE layers with adapters, but treat this as **advanced/experimental**. Start with attention projections first and validate KR benchmarks before expanding scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179543e6",
   "metadata": {},
   "source": [
    "> **Note:** Keep `transformers`, `peft`, `accelerate`, and `trl` at versions known to support BF16/4‚Äëbit LoRA.  \n",
    "If you pin `safetensors`, remember that **native MXFP4 serialization is not yet standardized**; loaders may upcast internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e743f0",
   "metadata": {},
   "source": [
    "### üîé Support Matrix ‚Äî At a glance\n",
    "- **Fine‚Äëtuning precision:** BF16/FP16 ‚úÖ ¬∑ QLoRA 4‚Äëbit ‚úÖ ¬∑ **MXFP4 FT ‚ùå**\n",
    "- **Quantization target:** MXFP4 ‚úÖ (post‚Äëtraining)\n",
    "- **API FT (hosted) for OSS models:** ‚ùå\n",
    "- **Open‚Äësource FT (Transformers/TRL/PEFT):** ‚úÖ\n",
    "- **LoRA targets:** `q_proj`, `k_proj`, `v_proj`, `o_proj` ‚úÖ; MoE expert adapters **experimental** ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dec1f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d489c2",
   "metadata": {},
   "source": [
    "## 0) Goals & Scope ¬∑ Î™©Ìëú & Î≤îÏúÑ\n",
    "- **KR**: ÌïúÍµ≠Ïñ¥ ÏùºÎ∞ò Îâ¥Ïä§ + ÏùºÏÉÅ/ÏÉÅÎã¥ ÎåÄÌôîÏ≤¥Ïóê ÏµúÏ†ÅÌôî. `style=news_headline|news_lead|news_body|kakao_casual|kakao_formal` Ï†úÏñ¥.\n",
    "- **EN**: Optimize for Korean news writing and modern chat tone; control output via style tags above.\n",
    "- **Stack**: `transformers`, `trl(SFTTrainer)`, `peft(LoRA/QLoRA)`, `datasets`.\n",
    "- **Hardware**: Single/few GPUs (BF16 preferred). CPU/Mac for lightweight tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97218d",
   "metadata": {},
   "source": [
    "## 1) Environment check ¬∑ ÌôòÍ≤Ω Ï†êÍ≤Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5babb2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\n",
      "OS/Platform: Linux-6.8.0-60-generic-x86_64-with-glibc2.35\n",
      "CUDA_VISIBLE_DEVICES: \n",
      "Torch: 2.7.1+cu126 CUDA: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OS/Platform:\", platform.platform())\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\"))\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"Torch not installed or GPU not detected:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25688688",
   "metadata": {},
   "source": [
    "## 2) ÏÑ§Ï†ïÍ∞í ¬∑ Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15817f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# === Model & Training Params ===\n",
    "BASE_URL = \"http://localhost:8000/v1\"     # vLLM OpenAI-compatible endpoint\n",
    "API_KEY  = \"dummy-key\"                     # vLLM ignores; SDK requires a value\n",
    "MODEL    = \"openai/gpt-oss-120b\"           # must match the model vLLM loaded\n",
    "OUTPUT_DIR = \"ft-oss-kr-news-chat-bilingual\"\n",
    "\n",
    "# Data mix (news : chat)\n",
    "MIX_NEWS = 0.6\n",
    "MIX_CHAT = 0.4\n",
    "\n",
    "# LoRA\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # adjust per model\n",
    "\n",
    "# Training\n",
    "EPOCHS = 1\n",
    "PER_DEVICE_BS = 2\n",
    "GRAD_ACCUM = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "BF16 = True\n",
    "LOG_STEPS = 20\n",
    "SAVE_STEPS = 200\n",
    "SAVE_TOTAL_LIMIT = 2\n",
    "\n",
    "print(\"Config ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f258eb",
   "metadata": {},
   "source": [
    "## 3) Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ¬∑ Install Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b75968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.55.3\n",
      "accelerate: 1.10.0\n",
      "datasets: 4.0.0\n",
      "peft: not installed\n",
      "trl: 0.21.0\n",
      "bitsandbytes: not installed\n",
      "sentencepiece: 0.2.1\n",
      "vllm: 0.10.1\n",
      "llama_cpp: 0.3.16\n",
      "pip: 25.2\n",
      "Install cells are commented. Un-comment in your environment.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install transformers accelerate datasets peft trl bitsandbytes sentencepiece\n",
    "# (optional) serving/runtimes\n",
    "# %pip install vllm\n",
    "# %pip install llama-cpp-python\n",
    "\n",
    "import importlib, pip\n",
    "\n",
    "for dep in [\"transformers\",\"accelerate\",\"datasets\",\"peft\",\"trl\",\n",
    "            \"bitsandbytes\",\"sentencepiece\",\"vllm\",\"llama_cpp\"]:\n",
    "    try:\n",
    "        print(f\"{dep}: {importlib.import_module(dep).__version__}\")\n",
    "    except Exception:\n",
    "        print(f\"{dep}: not installed\")\n",
    "\n",
    "print(f\"pip: {pip.__version__}\")\n",
    "\n",
    "print(\"Install cells are commented. Un-comment in your environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8647fd",
   "metadata": {},
   "source": [
    "## 4) Îç∞Ïù¥ÌÑ∞ ÏÜåÏã±(ÌïúÍµ≠Ìòï) ¬∑ KR‚ÄëContext Data Sourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22cbd6",
   "metadata": {},
   "source": [
    "**KR**  \n",
    "- Í≥µÍ∞ú Î≤§ÏπòÎßàÌÅ¨(Ï£ºÏ†ú Î∂ÑÎ•ò/ÏöîÏïΩ/QA) + **ÌóàÏö©Îêú Îâ¥Ïä§ APIÏùò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞(Ï†úÎ™©/ÏöîÏïΩ/ÏÑπÏÖò)** Ï§ëÏã¨ÏúºÎ°ú Ïä§ÌÉÄÏùº Î≥¥Ï†ï.\n",
    "- Í∏∞ÏÇ¨ **ÏõêÎ¨∏ ÎåÄÎüâ Ïû¨ÌïôÏäµÏùÄ Ï†ÄÏûëÍ∂å/ÏïΩÍ¥Ä Ïù¥Ïäà** ‚Üí Î©îÌÉÄÎç∞Ïù¥ÌÑ∞¬∑Í≥µÍ∞ú ÏΩîÌçºÏä§ ÏúÑÏ£º.\n",
    "- ÎåÄÌôîÏ≤¥Îäî Ìï©Î≤ï Í≥µÍ∞ú ÏΩîÌçºÏä§(Î∞òÎßê/Ï°¥ÎåìÎßê/Ïù¥Î™®Ìã∞ÏΩò/Ï∂ïÏïΩÏñ¥ ÎùºÎ≤® Ìè¨Ìï®) Ïö∞ÏÑ†.\n",
    "- PIPA: Ï£ºÎØºÎ≤àÌò∏/Ïó∞ÎùΩÏ≤ò/Ïù¥Î©îÏùº/Í≥ÑÏ¢å Îì± Í∞úÏù∏Ï†ïÎ≥¥Îäî **ÌõàÎ†® Ï†Ñ/Î°úÍ∑∏ Ï†Ñ** Ïä§ÌÅ¨Îü¨Îπô.\n",
    "\n",
    "**EN**  \n",
    "- Prefer public KR benchmarks (topic classification / summarization / QA) and **allowed news API metadata** for style calibration.\n",
    "- Avoid mass training on news full texts due to license/ToS constraints; use metadata + open corpora.\n",
    "- For chat, use lawful open corpora with tone/emoji/informal‚Äëformal annotations.\n",
    "- Scrub PII (phone, RRNs, emails, accounts) before training/logging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b918411",
   "metadata": {},
   "source": [
    "## 5) ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ¬∑ Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18db10a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: data/news.jsonl, data/chat.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib\n",
    "pathlib.Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "news_samples = [\n",
    "  {\"style\":\"news_lead\",\"topic\":\"Í≤ΩÏ†ú\",\"title\":\"Î∞òÎèÑÏ≤¥ ÏàòÏ∂ú Ìò∏Ï°∞‚Ä¶ 7Ïõî ÏàòÏ∂úÏï° 20% Ï¶ùÍ∞Ä\",\"summary\":\"ÏàòÏ∂ú Í∞úÏÑ†ÏÑ∏Í∞Ä Ïù¥Ïñ¥ÏßÄÎ©∞ Í≤ΩÍ∏∞ ÌöåÎ≥µ Í∏∞ÎåÄÍ∞Ä Ïª§Ï°åÎã§.\"},\n",
    "  {\"style\":\"news_headline\",\"topic\":\"Ï†ïÏπò\",\"title\":\"Íµ≠Ìöå, Îç∞Ïù¥ÌÑ∞ ÏÇ∞ÏóÖ Ïú°ÏÑ±Î≤ï Î≥∏ÌöåÏùò ÌÜµÍ≥º\",\"summary\":\"Îç∞Ïù¥ÌÑ∞ ÌôúÏö© Ï¥âÏßÑÍ≥º Í∞úÏù∏Ï†ïÎ≥¥ Î≥¥Ìò∏Î•º Í∞ïÌôîÌïòÎäî ÎÇ¥Ïö©.\"},\n",
    "  {\n",
    "    \"style\": \"news_lead\",\n",
    "    \"topic\": \"Í≤ΩÏ†ú\",\n",
    "    \"title\": \"Ïπ¥Ïπ¥Ïò§ÌéòÏù¥ Î≥¥Ïïà Ï†êÍ≤Ä‚Ä¶ Í≥†Í∞ùÎ¨∏Ïùò: help+vip@corp.co.kr\",\n",
    "    \"summary\": \"Í≥†Í∞ùÏÑºÌÑ∞ 010-1234-5678Î°ú Î¨∏Ïùò Ìè≠Ï£º. Í≥ÑÏ¢å 110-123-456789 Í¥ÄÎ†® Í≤∞Ï†ú Ïò§Î•ò ÎÖºÎûÄ.\"\n",
    "  },\n",
    "  {\n",
    "    \"style\": \"news_headline\",\n",
    "    \"topic\": \"ÏÇ¨Ìöå\",\n",
    "    \"title\": \"Í∞úÏù∏Ï†ïÎ≥¥ Ïú†Ï∂ú ÏùòÌòπ‚Ä¶ Ï£ºÎØºÎ≤àÌò∏ 901010-1234567 Ïú†ÌÜµ Ï£ºÏû•\",\n",
    "    \"summary\": \"ÏÑúÏö∏ÌäπÎ≥ÑÏãú Í∞ïÎÇ®Íµ¨ ÌÖåÌó§ÎûÄÎ°ú 123ÏóêÏÑú ÏûêÎ£å ÌôïÎ≥¥‚Ä¶ Îã¥ÎãπÏûê john.doe+news@example.com\"\n",
    "  }\n",
    "]\n",
    "\n",
    "chat_samples = [\n",
    "  {\"style\":\"kakao_casual\",\"dialog\":[\"Ï£ºÎßêÏóê ÎπÑ Ïò®ÎåÄ?\",\"Ïùë ÏùºÏöîÏùºÏóê ÍΩ§ Ïò®Îã§ÎçîÎùº ‚òî\",\"Ìóê Ïö∞ÏÇ∞ Ï±ôÍ≤®ÏïºÍ≤†Îã§\"]},\n",
    "  {\"style\":\"kakao_formal\",\"dialog\":[\"ÏïàÎÖïÌïòÏÑ∏Ïöî. Î∞∞ÏÜ° ÏùºÏ†ï ÌôïÏù∏ Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§.\",\"ÎÇ¥Ïùº Ï§ë ÎèÑÏ∞© ÏòàÏ†ïÏûÖÎãàÎã§.\",\"ÏïàÎÇ¥ Í∞êÏÇ¨Ìï©ÎãàÎã§.\"]},\n",
    "  {\n",
    "    \"style\": \"kakao_formal\",\n",
    "    \"dialog\": [\n",
    "      \"Î∞∞ÏÜ° ÌôïÏù∏ Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§. Ï£ºÎ¨∏Î≤àÌò∏ ORD-2025-0001 ÏûÖÎãàÎã§.\",\n",
    "      \"Ïó∞ÎùΩÏ≤òÎäî 010-2222-3333 ÏûÖÎãàÎã§. (Ïú†ÎãàÏΩîÎìú ÌïòÏù¥Ìîà)\",\n",
    "      \"Ï£ºÎØºÎì±Î°ùÎ≤àÌò∏Îäî Ï†úÍ≥µÌï† Ïàò ÏóÜÏäµÎãàÎã§.\"\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "with open(\"data/news.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for ex in news_samples: f.write(json.dumps(ex, ensure_ascii=False)+\"\\n\")\n",
    "with open(\"data/chat.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for ex in chat_samples: f.write(json.dumps(ex, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "print(\"Created: data/news.jsonl, data/chat.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1eaa27",
   "metadata": {},
   "source": [
    "## 6) Ï†ÑÏ≤òÎ¶¨(PIPA) & Ïä§ÌÉÄÏùº ÎùºÎ≤® ¬∑ PII Scrubbing & Style Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430c1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/news.jsonl -> data/news_clean.jsonl | rows: 4, redacted_rows: 2, hits: {'[EMAIL]': 2, '[ACCOUNT]': 1, '[RRN]': 1, '[CITY]': 1}\n",
      "data/chat.jsonl -> data/chat_clean.jsonl | rows: 3, redacted_rows: 1, hits: {'[PHONE]': 1}\n"
     ]
    }
   ],
   "source": [
    "# Step 6 ‚Äî PII scrubbing + style tags (no Harmony here)\n",
    "import json, re, unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Normalization helpers ---\n",
    "HYPHENS = dict.fromkeys(map(ord, \"‚Äê-‚Äí‚Äì‚Äî‚ÄïÔπòÔπ£Ôºç\"), ord(\"-\"))  # map unicode hyphens ‚Üí ASCII\n",
    "def normalize(s: str) -> str:\n",
    "    if not isinstance(s, str): return s\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.translate(HYPHENS)\n",
    "    return s\n",
    "\n",
    "# --- PII patterns (illustrative; tune for production) ---\n",
    "RE_EMAIL = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "# KR mobile numbers with spaces/hyphens: 010-1234-5678, 010 1234 5678, etc.\n",
    "RE_PHONE = re.compile(r\"\\b01[016789][-\\s]?\\d{3,4}[-\\s]?\\d{4}\\b\")\n",
    "# Korean RRN (Ï£ºÎØºÎì±Î°ùÎ≤àÌò∏) basic pattern\n",
    "RE_RRN = re.compile(r\"\\b\\d{6}-\\d{7}\\b\")\n",
    "# Bank-ish account numbers: strictly digits in groups (avoid codes with letters)\n",
    "RE_ACCOUNT = re.compile(r\"\\b\\d{2,3}-\\d{2,4}-\\d{3,6}\\b\")\n",
    "# Very simple postal address cue (city names) ‚Äì conservative, just redact the token (optional)\n",
    "RE_CITY = re.compile(r\"(ÏÑúÏö∏ÌäπÎ≥ÑÏãú|Î∂ÄÏÇ∞Í¥ëÏó≠Ïãú|ÎåÄÍµ¨Í¥ëÏó≠Ïãú|Ïù∏Ï≤úÍ¥ëÏó≠Ïãú|Í¥ëÏ£ºÍ¥ëÏó≠Ïãú|ÎåÄÏ†ÑÍ¥ëÏó≠Ïãú|Ïö∏ÏÇ∞Í¥ëÏó≠Ïãú|ÏÑ∏Ï¢ÖÌäπÎ≥ÑÏûêÏπòÏãú|Í≤ΩÍ∏∞ÎèÑ|Í∞ïÏõêÎèÑ|Ï∂©Ï≤≠Î∂ÅÎèÑ|Ï∂©Ï≤≠ÎÇ®ÎèÑ|Ï†ÑÎùºÎ∂ÅÎèÑ|Ï†ÑÎùºÎÇ®ÎèÑ|Í≤ΩÏÉÅÎ∂ÅÎèÑ|Í≤ΩÏÉÅÎÇ®ÎèÑ|Ï†úÏ£ºÌäπÎ≥ÑÏûêÏπòÎèÑ)\")\n",
    "\n",
    "# Allowlist: things that look like PII but aren‚Äôt (e.g., bill/order codes w/ letters)\n",
    "def looks_like_code(s: str) -> bool:\n",
    "    return bool(re.search(r\"[A-Za-z]\", s))  # if letters present, treat as code, not account/phone\n",
    "\n",
    "# Order of application matters (longest/most specific first sometimes helps)\n",
    "SCRUBBERS = [\n",
    "    (\"[RRN]\", RE_RRN),\n",
    "    (\"[EMAIL]\", RE_EMAIL),\n",
    "    (\"[PHONE]\", RE_PHONE),\n",
    "    (\"[ACCOUNT]\", RE_ACCOUNT),\n",
    "    (\"[CITY]\", RE_CITY),  # optional; comment out if you don't want to redact city tokens\n",
    "]\n",
    "\n",
    "def scrub_text(text: str) -> tuple[str, dict]:\n",
    "    \"\"\"Return (scrubbed_text, hits_dict). Avoid false positives with basic allowlisting.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return text, {}\n",
    "    orig = text\n",
    "    text = normalize(text)\n",
    "    hits = {}\n",
    "\n",
    "    # Guard account-like and phone-like strings that contain letters (likely codes)\n",
    "    guarded = set()\n",
    "    for m in RE_ACCOUNT.finditer(text):\n",
    "        if looks_like_code(m.group(0)):\n",
    "            guarded.add(m.span())\n",
    "    for m in RE_PHONE.finditer(text):\n",
    "        if looks_like_code(m.group(0)):\n",
    "            guarded.add(m.span())\n",
    "\n",
    "    # Apply scrubs\n",
    "    for label, pattern in SCRUBBERS:\n",
    "        out = []\n",
    "        last = 0\n",
    "        count = 0\n",
    "        for m in pattern.finditer(text):\n",
    "            span = m.span()\n",
    "            if pattern in (RE_ACCOUNT, RE_PHONE) and span in guarded:\n",
    "                continue\n",
    "            out.append(text[last:span[0]])\n",
    "            out.append(label)\n",
    "            last = span[1]\n",
    "            count += 1\n",
    "        out.append(text[last:])\n",
    "        text = \"\".join(out)\n",
    "        if count:\n",
    "            hits[label] = hits.get(label, 0) + count\n",
    "\n",
    "    return text, hits if text != orig else {}\n",
    "\n",
    "def scrub_record(rec: dict, kind: str) -> tuple[dict, dict]:\n",
    "    \"\"\"Scrub fields in a news/chat record; return (new_rec, hits).\"\"\"\n",
    "    rec = dict(rec)  # shallow copy\n",
    "    total_hits = {}\n",
    "\n",
    "    def scrub_field(key):\n",
    "        val = rec.get(key)\n",
    "        new, hits = scrub_text(val) if isinstance(val, str) else (val, {})\n",
    "        rec[key] = new\n",
    "        for k, v in hits.items():\n",
    "            total_hits[k] = total_hits.get(k, 0) + v\n",
    "\n",
    "    if kind == \"news\":\n",
    "        for key in (\"title\", \"summary\", \"topic\"):\n",
    "            scrub_field(key)\n",
    "    elif kind == \"chat\":\n",
    "        scrub_field(\"style\")\n",
    "        if isinstance(rec.get(\"dialog\"), list):\n",
    "            cleaned_dialog = []\n",
    "            for turn in rec[\"dialog\"]:\n",
    "                new, hits = scrub_text(turn) if isinstance(turn, str) else (turn, {})\n",
    "                cleaned_dialog.append(new)\n",
    "                for k, v in hits.items():\n",
    "                    total_hits[k] = total_hits.get(k, 0) + v\n",
    "            rec[\"dialog\"] = cleaned_dialog\n",
    "\n",
    "    return rec, total_hits\n",
    "\n",
    "# --- Style tagger (lightweight labels for later routing/metrics) ---\n",
    "def build_style_tags(rec: dict, kind: str) -> list[str]:\n",
    "    tags = []\n",
    "    if kind == \"news\":\n",
    "        tags.append(\"domain:\" + (rec.get(\"topic\") or \"unknown\"))\n",
    "        tags.append(\"style:\" + (rec.get(\"style\") or \"news\"))\n",
    "        tags.append(\"tone:formal\")\n",
    "        tags.append(\"medium:news\")\n",
    "    elif kind == \"chat\":\n",
    "        style = (rec.get(\"style\") or \"\").lower()\n",
    "        tags.append(\"style:\" + (style or \"chat\"))\n",
    "        tags.append(\"tone:\" + (\"formal\" if \"formal\" in style else \"casual\"))\n",
    "        tags.append(\"medium:kakao\")\n",
    "    return [t.replace(\" \", \"_\") for t in tags]\n",
    "\n",
    "# --- Process files ---\n",
    "def process_file(src: str, dst: str, kind: str):\n",
    "    total = 0\n",
    "    redacted = 0\n",
    "    counters = {}\n",
    "    with open(src, encoding=\"utf-8\") as fin, open(dst, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            if not line.strip(): continue\n",
    "            rec = json.loads(line)\n",
    "            total += 1\n",
    "            cleaned, hits = scrub_record(rec, kind)\n",
    "            cleaned[\"style_tags\"] = build_style_tags(cleaned, kind)\n",
    "            cleaned[\"_pii_hits\"] = hits  # keep for inspection; drop later if you want\n",
    "            if hits: redacted += 1\n",
    "            for k, v in hits.items():\n",
    "                counters[k] = counters.get(k, 0) + v\n",
    "            fout.write(json.dumps(cleaned, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"{src} -> {dst} | rows: {total}, redacted_rows: {redacted}, hits: {counters}\")\n",
    "\n",
    "process_file(\"data/news.jsonl\", \"data/news_clean.jsonl\", kind=\"news\")\n",
    "process_file(\"data/chat.jsonl\", \"data/chat_clean.jsonl\", kind=\"chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac01dca",
   "metadata": {},
   "source": [
    "## 7) Îç∞Ïù¥ÌÑ∞ Î°úÎî©/Ìè¨Îß∑ÌåÖ ¬∑ Load & Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd825e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: data/news_harmony.jsonl data/chat_harmony.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f769d524f424ed5a11781a157cfa796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating news split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2e4dc971884747a719d500caf52722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating chat split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 3, 'validation': 4}\n"
     ]
    }
   ],
   "source": [
    "# Step 7 ‚Äî Harmony conversion + dataset loading & tokenization\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DATA = Path(\"data\")\n",
    "assert (DATA / \"news_clean.jsonl\").exists(), \"Run Step 6 first\"\n",
    "assert (DATA / \"chat_clean.jsonl\").exists(), \"Run Step 6 first\"\n",
    "\n",
    "# ---------- 7A) Convert cleaned ‚Üí Harmony messages ----------\n",
    "\n",
    "def news_to_messages(rec):\n",
    "    # system style from Step 6 tags; default to KR news tone\n",
    "    system = \"ÌïúÍµ≠ Îâ¥Ïä§ Î¨∏Ï≤¥Î°ú Í∞ÑÍ≤∞ÌïòÍ≥† ÏÇ¨Ïã§ ÏúÑÏ£ºÎ°ú ÏûëÏÑ±.\"\n",
    "    # user asks for a headline+lead from topic; assistant is the expected formatted answer\n",
    "    user = f\"Ï£ºÏ†ú: {rec.get('topic','ÏïåÏàòÏóÜÏùå')}. Í∏∞ÏÇ¨ Ï†úÎ™©Í≥º ÏöîÏïΩÏùÑ ÏÉùÏÑ±Ìï¥Ï§ò.\"\n",
    "    assistant = f\"{rec.get('title','')} ‚Äî {rec.get('summary','')}\"\n",
    "    return [{\"role\":\"system\",\"content\":system},\n",
    "            {\"role\":\"user\",\"content\":user},\n",
    "            {\"role\":\"assistant\",\"content\":assistant}]\n",
    "\n",
    "def chat_to_messages(rec):\n",
    "    # Keep style hint (casual/formal) in system\n",
    "    style = (rec.get(\"style\") or \"\").lower()\n",
    "    system = f\"Ïπ¥Ïπ¥Ïò§ÌÜ° ÎåÄÌôî Ïä§ÌÉÄÏùº. style={style or 'chat'}\"\n",
    "    dialog = rec.get(\"dialog\") or []\n",
    "    msgs = [{\"role\":\"system\",\"content\":system}]\n",
    "    # Alternate user/assistant turns; if odd length, last user stays without assistant label\n",
    "    roles = [\"user\",\"assistant\"]\n",
    "    for i, turn in enumerate(dialog[:6]):  # cap tiny demos to avoid runaway\n",
    "        msgs.append({\"role\": roles[i % 2], \"content\": str(turn)})\n",
    "    # Ensure there is at least one assistant turn for SFT\n",
    "    if not any(m[\"role\"]==\"assistant\" for m in msgs):\n",
    "        msgs.append({\"role\":\"assistant\",\"content\":\"ÎÑ§, ÌôïÏù∏ÌñàÏäµÎãàÎã§.\"})\n",
    "    return msgs\n",
    "\n",
    "def write_harmony(src, dst, kind):\n",
    "    convert = news_to_messages if kind==\"news\" else chat_to_messages\n",
    "    with open(src, encoding=\"utf-8\") as fin, open(dst, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            if not line.strip(): continue\n",
    "            rec = json.loads(line)\n",
    "            msgs = convert(rec)\n",
    "            fout.write(json.dumps({\"messages\": msgs}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_harmony(DATA/\"news_clean.jsonl\", DATA/\"news_harmony.jsonl\", \"news\")\n",
    "write_harmony(DATA/\"chat_clean.jsonl\", DATA/\"chat_harmony.jsonl\", \"chat\")\n",
    "print(\"Created:\", DATA/\"news_harmony.jsonl\", DATA/\"chat_harmony.jsonl\")\n",
    "\n",
    "# ---------- 7B) Load Harmony JSONL with ü§ó Datasets ----------\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"news\": str(DATA/\"news_harmony.jsonl\"),\n",
    "                \"chat\": str(DATA/\"chat_harmony.jsonl\")}\n",
    ")\n",
    "\n",
    "# Mix train split using your Step-2 mix ratios\n",
    "news = raw[\"news\"]\n",
    "chat = raw[\"chat\"]\n",
    "\n",
    "def take_portion(ds, frac):\n",
    "    n = max(1, int(round(len(ds) * frac)))\n",
    "    return ds.select(range(n)) if n < len(ds) else ds\n",
    "\n",
    "news_part = take_portion(news, MIX_NEWS if 'MIX_NEWS' in globals() else 0.5)\n",
    "chat_part = take_portion(chat, MIX_CHAT if 'MIX_CHAT' in globals() else 0.5)\n",
    "train_ds = concatenate_datasets([news_part, chat_part]).shuffle(seed=42)\n",
    "\n",
    "# Tiny validation built from remaining examples (if any)\n",
    "remaining_news = news.select(range(len(news_part), len(news))) if len(news) > len(news_part) else news_part\n",
    "remaining_chat = chat.select(range(len(chat_part), len(chat))) if len(chat) > len(chat_part) else chat_part\n",
    "val_candidates = concatenate_datasets([remaining_news, remaining_chat])\n",
    "val_ds = val_candidates.shuffle(seed=43).select(range(min(64, len(val_candidates)))) if len(val_candidates) else train_ds.select(range(min(32, len(train_ds))))\n",
    "\n",
    "dataset = {\"train\": train_ds, \"validation\": val_ds}\n",
    "print({k: len(v) for k, v in dataset.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c9122",
   "metadata": {},
   "source": [
    "## 8) Î™®Îç∏/ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ¬∑ Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db67b6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfc411479e145e4b5b161df311d4b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebea3ddd62e340cc83e2a484a04e3e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330fd60c5e1248998f0f5bc8c394b2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027cffb8b0f94cecbd92dd8514ddbbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ad8283ea01465595cfb7d4c89279eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2a106e66474c68b3a7cc746218b4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeba45c1c0c4083817e302e23e316a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68baa8de3457430fa7cee836ca3257db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization done. train: 3 val: 4 example lens: [200006, 17360, 200008, 3575, 553, 17554, 162016, 11, 261, 4410, 6439, 2359] ...\n"
     ]
    }
   ],
   "source": [
    "# ---------- 7C) Tokenizer + Harmony template fallback ----------\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL,\n",
    "    use_fast=True,          # required if only tokenizer.json exists\n",
    "    trust_remote_code=True,\n",
    "    force_download=True     # ensures a fresh pull\n",
    ")\n",
    "\n",
    "if not getattr(tokenizer, \"chat_template\", None):\n",
    "    # Minimal Harmony-style fallback (server already knows Harmony; this is ONLY for training tokenization)\n",
    "    tokenizer.chat_template = \"\"\"{% for m in messages -%}\n",
    "{%- if m['role'] == 'system' -%}<|system|>\n",
    "{{ m['content'] }}<|end|>\n",
    "{%- elif m['role'] == 'user' -%}<|user|>\n",
    "{{ m['content'] }}<|end|>\n",
    "{%- elif m['role'] == 'assistant' -%}<|assistant|>\n",
    "{{ m['content'] }}<|end|>\n",
    "{%- endif -%}\n",
    "{%- endfor -%}\"\"\"\n",
    "\n",
    "# Ensure pad/eos are sane\n",
    "tokenizer.pad_token = tokenizer.eos_token or tokenizer.pad_token\n",
    "\n",
    "# ---------- 7D) Tokenize with assistant-only labels ----------\n",
    "ASST_TOKEN = None\n",
    "END_TOKEN = None\n",
    "try:\n",
    "    ASST_TOKEN = tokenizer.convert_tokens_to_ids(\"<|assistant|>\")\n",
    "    END_TOKEN = tokenizer.convert_tokens_to_ids(\"<|end|>\")\n",
    "except Exception:\n",
    "    # If the base vocab lacks these tokens, it's okay; masking fallback below will still work heuristically\n",
    "    pass\n",
    "\n",
    "MAX_LEN = 2048  # you can raise this if you have room\n",
    "\n",
    "def tokenize_with_labels(example):\n",
    "    # 1) Render with chat template (includes assistant answer)\n",
    "    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "    # 2) Tokenize\n",
    "    enc = tokenizer(text, truncation=True, max_length=MAX_LEN)\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    labels = [-100] * len(input_ids)\n",
    "\n",
    "    # 3) Label only assistant content\n",
    "    if ASST_TOKEN is not None and END_TOKEN is not None:\n",
    "        start = None\n",
    "        for i, tid in enumerate(input_ids):\n",
    "            if tid == ASST_TOKEN:\n",
    "                start = i + 1  # learn after the tag\n",
    "            elif start is not None and tid == END_TOKEN:\n",
    "                start = None\n",
    "            elif start is not None:\n",
    "                labels[i] = input_ids[i]\n",
    "    else:\n",
    "        # Heuristic fallback: learn on the last third of tokens (crude but avoids total silence)\n",
    "        start = int(len(input_ids) * 0.66)\n",
    "        for i in range(start, len(input_ids)):\n",
    "            labels[i] = input_ids[i]\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": enc[\"attention_mask\"], \"labels\": labels}\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(tokenize_with_labels, remove_columns=[\"messages\"])\n",
    "tokenized_val   = dataset[\"validation\"].map(tokenize_with_labels, remove_columns=[\"messages\"])\n",
    "\n",
    "print(\"Tokenization done.\",\n",
    "      \"train:\", len(tokenized_train),\n",
    "      \"val:\", len(tokenized_val),\n",
    "      \"example lens:\", tokenized_train[0][\"input_ids\"][:12], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67dd4ef",
   "metadata": {},
   "source": [
    "## 9) Fine‚ÄëTuning (LoRA/QLoRA) ¬∑ ÏÑ∏Î∞Ä ÌäúÎãù\n",
    "### 9a) Data curation & splits\n",
    "_(See Section 7/8 for dataset prep; move relevant snippets here if needed.)_\n",
    "### 9b) Hyperparameters (r/alpha/dropout)\n",
    "```python\n",
    "# Example LoRA hyperparameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "```\n",
    "\n",
    "### 9c) Merge adapters (BF16)\n",
    "```python\n",
    "# Example merge step (after training)\n",
    "# model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "# merged_model = model.merge_and_unload()\n",
    "```\n",
    "\n",
    "### 9d) Save merged BF16 (`save_pretrained`)\n",
    "```python\n",
    "# merged_model.save_pretrained(OUTPUT_DIR)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9157315",
   "metadata": {},
   "source": [
    "### 9e) Export & Quantize (BF16 ‚Üí MXFP4) ¬∑ ÎÇ¥Î≥¥ÎÇ¥Í∏∞ & ÏñëÏûêÌôî\n",
    "\n",
    "**EN (neutral, framework-agnostic):**  \n",
    "Public libraries currently do **not** support training/fine‚Äëtuning *directly* in MXFP4. The common pipeline is:\n",
    "1) **Train/SFT** in **BF16** (or **QLoRA 4‚Äëbit nf4**).  \n",
    "2) **Merge LoRA adapters** into the base model (BF16).  \n",
    "3) **Save** the merged BF16 checkpoint with `save_pretrained()`.  \n",
    "4) **Post‚Äëtraining quantize** the merged BF16 tensors to **MXFP4** using a **vendor/toolchain‚Äëprovided packer**.  \n",
    "5) **Save/export** the MXFP4 artifact (same shape as Hugging Face `save_pretrained()` output) for deployment/serving.\n",
    "\n",
    "> Notes:  \n",
    "> - If your serving stack supports **LoRA at inference**, you may skip merging and quantization and ship: **base (MXFP4 or BF16) + LoRA adapters**.  \n",
    "> - If your runtime requires **merged MXFP4**, you must run a **BF16 ‚Üí MXFP4** quantization step after merging adapters.  \n",
    "> - Keep **tokenizer/config** files aligned across BF16 and MXFP4 exports.\n",
    "\n",
    "**KR (Ï§ëÎ¶ΩÏ†Å, ÎèÑÍµ¨ ÎπÑÏùòÏ°¥):**  \n",
    "ÌòÑÏû¨ Í≥µÍ∞ú ÎùºÏù¥Î∏åÎü¨Î¶¨Îäî MXFP4ÏóêÏÑú **ÏßÅÏ†ë ÌïôÏäµ/ÌååÏù∏ÌäúÎãùÏùÑ ÏßÄÏõêÌïòÏßÄ ÏïäÏäµÎãàÎã§**. ÏùºÎ∞òÏ†ÅÏù∏ ÌååÏù¥ÌîÑÎùºÏù∏ÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:  \n",
    "1) **BF16**(ÎòêÎäî **QLoRA 4‚Äëbit nf4**)Î°ú **ÌïôÏäµ/ÌååÏù∏ÌäúÎãù**  \n",
    "2) **LoRA Ïñ¥ÎåëÌÑ∞ Î≥ëÌï©**(BF16 Í∏∞Ï§Ä)  \n",
    "3) `save_pretrained()`Î°ú **Î≥ëÌï©Îêú BF16 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•**  \n",
    "4) Î≤§Îçî/Ìà¥Ï≤¥Ïù∏ÏóêÏÑú Ï†úÍ≥µÌïòÎäî **ÏñëÏûêÌôî ÎèÑÍµ¨**Î°ú **BF16 ‚Üí MXFP4 ÏÇ¨ÌõÑ ÏñëÏûêÌôî**  \n",
    "5) Î∞∞Ìè¨/ÏÑúÎπôÏö© **MXFP4 ÏïÑÌã∞Ìå©Ìä∏ Ï†ÄÏû•/ÎÇ¥Î≥¥ÎÇ¥Í∏∞** (Hugging Face `save_pretrained()` Íµ¨Ï°∞ÏôÄ ÎèôÏùº)\n",
    "\n",
    "> Ï∞∏Í≥†:  \n",
    "> - **ÏÑúÎπôÏóêÏÑú LoRAÎ•º ÏßÄÏõê**ÌïúÎã§Î©¥, Î≥ëÌï©¬∑ÏñëÏûêÌôîÎ•º ÏÉùÎûµÌïòÍ≥† **Í∏∞Ï†Ä( MXFP4 ÎòêÎäî BF16 ) + LoRA Ïñ¥ÎåëÌÑ∞**Î°ú Ï†úÍ≥µÌï† Ïàò ÏûàÏäµÎãàÎã§.  \n",
    "> - **Î≥ëÌï©Îêú MXFP4**Í∞Ä ÌïÑÏöîÌïú Îü∞ÌÉÄÏûÑÏùò Í≤ΩÏö∞, Ïñ¥ÎåëÌÑ∞ Î≥ëÌï© ÌõÑ **BF16 ‚Üí MXFP4 Ïû¨ÏñëÏûêÌôî** Îã®Í≥ÑÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.  \n",
    "> - **tokenizer/config** ÌååÏùºÏùÄ BF16Í≥º MXFP4 ÏïÑÌã∞Ìå©Ìä∏ Í∞ÑÏóê ÏùºÍ¥ÄÎêòÍ≤å Ïú†ÏßÄÌïòÏÑ∏Ïöî.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48a5cbc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine‚Äëtuning skeleton ready. Un‚Äëcomment on your machine.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES\n",
    ")\n",
    "\n",
    "# base_model = get_peft_model(base_model, lora_cfg)\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=BF16,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT\n",
    ")\n",
    "\n",
    "# trainer = SFTTrainer(model=base_model, args=sft_args, train_dataset=combined, tokenizer=tokenizer)\n",
    "# trainer.train()\n",
    "# trainer.save_model(OUTPUT_DIR)\n",
    "print(\"Fine‚Äëtuning skeleton ready. Un‚Äëcomment on your machine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490798f2",
   "metadata": {},
   "source": [
    "## 10) ÌèâÍ∞Ä(Îâ¥Ïä§/ÎåÄÌôî) ¬∑ Evaluation (News/Chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdafe4",
   "metadata": {},
   "source": [
    "**KR ÏßÄÌëú ¬∑ KR Metrics**  \n",
    "- Îâ¥Ïä§ÏÑ±: Ï£ºÏ†ú Î∂ÑÎ•ò Ï†ÅÌï©ÎèÑ(F1), ÏöîÏïΩ ÌíàÏßà(ROUGE‚Äë1/2/L), ÎèÖÌï¥ QA(EM/F1).  \n",
    "- ÎåÄÌôîÏÑ±: ÏûêÏó∞ÏÑ±/Îß•ÎùΩ Ïú†ÏßÄ, Í≤ΩÏñ¥/Î∞òÎßê Ï†ÑÌôò Ï†ïÌôïÎèÑ, Ïù¥Î™®Ìã∞ÏΩò/Ï∂ïÏïΩÏñ¥ Ï†ÅÏ†àÏÑ±.\n",
    "\n",
    "**EN Notes**  \n",
    "- Use public KR benchmarks (e.g., topic classification, KorQuAD‚Äëlike QA) where licenses permit.\n",
    "- Mix automatic metrics (F1/ROUGE) with human eval for tone & politeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "971b8dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval stubs ready.\n"
     ]
    }
   ],
   "source": [
    "# Example helpers (stub)\n",
    "def simple_accuracy(preds, labels):\n",
    "    return sum(int(p==g) for p,g in zip(preds, labels)) / max(1, len(labels))\n",
    "\n",
    "# For ROUGE:\n",
    "# import evaluate\n",
    "# rouge = evaluate.load(\"rouge\")\n",
    "# result = rouge.compute(predictions=pred_texts, references=ref_texts)\n",
    "# print(result)\n",
    "\n",
    "print(\"Eval stubs ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5594e",
   "metadata": {},
   "source": [
    "## 11) Inference Prompt Templates ¬∑ Ï∂îÎ°† ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f690452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-21\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "ÎÑàÎäî ÌïúÍµ≠ Í≥†Í∞ùÏùÑ ÎèïÎäî Ïú†Îä•Ìïú AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Îã§.\n",
      "\n",
      "<|end|><|start|>user<|message|>Íµ≠ÎÇ¥ PIPA Í∑úÏ†ïÏùÑ Ï§ÄÏàòÌïòÎ©¥ÏÑú ÏÇ¨ÎÇ¥ Î¨∏ÏÑú ÏöîÏïΩÍ∏∞Î•º Íµ¨ÏÑ±ÌïòÎ†§Î©¥ Ïñ¥Îñ§ ÏïÑÌÇ§ÌÖçÏ≤òÍ∞Ä Ï¢ãÏùÑÍπå?<|end|><|start|>assistant\n"
     ]
    }
   ],
   "source": [
    "from openai_harmony import Message, ChatFormatter\n",
    "\n",
    "# Example prompt construction using Harmony\n",
    "messages = [\n",
    "    Message(role=\"system\", content=\"ÎÑàÎäî ÌïúÍµ≠ Í≥†Í∞ùÏùÑ ÎèïÎäî Ïú†Îä•Ìïú AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Îã§.\"),\n",
    "    Message(role=\"user\", content=\"Íµ≠ÎÇ¥ PIPA Í∑úÏ†ïÏùÑ Ï§ÄÏàòÌïòÎ©¥ÏÑú ÏÇ¨ÎÇ¥ Î¨∏ÏÑú ÏöîÏïΩÍ∏∞Î•º Íµ¨ÏÑ±ÌïòÎ†§Î©¥ Ïñ¥Îñ§ ÏïÑÌÇ§ÌÖçÏ≤òÍ∞Ä Ï¢ãÏùÑÍπå?\")\n",
    "]\n",
    "\n",
    "prompt = ChatFormatter.to_chat_prompt(messages)\n",
    "print(prompt)  # For preview; pass to tokenizer when running inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216d049",
   "metadata": {},
   "source": [
    "## 12) ÏµúÏã†ÏÑ± Ïú†ÏßÄ ¬∑ Freshness Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452decd1",
   "metadata": {},
   "source": [
    "- **Ï£ºÍ∞Ñ Î≥¥Ï†ï SFT**: ÌóàÏö©Îêú Îâ¥Ïä§ API **Î©îÌÉÄÎç∞Ïù¥ÌÑ∞(Ï†úÎ™©/ÏöîÏïΩ/ÏÑπÏÖò)** ÏÉòÌîåÎßÅ ‚Üí Ïä§ÌÉÄÏùº Î≥¥Ï†ï.  \n",
    "- **ÎåÄÌôîÏ≤¥ ÏóÖÎç∞Ïù¥Ìä∏**: ÏµúÏã† Ï∂ïÏïΩÏñ¥/Ïã†Ï°∞Ïñ¥/Ïù¥Î™®Ìã∞ÏΩò ÏÇ¨Ï†Ñ Î∞òÏòÅ(Ïòà: „Ñ±„Ñ±, „Öá„Öã, „Öã„Öã, „Ñπ„Öá).  \n",
    "- **ÌöåÍ∑Ä ÌèâÍ∞Ä**: ÎèôÏùº ÏßÄÌëúÎ°ú before/after ÎπÑÍµê ‚Üí ÌòºÌï©ÎπÑ/Ïò®ÎèÑ/Ìå®ÎÑêÌã∞ ÌäúÎãù.\n",
    "\n",
    "- Weekly calibration SFT using **allowed news API metadata** for style;  \n",
    "- Update slang/emoji lexicons;  \n",
    "- Regression evals to track drift and adjust data mix/decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b9f2a",
   "metadata": {},
   "source": [
    "## 13) ÏïàÏ†Ñ/Ïª¥ÌîåÎùºÏù¥Ïñ∏Ïä§ ¬∑ Safety & Compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad24ef",
   "metadata": {},
   "source": [
    "- Îç∞Ïù¥ÌÑ∞ Ï∂úÏ≤ò/ÎùºÏù¥ÏÑ†Ïä§ ÌôïÏù∏(Î≤§ÏπòÎßàÌÅ¨, API, ÎÇ¥Î∂Ä Îç∞Ïù¥ÌÑ∞) ¬∑ Verify dataset/API licenses.\n",
    "- Í∞úÏù∏Ï†ïÎ≥¥ Ïä§ÌÅ¨Îü¨Îπô(ÌõàÎ†®/Î°úÍ∑∏/ÌèâÍ∞Ä Ï†Ñ) ¬∑ Scrub PII before training/logging/eval.\n",
    "- Ï†ÄÏûëÍ∂å/ÏïΩÍ¥Ä Ï§ÄÏàò(Í∏∞ÏÇ¨ **ÏõêÎ¨∏ ÎåÄÎüâ Ïû¨ÌïôÏäµ Í∏àÏßÄ**) ¬∑ Avoid mass training on full news articles.\n",
    "- Ï∂úÎ†• Í≤ÄÏ¶ù(Ïä§ÌÇ§Îßà/Í∏àÏπôÏñ¥/ÎØºÍ∞êÎèÑ Í∑úÏπô) ¬∑ Output validation & forbidden‚Äëterm filters.\n",
    "- Î≤ÑÏ†Ñ/ÌèâÍ∞Ä Î¶¨Ìè¨Ìä∏ Í¥ÄÎ¶¨ ¬∑ Version datasets/models and keep eval reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb8464b",
   "metadata": {},
   "source": [
    "## 14) Î¨∏Ï†úÌï¥Í≤∞ & Îã§Ïùå Îã®Í≥Ñ ¬∑ Troubleshooting & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee17077",
   "metadata": {},
   "source": [
    "- ÌòºÌï© ÎπÑÏú® ÌäúÎãù: (Îâ¥Ïä§:ÎåÄÌôî) 6:4 ‚Üí 7:3 ÎòêÎäî 5:5Î°ú Ï°∞Ï†ï  \n",
    "- LoRA ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞: r=8~16, Œ±=16~32, dropout=0.05~0.1  \n",
    "- ÏÑúÎπÑÏä§Ìôî: vLLM/llama.cpp ÏÑúÎπô + ÌÜ†ÌîΩ/Ïä§ÌÉÄÏùº ÎùºÏö∞ÌåÖ  \n",
    "- RAG Í≤∞Ìï©: ÏµúÏã† ÏÇ¨Ïã§ÏÑ± Î≥¥Í∞ïÏùÑ ÏúÑÌï¥ Îâ¥Ïä§/Î¨∏ÏÑú Ïù∏Îç±Ïä§ Í≤∞Ìï©  \n",
    "- A/B ÌÖåÏä§Ìä∏: ÌÜ§/Í∏∏Ïù¥/Ïù¥Î™®Ìã∞ÏΩò ÏÇ¨Ïö©Îüâ Îì± ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ Ï∏°Ï†ï\n",
    "\n",
    "- Tune mix ratios, run A/B tests, consider vLLM serving, and pair with RAG for factuality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
