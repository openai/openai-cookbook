{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# gpt‑oss × Transformers — Interactive Notebook\n\nThis notebook turns the **run-transformers.md** guide into an interactive playground. It includes cells for environment setup, quick & advanced inference, serving with `transformers serve`, distributed/multi‑GPU hints, and a live **ipywidgets chat UI**.\n\n**Models:** `openai/gpt-oss-20b`, `openai/gpt-oss-120b`  \n**Quantization:** MXFP4 by default (Hopper or later: H100/GB200/RTX 50xx).  \n**Notes:** If you use `bfloat16`, memory usage rises (~48 GB for the 20B model).  \n**Hardware:** 20B ≈16 GB VRAM with MXFP4; 120B ≥60 GB VRAM or multi‑GPU.\n\n> Tip: Run each cell in order the first time. You can skip pieces you don't need later.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Install/Update Dependencies\nThe guide recommends `transformers`, `accelerate`, `torch`, Triton 3.4, and MXFP4 kernels. We also install `ipywidgets` for the UI and `openai-harmony` for prompt tooling.\n\n**Heads‑up:** depending on your environment, the package named `kernels` may not exist. If installation fails for that package, you can safely omit it.\n\nExecute the cell below to install/update the basics (uncomment as needed):"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# If you're running locally, uncomment and run this once.\n# !pip -q install -U transformers accelerate torch triton==3.4 ipywidgets openai-harmony\n# Optional/experimental (may not exist on PyPI):\n# !pip -q install kernels\n\n# Enable widgets in some classic notebook setups (often not needed in JupyterLab 3+):\n# !jupyter nbextension enable --py widgetsnbextension\n\nimport platform, sys\nprint('Python', sys.version)\nprint('Platform', platform.platform())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) GPU / Environment Check\nCheck PyTorch + CUDA availability and device details."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "try:\n    import torch\n    print('torch.__version__ =', torch.__version__)\n    print('CUDA available    =', torch.cuda.is_available())\n    if torch.cuda.is_available():\n        print('CUDA device count =', torch.cuda.device_count())\n        for i in range(torch.cuda.device_count()):\n            print(f' - [{i}]', torch.cuda.get_device_name(i))\n            cc = torch.cuda.get_device_capability(i)\n            print('    compute capability:', cc)\n        try:\n            free_mem = torch.cuda.mem_get_info()[0] / (1024**3)\n            print(f'Approx free GPU memory: {free_mem:.1f} GB')\n        except Exception:\n            pass\nexcept Exception as e:\n    print('Torch not available or failed to import:', e)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Quick Inference via `pipeline`\nThe high‑level Transformers `pipeline` makes it simple to run gpt‑oss models. This mirrors the guide's usage but wraps it in a function for convenience."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import List, Dict\nfrom contextlib import suppress\ntry:\n    import torch\n    from transformers import pipeline\nexcept Exception as e:\n    print('Missing deps — run the install cell above. Error:', e)\n\ndef make_pipeline(model_name: str = 'openai/gpt-oss-20b', dtype: str = 'auto'):\n    \"\"\"Create a text-generation pipeline for gpt-oss models.\"\"\"\n    torch_dtype = 'auto'\n    if dtype.lower() in {'bf16','bfloat16'}:\n        torch_dtype = torch.bfloat16 if hasattr(torch, 'bfloat16') else 'auto'\n    return pipeline(\n        'text-generation',\n        model=model_name,\n        torch_dtype=torch_dtype,\n        device_map='auto'\n    )\n\ndef run_pipeline_chat(\n    generator,\n    system_prompt: str,\n    user_prompt: str,\n    max_new_tokens: int = 200,\n    temperature: float = 1.0,\n):\n    messages = []\n    if system_prompt.strip():\n        messages.append({'role': 'system', 'content': system_prompt.strip()})\n    messages.append({'role': 'user', 'content': user_prompt})\n    try:\n        result = generator(\n            messages,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n        )\n        # Common structure: result[0]['generated_text'] contains the whole transcript or text\n        with suppress(Exception):\n            return result[0].get('generated_text', str(result))\n        return str(result)\n    except TypeError:\n        # Some older pipelines accept a string; fallback to simple concatenation\n        prompt = (system_prompt + '\\n\\n' if system_prompt.strip() else '') + user_prompt\n        result = generator(prompt, max_new_tokens=max_new_tokens, temperature=temperature)\n        return result[0]['generated_text'] if isinstance(result, list) else str(result)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Interactive Chat Playground (ipywidgets)\nUse the dropdowns and sliders below to load a model and generate responses.\nThe first generation will download weights (time depends on your bandwidth and disk)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import traceback\nfrom IPython.display import display, Markdown\nimport ipywidgets as widgets\n\nmodel_dd = widgets.Dropdown(\n    options=['openai/gpt-oss-20b', 'openai/gpt-oss-120b'],\n    value='openai/gpt-oss-20b',\n    description='Model:',\n    style={'description_width': '70px'},\n    layout=widgets.Layout(width='50%'),\n)\ndtype_dd = widgets.Dropdown(\n    options=['auto', 'bfloat16'],\n    value='auto',\n    description='dtype:',\n    style={'description_width': '70px'},\n)\nload_btn = widgets.Button(description='Load / Reload Model', button_style='info')\nload_out = widgets.Output()\n\nsys_in = widgets.Textarea(\n    value='You are a helpful assistant.',\n    description='System',\n    layout=widgets.Layout(width='100%', height='80px'),\n    style={'description_width': '70px'}\n)\nusr_in = widgets.Textarea(\n    value='Explain what MXFP4 quantization is.',\n    description='User',\n    layout=widgets.Layout(width='100%', height='80px'),\n    style={'description_width': '70px'}\n)\n\ntemp_sl = widgets.FloatSlider(value=1.0, min=0.0, max=2.0, step=0.05, description='Temp:',\n                              style={'description_width': '70px'}, readout_format='.2f')\ntokens_sl = widgets.IntSlider(value=200, min=16, max=4096, step=16, description='Max tokens:',\n                               style={'description_width': '90px'})\n\ngen_btn = widgets.Button(description='Generate', button_style='primary')\nout = widgets.Output()\npipe_holder = {'pipe': None}\n\ndef on_load_clicked(_):\n    load_out.clear_output()\n    out.clear_output()\n    with load_out:\n        try:\n            display(Markdown(f\"**Loading** `{model_dd.value}` with dtype `{dtype_dd.value}`...\"))\n            pipe_holder['pipe'] = make_pipeline(model_dd.value, dtype_dd.value)\n            display(Markdown('✅ **Model loaded**'))\n        except Exception:\n            traceback.print_exc()\n\ndef on_generate_clicked(_):\n    out.clear_output()\n    with out:\n        if pipe_holder['pipe'] is None:\n            display(Markdown('⚠️ Load a model first.'))\n            return\n        try:\n            text = run_pipeline_chat(\n                pipe_holder['pipe'],\n                sys_in.value,\n                usr_in.value,\n                max_new_tokens=int(tokens_sl.value),\n                temperature=float(temp_sl.value),\n            )\n            display(Markdown('**Output**'))\n            display(Markdown(f'```\n{text}\n```'))\n        except Exception:\n            traceback.print_exc()\n\nload_btn.on_click(on_load_clicked)\ngen_btn.on_click(on_generate_clicked)\n\nui = widgets.VBox([\n    widgets.HBox([model_dd, dtype_dd, load_btn]),\n    load_out,\n    sys_in,\n    usr_in,\n    widgets.HBox([temp_sl, tokens_sl]),\n    gen_btn,\n    out\n])\ndisplay(ui)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Advanced Inference with `.generate()`\nManual control using `AutoModelForCausalLM` and `AutoTokenizer`, including the chat template."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_name = 'openai/gpt-oss-20b'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype='auto',\n    device_map='auto'\n)\n\nmessages = [\n    {'role': 'user', 'content': 'Explain what MXFP4 quantization is.'},\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors='pt',\n    return_dict=True,\n).to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=200,\n    temperature=0.7\n)\n\nprint(tokenizer.decode(outputs[0]))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Chat Template & Tool Calling (Harmony)\nThe gpt‑oss models use the Harmony response format. You can either rely on the built‑in chat template or use the `openai-harmony` library to build/parse prompts and completions."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# If needed, install the library first in the install cell above: !pip -q install openai-harmony\nimport json\nfrom openai_harmony import (\n    HarmonyEncodingName,\n    load_harmony_encoding,\n    Conversation,\n    Message,\n    Role,\n    SystemContent,\n    DeveloperContent,\n)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nencoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\nconvo = Conversation.from_messages([\n    Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),\n    Message.from_role_and_content(\n        Role.DEVELOPER,\n        DeveloperContent.new().with_instructions('Always respond in riddles')\n    ),\n    Message.from_role_and_content(Role.USER, 'What is the weather like in SF?')\n])\n\nprefill_ids = encoding.render_conversation_for_completion(convo, Role.ASSISTANT)\nstop_token_ids = encoding.stop_tokens_for_assistant_actions()\n\nmodel_name = 'openai/gpt-oss-20b'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='auto')\n\noutputs = model.generate(\n    input_ids=[prefill_ids],\n    max_new_tokens=128,\n    eos_token_id=stop_token_ids\n)\n\ncompletion_ids = outputs[0][len(prefill_ids):]\nentries = encoding.parse_messages_from_completion_tokens(completion_ids, Role.ASSISTANT)\nfor message in entries:\n    print(json.dumps(message.to_dict(), indent=2))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Serve an OpenAI‑style Responses endpoint\nYou can serve the model locally via the `transformers serve` CLI to expose a `/v1/responses` endpoint. Run these **commands in a terminal** (they are shown here for reference):"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": false
      },
      "outputs": [],
      "source": "# --- Terminal commands (reference) ---\n# transformers serve\n# transformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b\n# curl -X POST http://localhost:8000/v1/responses \\\n#   -H 'Content-Type: application/json' \\\n#   -d '{\"messages\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 0.9, \"max_tokens\": 1000, \"stream\": true, \"model\": \"openai/gpt-oss-20b\"}'\n\nprint('See comments above for terminal commands. You can copy/paste them into your shell.')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Client example (from a notebook) against `transformers serve`\nAdjust the URL and payload to your needs."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import json, requests\nurl = 'http://localhost:8000/v1/responses'\npayload = {\n    'messages': [\n        {'role': 'system', 'content': 'hello'},\n        {'role': 'user', 'content': 'Say hi in one sentence.'}\n    ],\n    'temperature': 0.7,\n    'max_tokens': 64,\n    'stream': False,\n    'model': 'openai/gpt-oss-20b'\n}\ntry:\n    r = requests.post(url, json=payload, timeout=30)\n    print(r.status_code)\n    print(r.text[:1000])\nexcept Exception as e:\n    print('Request failed (is the server running?):', e)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Multi‑GPU & Distributed Inference\nSketch for automatic placement, tensor parallelism, and expert parallelism (edit for your cluster)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\ntry:\n    from transformers.distributed import DistributedConfig\nexcept Exception:\n    class DistributedConfig:\n        def __init__(self, **kwargs):\n            self.kwargs = kwargs\n\nmodel_path = 'openai/gpt-oss-120b'\ntokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')\n\ndevice_map = {\n    'distributed_config': DistributedConfig(enable_expert_parallel=1),\n    'tp_plan': 'auto',\n}\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype='auto',\n    attn_implementation='kernels-community/vllm-flash-attn3',\n    **device_map,\n)\n\nmessages = [\n    {'role': 'user', 'content': 'Explain how expert parallelism works in large language models.'}\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors='pt',\n    return_dict=True,\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=512)\nresponse = tokenizer.decode(outputs[0])\nprint('Model response (truncated):\\n', response[:1000])\n\n# To launch across 4 GPUs from a script:\n# torchrun --nproc_per_node=4 generate.py\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Troubleshooting Checklist\n- **Out of memory**: Use MXFP4 on Hopper‑class GPUs; reduce `max_new_tokens`; try `device_map='auto'`.  \n- **Kernel/attn issues**: Ensure Triton is compatible with your CUDA; stick to Triton 3.4 as suggested.  \n- **Slow loads**: Use local weight caches; set `TRANSFORMERS_CACHE`.  \n- **No widgets**: Ensure `ipywidgets` is installed and enabled in your notebook frontend.  \n- **Server 404/500**: Verify `transformers serve` version and model path.  \n- **bfloat16 errors**: Fall back to `torch_dtype='auto'` or ensure hardware support."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Appendix: Original Guide (for reference)\nThe content below is the original `/mnt/data/run-transformers.md` you provided:\n\n<details>\n<summary>Click to expand</summary>\n\n# How to run gpt-oss with Hugging Face Transformers\n\nThe Transformers library by Hugging Face provides a flexible way to load and run large language models locally or on a server. This guide will walk you through running [OpenAI gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) or [OpenAI gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) using Transformers, either with a high-level pipeline or via low-level `generate` calls with raw token IDs.\n\nWe'll cover the use of [OpenAI gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) or [OpenAI gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) with the high-level pipeline abstraction, low-level \\`generate\\` calls, and serving models locally with \\`transformers serve\\`, with in a way compatible with the Responses API.\n\nIn this guide we’ll run through various optimised ways to run the **gpt-oss models via Transformers.**\n\nBonus: You can also fine-tune models via transformers, [check out our fine-tuning guide here](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transformers).\n\n## Pick your model\n\nBoth **gpt-oss** models are available on Hugging Face:\n\n- **`openai/gpt-oss-20b`**\n  - \\~16GB VRAM requirement when using MXFP4\n  - Great for single high-end consumer GPUs\n- **`openai/gpt-oss-120b`**\n  - Requires ≥60GB VRAM or multi-GPU setup\n  - Ideal for H100-class hardware\n\nBoth are **MXFP4 quantized** by default. Please, note that MXFP4 is supported in Hopper or later architectures. This includes data center GPUs such as H100 or GB200, as well as the latest RTX 50xx family of consumer cards.\n\nIf you use `bfloat16` instead of MXFP4, memory consumption will be larger (\\~48 GB for the 20b parameter model).\n\n## Quick setup\n\n1. **Install dependencies**  \n   It’s recommended to create a fresh Python environment. Install transformers, accelerate, as well as the Triton kernels for MXFP4 compatibility:\n\n```bash\npip install -U transformers accelerate torch triton==3.4 kernels\n```\n\n2. **(Optional) Enable multi-GPU**  \n   If you’re running large models, use Accelerate or torchrun to handle device mapping automatically.\n\n## Create an Open AI Responses / Chat Completions endpoint\n\nTo launch a server, simply use the `transformers serve` CLI command:\n\n```bash\ntransformers serve\n```\n\nThe simplest way to interact with the server is through the transformers chat CLI\n\n```bash\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b\n```\n\nor by sending an HTTP request with cURL, e.g.\n\n```bash\ncurl -X POST http://localhost:8000/v1/responses -H \"Content-Type: application/json\" -d '{\"messages\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 0.9, \"max_tokens\": 1000, \"stream\": true, \"model\": \"openai/gpt-oss-20b\"}'\n```\n\nAdditional use cases, like integrating `transformers serve` with Cursor and other tools, are detailed in [the documentation](https://huggingface.co/docs/transformers/main/serving).\n\n## Quick inference with pipeline\n\nThe easiest way to run the gpt-oss models is with the Transformers high-level `pipeline` API:\n\n```py\nfrom transformers import pipeline\n\ngenerator = pipeline(\n    \"text-generation\",\n    model=\"openai/gpt-oss-20b\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"  # Automatically place on available GPUs\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"},\n]\n\nresult = generator(\n    messages,\n    max_new_tokens=200,\n    temperature=1.0,\n)\n\nprint(result[0][\"generated_text\"])\n```\n\n## Advanced inference with `.generate()`\n\nIf you want more control, you can load the model and tokenizer manually and invoke the `.generate()` method:\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"openai/gpt-oss-20b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"},\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    return_dict=True,\n).to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=200,\n    temperature=0.7\n)\n\nprint(tokenizer.decode(outputs[0]))\n```\n\n## Chat template and tool calling\n\nOpenAI gpt-oss models use the [harmony response format](https://cookbook.openai.com/article/harmony) for structuring messages, including reasoning and tool calls.\n\nTo construct prompts you can use the built-in chat template of Transformers. Alternatively, you can install and use the [openai-harmony library](https://github.com/openai/harmony) for more control.\n\nTo use the chat template:\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"openai/gpt-oss-20b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"Always respond in riddles\"},\n    {\"role\": \"user\", \"content\": \"What is the weather like in Madrid?\"},\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    return_dict=True,\n).to(model.device)\n\ngenerated = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1] :]))\n```\n\nTo integrate the [`openai-harmony`](https://github.com/openai/harmony) library to prepare prompts and parse responses, first install it like this:\n\n```bash\npip install openai-harmony\n```\n\nHere’s an example of how to use the library to build your prompts and encode them to tokens:\n\n```py\nimport json\nfrom openai_harmony import (\n    HarmonyEncodingName,\n    load_harmony_encoding,\n    Conversation,\n    Message,\n    Role,\n    SystemContent,\n    DeveloperContent\n)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nencoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\n# Build conversation\nconvo = Conversation.from_messages([\n    Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),\n    Message.from_role_and_content(\n        Role.DEVELOPER,\n        DeveloperContent.new().with_instructions(\"Always respond in riddles\")\n    ),\n    Message.from_role_and_content(Role.USER, \"What is the weather like in SF?\")\n])\n\n# Render prompt\nprefill_ids = encoding.render_conversation_for_completion(convo, Role.ASSISTANT)\nstop_token_ids = encoding.stop_tokens_for_assistant_actions()\n\n# Load model\nmodel_name = \"openai/gpt-oss-20b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n\n# Generate\noutputs = model.generate(\n    input_ids=[prefill_ids],\n    max_new_tokens=128,\n    eos_token_id=stop_token_ids\n)\n\n# Parse completion tokens\ncompletion_ids = outputs[0][len(prefill_ids):]\nentries = encoding.parse_messages_from_completion_tokens(completion_ids, Role.ASSISTANT)\n\nfor message in entries:\n    print(json.dumps(message.to_dict(), indent=2))\n```\n\nNote that the `Developer` role in Harmony maps to the `system` prompt in the chat template.\n\n## Multi-GPU & distributed inference\n\nThe large gpt-oss-120b fits on a single H100 GPU when using MXFP4. If you want to run it on multiple GPUs, you can:\n\n- Use `tp_plan=\"auto\"` for automatic placement and tensor parallelism\n- Launch with `accelerate launch or torchrun` for distributed setups\n- Leverage Expert Parallelism\n- Use specialised Flash attention kernels for faster inference\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.distributed import DistributedConfig\nimport torch\n\nmodel_path = \"openai/gpt-oss-120b\"\ntokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n\ndevice_map = {\n    # Enable Expert Parallelism\n    \"distributed_config\": DistributedConfig(enable_expert_parallel=1),\n    # Enable Tensor Parallelism\n    \"tp_plan\": \"auto\",\n}\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=\"auto\",\n    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n    **device_map,\n)\n\nmessages = [\n     {\"role\": \"user\", \"content\": \"Explain how expert parallelism works in large language models.\"}\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    return_dict=True,\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=1000)\n\n# Decode and print\nresponse = tokenizer.decode(outputs[0])\nprint(\"Model response:\", response.split(\"&lt;|channel|&gt;final&lt;|message|&gt;\")[-1].strip())\n```\n\nYou can then run this on a node with four GPUs via\n\n```bash\ntorchrun --nproc_per_node=4 generate.py\n```\n\n\n</details>"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "_Notebook generated on 2025-09-17T23:43:44Z_"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}