{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run gpt-oss with Hugging Face Transformers\n",
    "\n",
    "The Transformers library by Hugging Face provides a flexible way to load and run large language models locally or on a server. This guide will walk you through running [OpenAI gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) or [OpenAI gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) using Transformers, either with a high-level pipeline or via low-level `generate` calls with raw token IDs.\n",
    "\n",
    "We'll cover the use of [OpenAI gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) or [OpenAI gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) with the high-level pipeline abstraction, low-level `generate` calls, and serving models locally with `transformers serve`, in a way compatible with the Responses API.\n",
    "\n",
    "In this guide we'll run through various optimised ways to run the **gpt-oss models via Transformers.**\n",
    "\n",
    "**Bonus:** You can also fine-tune models via transformers, [check out our fine-tuning guide here](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transformers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick your model\n",
    "\n",
    "Both **gpt-oss** models are available on Hugging Face:\n",
    "\n",
    "- **`openai/gpt-oss-20b`**\n",
    "  - ~16GB VRAM requirement when using MXFP4\n",
    "  - Great for single high-end consumer GPUs\n",
    "- **`openai/gpt-oss-120b`**\n",
    "  - Requires ≥60GB VRAM or multi-GPU setup\n",
    "  - Ideal for H100-class hardware\n",
    "\n",
    "Both are **MXFP4 quantized** by default. Please, note that MXFP4 is supported in Hopper or later architectures. This includes data center GPUs such as H100 or GB200, as well as the latest RTX 50xx family of consumer cards.\n",
    "\n",
    "If you use `bfloat16` instead of MXFP4, memory consumption will be larger (~48 GB for the 20b parameter model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick setup\n",
    "\n",
    "### 1. Install dependencies\n",
    "\n",
    "It's recommended to create a fresh Python environment. Install transformers, accelerate, as well as the Triton kernels for MXFP4 compatibility:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: The current version of HF Transformers has a glitch where an outdated torchvision dependency prevents transformers module from importing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers kernels torch accelerate torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick inference with pipeline\n",
    "\n",
    "The easiest way to run the gpt-oss models is with the Transformers high-level `pipeline` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b8482ed938472083ac6cd012156902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f00acfa64464f4b95eccabb37d2c2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa724229116c43908f2885b33987ca91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'analysisUser: \"Explain quantum mechanics clearly and concisely.\" They want a concise explanation, likely for general audience, but clear. They didn\\'t specify length. We can aim ~400-600 words. Should cover basics: wave-particle duality, uncertainty principle, superposition, measurement, entanglement. Also mention math: Schrödinger equation. Provide analogies. Avoid heavy math. Let\\'s produce.assistantfinal**Quantum mechanics in a nutshell**\\n\\nQuantum mechanics (QM) is the physics that describes how the tiniest parts of the universe—atoms, electrons, photons—behave. It replaced the classical picture of smooth, predictable motion with a world that is probabilistic, wavy, and sometimes oddly correlated. Here’s a quick tour of its core ideas:\\n\\n| Concept | What it means | Everyday picture |\\n|---------|---------------|------------------|\\n| **Wave‑particle duality** | Particles such as electrons behave both like tiny solid balls *and* like waves. | Think of a light‑bulb filament: it can shine (particle) and also create interference patterns like water waves. |\\n| **Probability waves (wavefunction)** | The state of a quantum system is described by a complex wavefunction, ψ'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example conversation with the model using the chat template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Be concise.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "print(\"Generated response:\")\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced inference with `.generate()`\n",
    "\n",
    "If you want more control, you can load the model and tokenizer manually and invoke the `.generate()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"openai/gpt-oss-20b\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example generation with manual control\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"},\n",
    "]\n",
    "\n",
    "# Apply chat template and tokenize\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "print(f\"Input tokens shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the full response\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nFull response:\")\n",
    "print(full_response)\n",
    "\n",
    "# Extract only the generated part\n",
    "input_length = inputs['input_ids'].shape[-1]\n",
    "generated_response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
    "print(\"\\nGenerated response only:\")\n",
    "print(generated_response)\n",
    "\n",
    "# Decode and extract only the assistant's final content (Harmony-style)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "final_marker = \"<|assistant|><|final|>\"\n",
    "end_marker = \"<|end|>\"\n",
    "if final_marker in text:\n",
    "    text = text.split(final_marker, 1)[1]\n",
    "if end_marker in text:\n",
    "    text = text.split(end_marker, 1)[0]\n",
    "print(\"Final:\", text.strip()[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31988a1",
   "metadata": {},
   "source": [
    "## Streaming tokens (prints only assistant **final**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, sys\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "def stream_final_only(model, tokenizer, messages, generate_kwargs):\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=False)\n",
    "\n",
    "    t = threading.Thread(target=model.generate, kwargs=dict(input_ids=inputs, streamer=streamer, **generate_kwargs))\n",
    "    t.start()\n",
    "\n",
    "    buf, printing_final = \"\", False\n",
    "    final_token, end_token = \"<|assistant|><|final|>\", \"<|end|>\"\n",
    "    for piece in streamer:\n",
    "        buf += piece\n",
    "        if not printing_final and final_token in buf:\n",
    "            printing_final = True\n",
    "            out = buf.split(final_token, 1)[1].replace(end_token, \"\")\n",
    "            sys.stdout.write(out); sys.stdout.flush()\n",
    "            buf = \"\"\n",
    "        elif printing_final:\n",
    "            sys.stdout.write(piece.replace(end_token, \"\")); sys.stdout.flush()\n",
    "    t.join()\n",
    "\n",
    "messages2 = [\n",
    "    {\"role\": \"system\", \"content\": \"Answer briefly.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What’s the difference between analysis and final channels?\"}\n",
    "]\n",
    "gen = dict(max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "stream_final_only(model, tokenizer, messages2, gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat template and tool calling\n",
    "\n",
    "OpenAI gpt-oss models use the [harmony response format](https://cookbook.openai.com/article/harmony) for structuring messages, including reasoning and tool calls.\n",
    "\n",
    "To construct prompts you can use the built-in chat template of Transformers. Alternatively, you can install and use the [openai-harmony library](https://github.com/openai/harmony) for more control.\n",
    "\n",
    "### Using the built-in chat template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with system prompt and chat template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Always respond in riddles\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather like in Madrid?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    "\n",
    "# Generate with the chat template\n",
    "generated = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Extract only the assistant's response\n",
    "response = tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(\"Assistant's riddle response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the openai-harmony library\n",
    "\n",
    "For more advanced control over the conversation format, you can use the openai-harmony library. \n",
    "\n",
    "First, install it:\n",
    "```bash\n",
    "pip install openai-harmony\n",
    "```\n",
    "\n",
    "**Note:** The following cell demonstrates the harmony library usage, but may require the actual library to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using openai-harmony library (requires installation)\n",
    "# Uncomment and run if you have openai-harmony installed\n",
    "\n",
    "'''\n",
    "import json\n",
    "from openai_harmony import (\n",
    "    HarmonyEncodingName,\n",
    "    load_harmony_encoding,\n",
    "    Conversation,\n",
    "    Message,\n",
    "    Role,\n",
    "    SystemContent,\n",
    "    DeveloperContent\n",
    ")\n",
    "\n",
    "# Load harmony encoding\n",
    "encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "\n",
    "# Build conversation\n",
    "convo = Conversation.from_messages([\n",
    "    Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),\n",
    "    Message.from_role_and_content(\n",
    "        Role.DEVELOPER,\n",
    "        DeveloperContent.new().with_instructions(\"Always respond in riddles\")\n",
    "    ),\n",
    "    Message.from_role_and_content(Role.USER, \"What is the weather like in SF?\")\n",
    "])\n",
    "\n",
    "# Render prompt\n",
    "prefill_ids = encoding.render_conversation_for_completion(convo, Role.ASSISTANT)\n",
    "stop_token_ids = encoding.stop_tokens_for_assistant_actions()\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(\n",
    "    input_ids=[prefill_ids],\n",
    "    max_new_tokens=128,\n",
    "    eos_token_id=stop_token_ids\n",
    ")\n",
    "\n",
    "# Parse completion tokens\n",
    "completion_ids = outputs[0][len(prefill_ids):]\n",
    "entries = encoding.parse_messages_from_completion_tokens(completion_ids, Role.ASSISTANT)\n",
    "\n",
    "for message in entries:\n",
    "    print(json.dumps(message.to_dict(), indent=2))\n",
    "'''\n",
    "\n",
    "print(\"Harmony library example code shown above (commented out)\")\n",
    "print(\"Note: The Developer role in Harmony maps to the system prompt in the chat template.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU & distributed inference\n",
    "\n",
    "The large gpt-oss-120b fits on a single H100 GPU when using MXFP4. If you want to run it on multiple GPUs, you can:\n",
    "\n",
    "- Use `tp_plan=\"auto\"` for automatic placement and tensor parallelism\n",
    "- Launch with `accelerate launch` or `torchrun` for distributed setups\n",
    "- Leverage Expert Parallelism\n",
    "- Use specialised Flash attention kernels for faster inference\n",
    "\n",
    "### Example multi-GPU setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU inference example (requires multiple GPUs)\n",
    "# This cell demonstrates the configuration but may not run on single GPU systems\n",
    "\n",
    "'''\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.distributed import DistributedConfig\n",
    "import torch\n",
    "\n",
    "model_path = \"openai/gpt-oss-120b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "\n",
    "device_map = {\n",
    "    # Enable Expert Parallelism\n",
    "    \"distributed_config\": DistributedConfig(enable_expert_parallel=1),\n",
    "    # Enable Tensor Parallelism\n",
    "    \"tp_plan\": \"auto\",\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "    **device_map,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "     {\"role\": \"user\", \"content\": \"Explain how expert parallelism works in large language models.\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=1000)\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(\"Model response:\", response.split(\"<|channel|>final<|message|>\")[-1].strip())\n",
    "'''\n",
    "\n",
    "print(\"Multi-GPU setup example shown above (commented out)\")\n",
    "print(\"\\nTo run this on a node with four GPUs, use:\")\n",
    "print(\"torchrun --nproc_per_node=4 your_script.py\")\n",
    "\n",
    "# Show current GPU configuration\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nCurrent setup:\")\n",
    "    print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\\nNo CUDA GPUs available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Examples and Tips\n",
    "\n",
    "### Memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Usage:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        cached = torch.cuda.memory_reserved(i) / 1e9\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        print(f\"  GPU {i}: {allocated:.1f}GB allocated, {cached:.1f}GB cached, {total:.1f}GB total\")\n",
    "\n",
    "# Clear cache if needed\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of processing multiple prompts\n",
    "batch_messages = [\n",
    "    [{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Explain quantum computing.\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"What is the future of AI?\"}]\n",
    "]\n",
    "\n",
    "print(\"Processing batch of prompts...\")\n",
    "for i, messages in enumerate(batch_messages):\n",
    "    print(f\"\\n--- Prompt {i+1} ---\")\n",
    "    print(f\"Input: {messages[0]['content']}\")\n",
    "\n",
    "    # You can use either the pipeline or manual generation here\n",
    "    # Using pipeline for simplicity:\n",
    "    if 'generator' in locals():\n",
    "        result = generator(\n",
    "            messages,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        print(f\"Output: {result[0]['generated_text'][-200:]}...\")  # Show last 200 chars\n",
    "    else:\n",
    "        print(\"Generator not available - run the pipeline example first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated various ways to run OpenAI's gpt-oss models using Hugging Face Transformers:\n",
    "\n",
    "1. **Quick setup** with required dependencies\n",
    "2. **Pipeline API** for simple, high-level inference\n",
    "3. **Manual generation** with `.generate()` for more control\n",
    "4. **Chat templates** for conversation-style interactions\n",
    "5. **Harmony library integration** for advanced message formatting\n",
    "6. **Multi-GPU configurations** for large-scale inference\n",
    "\n",
    "### Key takeaways:\n",
    "- Start with the pipeline API for quick experimentation\n",
    "- Use manual tokenization and generation for production deployments\n",
    "- Consider MXFP4 quantization for memory efficiency on compatible hardware\n",
    "- Leverage multi-GPU setups for the larger 120B model\n",
    "- Use proper chat templates for conversation-style applications\n",
    "\n",
    "### Next steps:\n",
    "- Explore fine-tuning capabilities\n",
    "- Set up serving endpoints for production use\n",
    "- Experiment with different sampling strategies\n",
    "- Integrate with your specific use case or application\n",
    "\n",
    "For more advanced topics, check out the [OpenAI Cookbook](https://cookbook.openai.com) and [Hugging Face Transformers documentation](https://huggingface.co/docs/transformers)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
