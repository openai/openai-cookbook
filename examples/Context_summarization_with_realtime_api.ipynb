{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Context Summarization with Realtime API\n",
    "\n",
    "Build an end‚Äëto‚Äëend **voice bot** that‚ÄØlistens to your mic, speaks back in real time and **summarises long conversations** so quality never drops.\n",
    "\n",
    "---\n",
    "\n",
    "## üèÉ‚Äç‚ôÇÔ∏è What You‚Äôll Build\n",
    "1. **Live microphone streaming** ‚Üí OpenAI *Realtime* (voice‚Äëto‚Äëvoice) endpoint.\n",
    "2. **Instant transcripts & speech playback** on every turn.\n",
    "3. **Conversation state container** that stores **every** user/assistant message.\n",
    "4. **Automatic ‚Äúcontext trim‚Äù** ‚Äì when the token window nears 32‚ÄØk, older turns are compressed into a summary.\n",
    "5. **Extensible design** you can adapt to support customer‚Äësupport bots, kiosks, or multilingual assistants.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook you can:\n",
    "\n",
    "| Skill | Why it matters |\n",
    "|-------|----------------|\n",
    "| Capture audio with `sounddevice` | Low‚Äëlatency input is critical for natural UX |\n",
    "| Use WebSockets with the OpenAI **Realtime** API | Streams beats polling for speed & simplicity |\n",
    "| Track token usage and detect when to summarize context | Prevents quality loss in long chats |\n",
    "| Summarise & prune history on‚Äëthe‚Äëfly | Keeps conversations coherent without manual resets |\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Prerequisites\n",
    "\n",
    "| Requirement | Details |\n",
    "|-------------|---------|\n",
    "| **Python¬†‚â•‚ÄØ3.10** | Will ensure that you don't hit any issues |\n",
    "| **OpenAI API key** | Set `OPENAI_API_KEY` in your shell or paste inline (*not ideal for prod*) |\n",
    "| Mic¬†+¬†speakers | Grant OS permission if prompted |\n",
    "\n",
    "\n",
    "**Need help setting up the key?**  \n",
    "> Follow the [official quick‚Äëstart guide](https://platform.openai.com/docs/quickstart#step-2-set-your-api-key).\n",
    "\n",
    "\n",
    "*Notes:*\n",
    "1. Why 32‚ÄØk? OpenAI‚Äôs public guidance notes that quality begins to decline well before the full 128‚ÄØk token limit; 32‚ÄØk is a conservative threshold observed in practice.\n",
    "\n",
    "2. Token window‚ÄØ=‚ÄØall tokens (words and audio tokens) the model currently keeps in memory for the session.x\n",
    "---\n",
    "\n",
    "### üöÄ One‚Äëliner install (run in a fresh cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Run¬†once¬†to¬†install¬†or¬†upgrade¬†dependencies (comment out if already installed)\n",
    "# !pip install --upgrade openai websockets sounddevice simpleaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Essential imports & constants\n",
    "import os, asyncio, base64, json, sys, itertools\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Literal\n",
    "\n",
    "import asyncio, base64, io, json, os, sys, wave, pathlib\n",
    "from typing import List\n",
    "\n",
    "import numpy as np, soundfile as sf, resampy, websockets, openai\n",
    "\n",
    "import sounddevice as sd         # microphone capture\n",
    "import simpleaudio               # speaker playback\n",
    "import websockets                # WebSocket client\n",
    "import openai                    # OpenAI¬†Python¬†SDK >=¬†1.14.0\n",
    "\n",
    "#¬†Audio/config¬†knobs\n",
    "SAMPLE_RATE_HZ    = 24_000   #¬†Required by pcm16\n",
    "CHUNK_DURATION_MS = 40       #¬†‚âà¬†latency granularity\n",
    "BYTES_PER_SAMPLE  = 2        #¬†pcm16 = 2¬†bytes/sample\n",
    "SUMMARY_TRIGGER   = 2_000    #¬†Summarise when context¬†‚â•¬†this\n",
    "KEEP_LAST_TURNS   = 4        #¬†Keep these turns verbatim\n",
    "SUMMARY_MODEL     = \"gpt-4o-mini\"  #¬†Cheaper, fast summariser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Set¬†your¬†API¬†key¬†safely\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found ‚Äì please set env var or edit this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2‚ÄØ¬∑‚ÄØKey Concepts Behind the Realtime‚ÄØVoice‚ÄØAPI\n",
    "\n",
    "This section gives you the mental model you‚Äôll need before diving into code.  Skim it now; refer back whenever something in the notebook feels ‚Äúmagic‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1¬†Realtime¬†vs¬†Chat¬†Completions¬†‚Äî¬†Why WebSockets?\n",
    "\n",
    "|  | **Chat‚ÄØCompletions¬†(HTTP)** | **Realtime¬†(WebSocket)** |\n",
    "|---|---|---|\n",
    "| Transport | Stateless request‚ÄØ‚Üí‚ÄØresponse | Persistent, bi‚Äëdirectional socket |\n",
    "| Best for | Plain text or batched jobs | *Live* audio + incremental text |\n",
    "| Latency model | 1‚ÄØRTT per message | Sub‚Äë200‚ÄØms deltas during one open session |\n",
    "| Event types | *None* (single JSON) | `session.*`, `input_audio_buffer.append`, `response.*`, ‚Ä¶ |\n",
    "\n",
    "\n",
    "**Flow**: you talk ‚ñ∏ server transcribes ‚ñ∏ assistant replies ‚ñ∏ you talk again.  \n",
    "> Mirrors natural conversation while keeping event handling simple.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2¬†Audio Encoding Fundamentals\n",
    "\n",
    "| Parameter | Value | Why it matters |\n",
    "|-----------|-------|----------------|\n",
    "| **Format** | PCM‚Äë16 (signed‚ÄØ16‚Äëbit) | Widely supported; no compression delay |\n",
    "| **Sample¬†rate** | 24‚ÄØkHz | Required by Realtime endpoint |\n",
    "| **Chunk size** | ‚âà‚ÄØ40‚ÄØms | Lower chunk‚ÄØ‚Üí‚ÄØsnappier response¬†‚Üî¬†higher packet overhead |\n",
    "\n",
    "`chunk_bytes  = sample_rate * bytes_per_sample * chunk_duration_s`\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3¬†Token Context Windows\n",
    "\n",
    "* GPT‚Äë4o‚ÄØRealtime accepts **up to‚ÄØ128‚ÄØK tokens** in theory.  \n",
    "* In practice, answer quality starts to drift around **‚âà‚ÄØ32‚ÄØK tokens**.  \n",
    "* Every user/assistant turn consumes tokens ‚Üí the window **only grows**.\n",
    "* **Strategy**: Summarise older turns into a single assistant message, keep the last few verbatim turns, and continue.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4¬†Conversation State\n",
    "\n",
    "Instead of scattered globals, the notebook uses with one **state object**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Turn:\n",
    "    \"\"\"One utterance in the dialogue (user **or** assistant).\"\"\"\n",
    "    role: Literal[\"user\", \"assistant\"]\n",
    "    item_id: str                    # Server‚Äëassigned identifier\n",
    "    text: str | None = None         # Filled once transcript is ready\n",
    "\n",
    "@dataclass\n",
    "class ConversationState:\n",
    "    \"\"\"All mutable data the session needs ‚Äî nothing more, nothing less.\"\"\"\n",
    "    history: List[Turn] = field(default_factory=list)         # Ordered log\n",
    "    waiting: dict[str, asyncio.Future] = field(default_factory=dict)  # Pending transcript fetches\n",
    "    summary_count: int = 0\n",
    "\n",
    "    latest_tokens: int = 0          # Window size after last reply\n",
    "    summarising: bool = False       # Guard so we don‚Äôt run two summaries at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick helper to peek at the transcript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_history(state) -> None:\n",
    "    \"\"\"Pretty-print the running transcript so far.\"\"\"\n",
    "    print(\"‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\")\n",
    "    for turn in state.history:\n",
    "        text_preview = (turn.text or \"\").strip().replace(\"\\n\", \" \")\n",
    "        print(f\"[{turn.role:<9}] {text_preview}  ({turn.item_id})\")\n",
    "    print(\"‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3‚ÄØ¬∑‚ÄØToken‚ÄØUtilisation¬†‚Äì‚ÄØText¬†vs¬†Voice\n",
    "\n",
    "Large‚Äëtoken windows are precious: every extra token you burn costs latency‚ÄØ+‚ÄØmoney.  \n",
    "For **audio** the bill climbs much faster than for plain text because amplitude, timing, and other acoustic details must be represented.\n",
    "\n",
    "*Rule of thumb*: **1 word of text ‚âà‚ÄØ1‚ÄØtoken**, but **1‚ÄØsecond of 24‚ÄëkHz PCM‚Äë16 ‚âà‚ÄØ~150‚ÄØaudio tokens**.  \n",
    "In practice you‚Äôll often see **‚âà‚ÄØ10‚ÄØ√ó** more tokens for the *same* sentence spoken aloud than typed.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1¬†Hands‚Äëon¬†comparison¬†üìä\n",
    "\n",
    "The cells below:\n",
    "\n",
    "1. **Sends `TEXT` to Chat‚ÄØCompletions** ‚Üí reads `prompt_tokens`.  \n",
    "2. **Turns the same `TEXT` into speech** with TTS.  \n",
    "3. **Feeds the speech back into the Realtime API Transcription endpoint** ‚Üí reads `audio input tokens`.  \n",
    "4. Prints a ratio so you can see the multiplier on *your* hardware / account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Text prompt tokens        : 42\n",
      "üîä Audio length (s)          : 10.75\n"
     ]
    }
   ],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë 3 ¬∑ Token Utilisation ‚Äì Text‚ÄØvs‚ÄØVoice                            ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "TEXT = (\n",
    "    \"Hello there, I am measuring tokens for text versus voice because we want to better compare the number of tokens used when sending a message as text versus when converting it to speech..\"\n",
    ")\n",
    "STT_MODEL   = \"gpt-4o-transcribe\"\n",
    "TTS_MODEL   = \"gpt-4o-mini-tts\"\n",
    "RT_MODEL    = \"gpt-4o-realtime-preview\"          # S2S model\n",
    "VOICE       = \"shimmer\"\n",
    "\n",
    "TARGET_SR   = 24_000\n",
    "PCM_SCALE   = 32_767\n",
    "CHUNK_MS    = 120                                # stream step\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {openai.api_key}\",\n",
    "    \"OpenAI-Beta\":   \"realtime=v1\",\n",
    "}\n",
    "\n",
    "show = lambda l, v: print(f\"{l:<28}: {v}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def float_to_pcm16(x: np.ndarray) -> bytes:\n",
    "    return (np.clip(x, -1, 1) * PCM_SCALE).astype(\"<i2\").tobytes()\n",
    "\n",
    "def chunk_pcm(pcm: bytes, ms: int = CHUNK_MS) -> List[bytes]:\n",
    "    step = TARGET_SR * 2 * ms // 1000\n",
    "    return [pcm[i:i + step] for i in range(0, len(pcm), step)]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1 ¬∑ Count text tokens ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "chat = openai.chat.completions.create(\n",
    "    model=CHAT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": TEXT}],\n",
    "    max_tokens=1,\n",
    "    temperature=0,\n",
    ")\n",
    "text_tokens = chat.usage.prompt_tokens\n",
    "show(\"üìÑ Text prompt tokens\", text_tokens)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2 ¬∑ Synthesis to WAV & PCM16 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "wav_bytes = openai.audio.speech.create(\n",
    "    model=TTS_MODEL, input=TEXT, voice=VOICE, response_format=\"wav\"\n",
    ").content\n",
    "\n",
    "with wave.open(io.BytesIO(wav_bytes)) as w:\n",
    "    pcm_bytes = w.readframes(w.getnframes())\n",
    "duration_sec = len(pcm_bytes) / (2 * TARGET_SR)\n",
    "show(\"üîä Audio length (s)\", f\"{duration_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Audio input tokens        : 105\n",
      "‚öñÔ∏è  Audio/Text ratio        : 2.5√ó\n",
      "\n",
      "‚âà9 audio‚Äëtokens‚ÄØ/‚ÄØsec vs ‚âà1 token‚ÄØ/‚ÄØword.\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ 3 ¬∑ Realtime streaming & token harvest ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "async def count_audio_tokens(pcm: bytes) -> int:\n",
    "    url = f\"wss://api.openai.com/v1/realtime?model={RT_MODEL}\"\n",
    "    chunks = chunk_pcm(pcm)\n",
    "\n",
    "    async with websockets.connect(url, extra_headers=HEADERS,\n",
    "                                  max_size=1 << 24) as ws:\n",
    "\n",
    "        # Wait for session.created\n",
    "        while json.loads(await ws.recv())[\"type\"] != \"session.created\":\n",
    "            pass\n",
    "\n",
    "        # Configure modalities + voice\n",
    "        await ws.send(json.dumps({\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"voice\": VOICE,\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"input_audio_transcription\": {\"model\": STT_MODEL},\n",
    "            }\n",
    "        }))\n",
    "\n",
    "        # Stream user audio chunks (no manual commit; server VAD handles it)\n",
    "        for c in chunks:\n",
    "            await ws.send(json.dumps({\n",
    "                \"type\": \"input_audio_buffer.append\",\n",
    "                \"audio\": base64.b64encode(c).decode(),\n",
    "            }))\n",
    "\n",
    "        async for raw in ws:\n",
    "            ev = json.loads(raw)\n",
    "            t = ev.get(\"type\")\n",
    "\n",
    "            if t == \"response.done\":\n",
    "                return ev[\"response\"][\"usage\"]\\\n",
    "                         [\"input_token_details\"][\"audio_tokens\"]\n",
    "\n",
    "audio_tokens = await count_audio_tokens(pcm_bytes)\n",
    "show(\"üé§ Audio input tokens\", audio_tokens)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4 ¬∑ Comparison ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ratio = audio_tokens / text_tokens if text_tokens else float(\"inf\")\n",
    "show(\"‚öñÔ∏è  Audio/Text ratio\", f\"{ratio:.1f}√ó\")\n",
    "print(f\"\\n‚âà{int(audio_tokens/duration_sec)} audio‚Äëtokens‚ÄØ/‚ÄØsec vs ‚âà1 token‚ÄØ/‚ÄØword.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This toy example uses a short input, but as transcripts get longer, the difference between text token count and voice token count grows substantially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3‚ÄØ¬∑‚ÄØStreaming Audio\n",
    "We‚Äôll stream raw PCM‚Äë16 microphone data straight into the Realtime API.\n",
    "\n",
    "The pipeline is: mic ‚îÄ‚ñ∫ async.Queue ‚îÄ‚ñ∫ WebSocket ‚îÄ‚ñ∫ Realtime API\n",
    "\n",
    "### 3.1¬†Capture Microphone Input\n",
    "We‚Äôll start with a coroutine that:\n",
    "\n",
    "* Opens the default mic at **24‚ÄØkHz, mono, PCM‚Äë16** (one of the [format](https://platform.openai.com/docs/api-reference/realtime-sessions/create#realtime-sessions-create-input_audio_format) Realtime accepts).  \n",
    "* Slices the stream into **‚âà‚ÄØ40‚ÄØms** blocks.  \n",
    "* Dumps each block into an `asyncio.Queue` so another task (next section) can forward it to OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë 3.1 ¬∑ Microphone ‚Üí async.Queue                                   ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "import asyncio, sys\n",
    "import sounddevice as sd\n",
    "\n",
    "# ‚îÄ‚îÄ Audio constants (match Realtime requirements) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "SAMPLE_RATE_HZ     = 24_000        # 24‚ÄëkHz mono\n",
    "CHUNK_DURATION_MS  = 40            # ‚âà40‚Äëms frames\n",
    "QUEUE_MAXSIZE      = 32            # Back‚Äëpressure buffer\n",
    "\n",
    "async def mic_to_queue(pcm_queue: asyncio.Queue[bytes]) -> None:\n",
    "    \"\"\"\n",
    "    Capture raw PCM‚Äë16 microphone audio and push ~CHUNK_DURATION_MS chunks\n",
    "    to *pcm_queue* until the surrounding task is cancelled.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pcm_queue : asyncio.Queue[bytes]\n",
    "        Destination queue for PCM‚Äë16 frames (little‚Äëendian int16).\n",
    "    \"\"\"\n",
    "    blocksize = int(SAMPLE_RATE_HZ * CHUNK_DURATION_MS / 1000)\n",
    "\n",
    "    def _callback(indata, _frames, _time, status):\n",
    "        if status:                               # XRuns, device changes, etc.\n",
    "            print(\"‚ö†Ô∏è\", status, file=sys.stderr)\n",
    "        try:\n",
    "            pcm_queue.put_nowait(bytes(indata))  # 1‚Äëshot enqueue\n",
    "        except asyncio.QueueFull:\n",
    "            # Drop frame if upstream (WebSocket) can‚Äôt keep up.\n",
    "            pass\n",
    "\n",
    "    # RawInputStream is synchronous; wrap in context manager to auto‚Äëclose.\n",
    "    with sd.RawInputStream(\n",
    "        samplerate=SAMPLE_RATE_HZ,\n",
    "        blocksize=blocksize,\n",
    "        dtype=\"int16\",\n",
    "        channels=1,\n",
    "        callback=_callback,\n",
    "    ):\n",
    "        try:\n",
    "            # Keep coroutine alive until cancelled by caller.\n",
    "            await asyncio.Event().wait()\n",
    "        finally:\n",
    "            print(\"‚èπÔ∏è  Mic stream closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2¬†Send Audio Chunks to the API\n",
    "\n",
    "Our mic task is now filling an `asyncio.Queue` with raw PCM‚Äë16 blocks.  \n",
    "Next step: pull chunks off that queue, **base‚Äë64‚ÄØencode** them (the protocol requires JSON‚Äësafe text), and ship each block to the Realtime WebSocket as an `input_audio_buffer.append` event.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b64 = lambda blob: base64.b64encode(blob).decode()\n",
    "\n",
    "async def queue_to_websocket(pcm_queue: asyncio.Queue[bytes], ws):\n",
    "    \"\"\"Read audio chunks from queue and send as JSON events.\"\"\"\n",
    "    try:\n",
    "        while (chunk := await pcm_queue.get()) is not None:\n",
    "            await ws.send(json.dumps({\n",
    "                \"type\": \"input_audio_buffer.append\",\n",
    "                \"audio\": b64(chunk),\n",
    "            }))\n",
    "    except websockets.ConnectionClosed:\n",
    "        print(\"WebSocket closed ‚Äì stopping uploader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3¬†Handle Incoming Events \n",
    "Once audio reaches the server, the Realtime API pushes a stream of JSON events back over the **same** WebSocket.  \n",
    "Understanding these events is critical for:\n",
    "\n",
    "* Printing live transcripts  \n",
    "* Playing incremental audio back to the user  \n",
    "* Keeping an accurate `ConversationState` so context trimming works later  \n",
    "\n",
    "| Event¬†type | Typical timing | What you should do with it |\n",
    "|------------|----------------|----------------------------|\n",
    "| **`session.created`** | Immediately after connection | Verify the handshake; stash the `session_id` if you need it for server logs. |\n",
    "| **`conversation.item.created`** (user) | Right after the user stops talking | Place a *placeholder* `Turn` in `state.history`. Transcript may still be `null`. |\n",
    "| **`conversation.item.retrieved`** | A few hundred‚ÄØms later | Fill in any missing user transcript once STT completes. |\n",
    "| **`response.audio.delta`** | Streaming chunks while the assistant speaks | Append bytes to a local buffer, play them (low‚Äëlatency) as they arrive. |\n",
    "| **`response.done`** | After final assistant token | Add assistant text + usage stats, update `state.latest_tokens`. |\n",
    "| **`conversation.item.deleted`** | Whenever you prune old turns | Remove superseded items from `conversation.item`. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4‚ÄØ¬∑‚ÄØDynamic Context Management¬†&¬†Summarisation\n",
    "\n",
    "### 4.1¬†Detect When to Summarise\n",
    "We monitor latest_tokens returned in response.done. When it exceeds SUMMARY_TRIGGER and we have more than KEEP_LAST_TURNS, we spin up a background summarisation coroutine.\n",
    "\n",
    "### 4.2¬†Generate¬†&¬†Insert a Summary\n",
    "We will be summarizing the conversation messages up to N-4 into french. We will later ask the Voice agent what language was the summary to test if the Summary insertion into Realtime API Conversation Context was successfull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_summary_llm(text: str) -> str:\n",
    "    \"\"\"Call a lightweight model to summarise `text`.\"\"\"\n",
    "    resp = await asyncio.to_thread(lambda: openai.chat.completions.create(\n",
    "        model=SUMMARY_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarise in French the following conversation \"\n",
    "                            \"in one concise paragraph so it can be used as \"\n",
    "                            \"context for future dialogue.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "    ))\n",
    "    return resp.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def summarise_and_prune(ws, state):\n",
    "    \"\"\"Summarise old turns, delete them server‚Äëside, and prepend a single summary\n",
    "    turn locally + remotely.\"\"\"\n",
    "    state.summarising = True\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è  Token window ‚âà{state.latest_tokens} ‚â• {SUMMARY_TRIGGER}. Summarising‚Ä¶\",\n",
    "    )\n",
    "    old_turns, recent_turns = state.history[:-KEEP_LAST_TURNS], state.history[-KEEP_LAST_TURNS:]\n",
    "    convo_text = \"\\n\".join(f\"{t.role}: {t.text}\" for t in old_turns if t.text)\n",
    "    \n",
    "    if not convo_text:\n",
    "        print(\"Nothing to summarise (transcripts still pending).\")\n",
    "        state.summarising = False\n",
    "\n",
    "    summary_text = await run_summary_llm(convo_text) if convo_text else \"\"\n",
    "    state.summary_count += 1\n",
    "    summary_id = f\"sum_{state.summary_count:03d}\"\n",
    "    state.history[:] = [Turn(\"assistant\", summary_id, summary_text)] + recent_turns\n",
    "    \n",
    "    print_history(state)    \n",
    "\n",
    "    #¬†Create summary on server\n",
    "    await ws.send(json.dumps({\n",
    "        \"type\": \"conversation.item.create\",\n",
    "        \"previous_item_id\": \"root\",\n",
    "        \"item\": {\n",
    "            \"id\": summary_id,\n",
    "            \"type\": \"message\",\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": summary_text}],\n",
    "        },\n",
    "    }))\n",
    "\n",
    "    #¬†Delete old items\n",
    "    for turn in old_turns:\n",
    "        await ws.send(json.dumps({\n",
    "            \"type\": \"conversation.item.delete\",\n",
    "            \"item_id\": turn.item_id,\n",
    "        }))\n",
    "\n",
    "    print(f\"‚úÖ Summary inserted ({summary_id})\")\n",
    "    \n",
    "    state.summarising = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_full_item(\n",
    "    ws, item_id: str, state: ConversationState, attempts: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Ask the server for a full conversation item; retry up to 5√ó if the\n",
    "    transcript field is still null.  Resolve the waiting future when done.\n",
    "    \"\"\"\n",
    "    # If there is already a pending fetch, just await it\n",
    "    if item_id in state.waiting:\n",
    "        return await state.waiting[item_id]\n",
    "\n",
    "    fut = asyncio.get_running_loop().create_future()\n",
    "    state.waiting[item_id] = fut\n",
    "\n",
    "    await ws.send(json.dumps({\n",
    "        \"type\": \"conversation.item.retrieve\",\n",
    "        \"item_id\": item_id,\n",
    "    }))\n",
    "    item = await fut\n",
    "\n",
    "    # If transcript still missing retry (max 5√ó)\n",
    "    if attempts < 5 and not item.get(\"content\", [{}])[0].get(\"transcript\"):\n",
    "        await asyncio.sleep(0.4 * attempts)\n",
    "        return await fetch_full_item(ws, item_id, state, attempts + 1)\n",
    "\n",
    "    # Done ‚Äì remove the marker\n",
    "    state.waiting.pop(item_id, None)\n",
    "    return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5‚ÄØ¬∑‚ÄØEnd‚Äëto‚ÄëEnd Workflow Demonstration\n",
    "\n",
    "Run the two cells below to launch an interactive session. Press Ctrl‚ÄëC¬†to stop recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# üé§ Realtime session                                                          #\n",
    "# --------------------------------------------------------------------------- #\n",
    "async def realtime_session(model=\"gpt-4o-realtime-preview\", voice=\"shimmer\", enable_playback=True):\n",
    "    \"\"\"\n",
    "    Main coroutine: connects to the Realtime endpoint, spawns helper tasks,\n",
    "    and processes incoming events in a big async‚Äëfor loop.\n",
    "    \"\"\"\n",
    "    state = ConversationState()  # Reset state for each run\n",
    "\n",
    "    pcm_queue: asyncio.Queue[bytes] = asyncio.Queue()\n",
    "    assistant_audio: List[bytes] = []\n",
    "\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    # Open the WebSocket connection to the Realtime API                       #\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    url = f\"wss://api.openai.com/v1/realtime?model={model}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {openai.api_key}\", \"OpenAI-Beta\": \"realtime=v1\"}\n",
    "\n",
    "    async with websockets.connect(url, extra_headers=headers, max_size=1 << 24) as ws:\n",
    "        # ------------------------------------------------------------------- #\n",
    "        # Wait until server sends session.created                             #\n",
    "        # ------------------------------------------------------------------- #\n",
    "        while json.loads(await ws.recv())[\"type\"] != \"session.created\":\n",
    "            pass\n",
    "        print(\"session.created ‚úÖ\")\n",
    "\n",
    "        # ------------------------------------------------------------------- #\n",
    "        # Configure session: voice, modalities, audio formats, transcription  #\n",
    "        # ------------------------------------------------------------------- #\n",
    "        await ws.send(json.dumps({\n",
    "            \"type\": \"session.update\",\n",
    "            \"session\": {\n",
    "                \"voice\": voice,\n",
    "                \"modalities\": [\"audio\", \"text\"],\n",
    "                \"input_audio_format\": \"pcm16\",\n",
    "                \"output_audio_format\": \"pcm16\",\n",
    "                \"input_audio_transcription\": {\"model\": \"gpt-4o-transcribe\"},\n",
    "            },\n",
    "        }))\n",
    "\n",
    "        # ------------------------------------------------------------------- #\n",
    "        # Launch background tasks: mic capture ‚Üí queue ‚Üí websocket            #\n",
    "        # ------------------------------------------------------------------- #\n",
    "        mic_task = asyncio.create_task(mic_to_queue(pcm_queue))\n",
    "        upl_task = asyncio.create_task(queue_to_websocket(pcm_queue, ws))\n",
    "\n",
    "        print(\"üéôÔ∏è¬†Speak now¬†(Ctrl‚ÄëC to quit)‚Ä¶\")\n",
    "\n",
    "        try:\n",
    "            # ------------------------------------------------------------------- #\n",
    "            # Main event loop: process incoming events from the websocket         #\n",
    "            # ------------------------------------------------------------------- #\n",
    "            async for event_raw in ws:\n",
    "                event = json.loads(event_raw)\n",
    "                etype = event[\"type\"]\n",
    "\n",
    "                # --------------------------------------------------------------- #\n",
    "                # User just spoke ‚á¢ conversation.item.created (role = user)        #\n",
    "                # --------------------------------------------------------------- #\n",
    "                if etype == \"conversation.item.created\" and event[\"item\"][\"role\"] == \"user\":\n",
    "                    item = event[\"item\"]\n",
    "                    text = None\n",
    "                    if item[\"content\"]:\n",
    "                        text = item[\"content\"][0].get(\"transcript\")\n",
    "                    \n",
    "                    state.history.append(Turn(\"user\", event[\"item\"][\"id\"], text))\n",
    "                    \n",
    "                    # If transcript not yet available, fetch it later\n",
    "                    if text is None:\n",
    "                        asyncio.create_task(fetch_full_item(ws, item[\"id\"], state))\n",
    "\n",
    "                # --------------------------------------------------------------- #\n",
    "                # Transcript fetched ‚á¢ conversation.item.retrieved                 #\n",
    "                # --------------------------------------------------------------- #\n",
    "                elif etype == \"conversation.item.retrieved\":\n",
    "                    content = event[\"item\"][\"content\"][0]\n",
    "                    # Fill missing transcript in history\n",
    "                    for t in state.history:\n",
    "                        if t.item_id == event[\"item\"][\"id\"]:\n",
    "                            t.text = content.get(\"transcript\")\n",
    "                            break\n",
    "\n",
    "                # --------------------------------------------------------------- #\n",
    "                # Assistant audio arrives in deltas                               #\n",
    "                # --------------------------------------------------------------- #\n",
    "                elif etype == \"response.audio.delta\":\n",
    "                    assistant_audio.append(base64.b64decode(event[\"delta\"]))\n",
    "\n",
    "                # --------------------------------------------------------------- #\n",
    "                # Assistant reply finished ‚á¢ response.done                        #\n",
    "                # --------------------------------------------------------------- #\n",
    "                elif etype == \"response.done\":\n",
    "                    for item in event[\"response\"][\"output\"]:\n",
    "                        if item[\"role\"] == \"assistant\":\n",
    "                            txt = item[\"content\"][0][\"transcript\"]\n",
    "                            state.history.append(Turn(\"assistant\", item[\"id\"], txt))\n",
    "                            # print(f\"\\nü§ñ {txt}\\n\")\n",
    "                    state.latest_tokens = event[\"response\"][\"usage\"][\"total_tokens\"]\n",
    "                    print(f\"‚Äî‚Äî response.done  (window ‚âà{state.latest_tokens} tokens) ‚Äî‚Äî\")\n",
    "                    print_history(state)\n",
    "                    \n",
    "                    # Fetch any still‚Äëmissing user transcripts\n",
    "                    for turn in state.history:\n",
    "                        if (turn.role == \"user\"\n",
    "                            and turn.text is None\n",
    "                            and turn.item_id not in state.waiting):\n",
    "                            asyncio.create_task(\n",
    "                                fetch_full_item(ws, turn.item_id, state)\n",
    "                            )\n",
    "\n",
    "                    # Playback collected audio once reply completes\n",
    "                    if enable_playback and assistant_audio:\n",
    "                        simpleaudio.play_buffer(b\"\".join(assistant_audio), 1, BYTES_PER_SAMPLE, SAMPLE_RATE_HZ)\n",
    "                        assistant_audio.clear()\n",
    "\n",
    "                    # Summarise if context too large ‚Äì fire in background so we don't block dialogue\n",
    "                    if state.latest_tokens >= SUMMARY_TRIGGER and len(state.history) > KEEP_LAST_TURNS and not state.summarising:\n",
    "                        asyncio.create_task(summarise_and_prune(ws, state))\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopping‚Ä¶\")\n",
    "        finally:\n",
    "            mic_task.cancel()\n",
    "            await pcm_queue.put(None)\n",
    "            await upl_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Run¬†the realtime¬†session (this cell blocks until you stop it)\n",
    "await realtime_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```raw\n",
    "üéôÔ∏è Speak now (Ctrl‚ÄëC to quit)‚Ä¶\n",
    "‚Äî‚Äî response.done  (window ‚âà228 tokens) ‚Äî‚Äî\n",
    "‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "[user     ]   (item_BTfKeRGKfjQ976Ojmgpl6)\n",
    "[assistant] Hey there! Not much, just here to help out. What's up with you?  (item_BTfKeuSJAlvr8WMqewiOo)\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "‚Äî‚Äî response.done  (window ‚âà0 tokens) ‚Äî‚Äî\n",
    "‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "[user     ] Hey, what's up?  (item_BTfKeRGKfjQ976Ojmgpl6)\n",
    "[assistant] Hey there! Not much, just here to help out. What's up with you?  (item_BTfKeuSJAlvr8WMqewiOo)\n",
    "[user     ]   (item_BTfMI1PSozC8zYfxBGDEA)\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "‚Äî‚Äî response.done  (window ‚âà422 tokens) ‚Äî‚Äî\n",
    "‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "[user     ] Hey, what's up?  (item_BTfKeRGKfjQ976Ojmgpl6)\n",
    "[assistant] Hey there! Not much, just here to help out. What's up with you?  (item_BTfKeuSJAlvr8WMqewiOo)\n",
    "[user     ]   (item_BTfMI1PSozC8zYfxBGDEA)\n",
    "[user     ]   (item_BTfMIbULFByNpzbBMjP18)\n",
    "[assistant] Sure thing! Why don't scientists trust atoms? Because they make up everything!  (item_BTfMI7oH0KvSsGxEfSOTP)\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "‚Äî‚Äî response.done  (window ‚âà1897 tokens) ‚Äî‚Äî\n",
    "‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "[user     ] Hey, what's up?  (item_BTfKeRGKfjQ976Ojmgpl6)\n",
    "[assistant] Hey there! Not much, just here to help out. What's up with you?  (item_BTfKeuSJAlvr8WMqewiOo)\n",
    "[user     ]   (item_BTfMI1PSozC8zYfxBGDEA)\n",
    "[user     ] Can you tell me a joke?  (item_BTfMIbULFByNpzbBMjP18)\n",
    "[assistant] Sure thing! Why don't scientists trust atoms? Because they make up everything!  (item_BTfMI7oH0KvSsGxEfSOTP)\n",
    "[user     ]   (item_BTfOaHtFgPzBAcUJiZ6Jp)\n",
    "[assistant] Once upon a time, in a cozy little village, there lived a baker named Lucy. Known for her magical touch with pastries, Lucy dreamed of creating a pie so extraordinary, it would put their village on the map.  One day, news of a royal pie contest reached the village. The winner would earn a place in the royal kitchen, and Lucy knew this was her chance. She experimented day and night, seeking the perfect recipe.  Finally, she crafted a pie with a golden crust, filled with enchanted berries that shimmered. The day of the contest arrived, and Lucy's pie dazzled the judges, winning first place!  Her victory brought fame to the village, and Lucy's bakery became a beloved destination for all. And so, Lucy's dream came true, one delicious pie at a time.  (item_BTfOaW9YNEXg1c7jAVP71)\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "‚Äî‚Äî response.done  (window ‚âà2138 tokens) ‚Äî‚Äî\n",
    "‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "[user     ] Hey, what's up?  (item_BTfKeRGKfjQ976Ojmgpl6)\n",
    "[assistant] Hey there! Not much, just here to help out. What's up with you?  (item_BTfKeuSJAlvr8WMqewiOo)\n",
    "[user     ]   (item_BTfMI1PSozC8zYfxBGDEA)\n",
    "[user     ] Can you tell me a joke?  (item_BTfMIbULFByNpzbBMjP18)\n",
    "[assistant] Sure thing! Why don't scientists trust atoms? Because they make up everything!  (item_BTfMI7oH0KvSsGxEfSOTP)\n",
    "[user     ] Hey, €å€Å 500 word story?  (item_BTfOaHtFgPzBAcUJiZ6Jp)\n",
    "[assistant] Once upon a time, in a cozy little village, there lived a baker named Lucy. Known for her magical touch with pastries, Lucy dreamed of creating a pie so extraordinary, it would put their village on the map.  One day, news of a royal pie contest reached the village. The winner would earn a place in the royal kitchen, and Lucy knew this was her chance. She experimented day and night, seeking the perfect recipe.  Finally, she crafted a pie with a golden crust, filled with enchanted berries that shimmered. The day of the contest arrived, and Lucy's pie dazzled the judges, winning first place!  Her victory brought fame to the village, and Lucy's bakery became a beloved destination for all. And so, Lucy's dream came true, one delicious pie at a time.  (item_BTfOaW9YNEXg1c7jAVP71)\n",
    "[user     ]   (item_BTfQYIvLsqARzsSwUF5Wv)\n",
    "[assistant] Absolutely! How about this: Why did the scarecrow win an award? Because he was outstanding in his field!  (item_BTfQYoNSn0Lv33LMrWle5)\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "[assistant] Dans cette conversation, l'utilisateur commence par saluer et demander de l'aide, puis il demande une blague. L'assistant r√©pond avec une blague sur les atomes, soulignant son r√¥le d'aide et d'interaction amicale.  (sum_001)\n",
    "[user     ] Hey, €å€Å 500 word story?  (item_BTfOaHtFgPzBAcUJiZ6Jp)\n",
    "[assistant] Once upon a time, in a cozy little village, there lived a baker named Lucy. Known for her magical touch with pastries, Lucy dreamed of creating a pie so extraordinary, it would put their village on the map.  One day, news of a royal pie contest reached the village. The winner would earn a place in the royal kitchen, and Lucy knew this was her chance. She experimented day and night, seeking the perfect recipe.  Finally, she crafted a pie with a golden crust, filled with enchanted berries that shimmered. The day of the contest arrived, and Lucy's pie dazzled the judges, winning first place!  Her victory brought fame to the village, and Lucy's bakery became a beloved destination for all. And so, Lucy's dream came true, one delicious pie at a time.  (item_BTfOaW9YNEXg1c7jAVP71)\n",
    "[user     ] Any other funny things you can tell me?  (item_BTfQYIvLsqARzsSwUF5Wv)\n",
    "[assistant] Absolutely! How about this: Why did the scarecrow win an award? Because he was outstanding in his field!  (item_BTfQYoNSn0Lv33LMrWle5)\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "‚Äî‚Äî response.done  (window ‚âà0 tokens) ‚Äî‚Äî\n",
    "‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "[assistant] Dans cette conversation, l'utilisateur commence par saluer et demander de l'aide, puis il demande une blague. L'assistant r√©pond avec une blague sur les atomes, soulignant son r√¥le d'aide et d'interaction amicale.  (sum_001)\n",
    "[user     ] Hey, €å€Å 500 word story?  (item_BTfOaHtFgPzBAcUJiZ6Jp)\n",
    "[assistant] Once upon a time, in a cozy little village, there lived a baker named Lucy. Known for her magical touch with pastries, Lucy dreamed of creating a pie so extraordinary, it would put their village on the map.  One day, news of a royal pie contest reached the village. The winner would earn a place in the royal kitchen, and Lucy knew this was her chance. She experimented day and night, seeking the perfect recipe.  Finally, she crafted a pie with a golden crust, filled with enchanted berries that shimmered. The day of the contest arrived, and Lucy's pie dazzled the judges, winning first place!  Her victory brought fame to the village, and Lucy's bakery became a beloved destination for all. And so, Lucy's dream came true, one delicious pie at a time.  (item_BTfOaW9YNEXg1c7jAVP71)\n",
    "[user     ] Any other funny things you can tell me?  (item_BTfQYIvLsqARzsSwUF5Wv)\n",
    "[assistant] Absolutely! How about this: Why did the scarecrow win an award? Because he was outstanding in his field!  (item_BTfQYoNSn0Lv33LMrWle5)\n",
    "[user     ]   (item_BTfTVqDdNbi2X05U8rHIs)\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "‚Äî‚Äî response.done  (window ‚âà0 tokens) ‚Äî‚Äî\n",
    "‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "[assistant] Dans cette conversation, l'utilisateur commence par saluer et demander de l'aide, puis il demande une blague. L'assistant r√©pond avec une blague sur les atomes, soulignant son r√¥le d'aide et d'interaction amicale.  (sum_001)\n",
    "[user     ] Hey, €å€Å 500 word story?  (item_BTfOaHtFgPzBAcUJiZ6Jp)\n",
    "[assistant] Once upon a time, in a cozy little village, there lived a baker named Lucy. Known for her magical touch with pastries, Lucy dreamed of creating a pie so extraordinary, it would put their village on the map.  One day, news of a royal pie contest reached the village. The winner would earn a place in the royal kitchen, and Lucy knew this was her chance. She experimented day and night, seeking the perfect recipe.  Finally, she crafted a pie with a golden crust, filled with enchanted berries that shimmered. The day of the contest arrived, and Lucy's pie dazzled the judges, winning first place!  Her victory brought fame to the village, and Lucy's bakery became a beloved destination for all. And so, Lucy's dream came true, one delicious pie at a time.  (item_BTfOaW9YNEXg1c7jAVP71)\n",
    "[user     ] Any other funny things you can tell me?  (item_BTfQYIvLsqARzsSwUF5Wv)\n",
    "[assistant] Absolutely! How about this: Why did the scarecrow win an award? Because he was outstanding in his field!  (item_BTfQYoNSn0Lv33LMrWle5)\n",
    "[user     ]   (item_BTfTVqDdNbi2X05U8rHIs)\n",
    "[user     ]   (item_BTfTVHuCRBUzG82xQlx1o)\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "‚Äî‚Äî response.done  (window ‚âà2082 tokens) ‚Äî‚Äî\n",
    "‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "[assistant] Dans cette conversation, l'utilisateur commence par saluer et demander de l'aide, puis il demande une blague. L'assistant r√©pond avec une blague sur les atomes, soulignant son r√¥le d'aide et d'interaction amicale.  (sum_001)\n",
    "[user     ] Hey, €å€Å 500 word story?  (item_BTfOaHtFgPzBAcUJiZ6Jp)\n",
    "[assistant] Once upon a time, in a cozy little village, there lived a baker named Lucy. Known for her magical touch with pastries, Lucy dreamed of creating a pie so extraordinary, it would put their village on the map.  One day, news of a royal pie contest reached the village. The winner would earn a place in the royal kitchen, and Lucy knew this was her chance. She experimented day and night, seeking the perfect recipe.  Finally, she crafted a pie with a golden crust, filled with enchanted berries that shimmered. The day of the contest arrived, and Lucy's pie dazzled the judges, winning first place!  Her victory brought fame to the village, and Lucy's bakery became a beloved destination for all. And so, Lucy's dream came true, one delicious pie at a time.  (item_BTfOaW9YNEXg1c7jAVP71)\n",
    "[user     ] Any other funny things you can tell me?  (item_BTfQYIvLsqARzsSwUF5Wv)\n",
    "[assistant] Absolutely! How about this: Why did the scarecrow win an award? Because he was outstanding in his field!  (item_BTfQYoNSn0Lv33LMrWle5)\n",
    "[user     ]   (item_BTfTVqDdNbi2X05U8rHIs)\n",
    "[user     ]   (item_BTfTVHuCRBUzG82xQlx1o)\n",
    "[user     ]   (item_BTfTV6S7x7gTfHgBhZFst)\n",
    "[assistant] The language of the first summary of our conversation was French.  (item_BTfTVSiLtjisYYInKT40R)\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "‚Äî‚Äî Conversation so far ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "[assistant] Dans cette conversation, l'utilisateur demande de l'aide et une blague, tandis que l'assistant r√©pond avec une blague sur les atomes et une autre sur un √©pouvantail, montrant son r√¥le d'interaction amicale et humoristique. L'utilisateur √©voque √©galement une histoire de 500 mots sur une boulang√®re nomm√©e Lucy, qui remporte un concours de tartes, ce qui apporte la renomm√©e √† son village.  (sum_002)\n",
    "[user     ]   (item_BTfTVqDdNbi2X05U8rHIs)\n",
    "[user     ]   (item_BTfTVHuCRBUzG82xQlx1o)\n",
    "[user     ] The summary of our conversation  (item_BTfTV6S7x7gTfHgBhZFst)\n",
    "[assistant] The language of the first summary of our conversation was French.  (item_BTfTVSiLtjisYYInKT40R)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6‚ÄØ¬∑‚ÄØReal‚ÄëWorld Applications & Extension Ideas\n",
    "- **Customer‚Äësupport voicebots:** Summaries enable seamless hand‚Äëoff to human agents while preserving privacy.\n",
    "- **Multilingual assistants:** Swap `SUMMARY_MODEL` and system prompt to translate & condense context across languages.\n",
    "- **Accessibility tools:** Real‚Äëtime captioning plus summarised notes for hearing‚Äëimpaired users.\n",
    "- **Embedded devices:** Edge streaming with local VAD to conserve data.\n",
    "\n",
    "> **Extension Challenge:** Integrate a browser‚Äëbased UI with Web¬†Speech¬†API so users need no Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next‚ÄØSteps & Further Reading\n",
    "Try out the notebook and try integrating context summary into your application.\n",
    "\n",
    "Few things you can try:\n",
    "- Evaluate if context summary helps with your eval and use case\n",
    "- Try various methods of summarizing\n",
    "- ect\n",
    "\n",
    "Resources:\n",
    "- https://platform.openai.com/docs/guides/realtime \n",
    "- https://platform.openai.com/docs/guides/realtime-conversations\n",
    "- https://platform.openai.com/docs/api-reference/realtime\n",
    "- https://voiceaiandvoiceagents.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
