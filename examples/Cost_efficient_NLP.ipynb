{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Efficient NLP using OpenAI GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the past few years, developers have encountered significant challenges when dealing with text data, despite the availability of rich resources such as `SpaCy` and other NLP libraries. These challenges persist due to the constantly increasing volume of data, which demands considerable human involvement, thought, and effort. \n",
    "\n",
    "Large language models (LLMs), are changing the way we handle text data. LLMs can analyze text data much faster than humans and can understand it in a way that's similar to how we do. So why not leverage their advantages for NLP tasks?\n",
    "\n",
    "This notebook demonstrates how to use the **OpenAI GPT-3 model** for cost-effective NLP tasks. Prompt engineering plays a crucial role in guiding the model to produce desired outputs with minimal token generation, thereby reducing API usage costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required packages if not already installed\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries (OpenAI)\n",
    "from openai import OpenAI\n",
    "\n",
    "# Setting up the OpenAI API key (Replace \"YOUR_OPENAI_API_KEY_HERE\" with your OpenAI API key)\n",
    "client = OpenAI(api_key=\"YOUR_OPENAI_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the `gpt-3.5-turbo-0125` language model (as of March 2024). You can see the latest pricing [here](https://openai.com/pricing#:~:text=Output-,gpt%2D3.5%2Dturbo%2D0125,-%240.50%C2%A0/%201M). Let's check out its specs, like pricing, token limits, and more.\n",
    "\n",
    "| MODEL             | CONTEXT WINDOW | TRAINING DATA | Input                  | Output             |\n",
    "|-------------------|----------------|---------------|------------------------|--------------------|\n",
    "| gpt-3.5-turbo-0125 | 16,385 tokens  | Up to Sep 2021 | $0.50 / 1M tokens     | $1.50 / 1M tokens |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the model name\n",
    "model_name = 'gpt-3.5-turbo-0125'\n",
    "\n",
    "# setting the temperature\n",
    "temperature = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `temperature` value is set to `0.2` ensuring consistent results. It controls the randomness of the output. A lower value will make the output more deterministic, while a higher value will make it more random.\n",
    "\n",
    "We'll create a function that takes the user's input, the model name, and the temperature as inputs. The function will return a dictionary containing the following:\n",
    "1. API response\n",
    "2. Input tokens price\n",
    "3. output tokens price\n",
    "4. Total price (input + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to chat with the OpenAI model \n",
    "def openai_chat(user_prompt: str, model_name: str, temperature:str) -> dict:\n",
    "\n",
    "    # Create a chat completion using the OpenAI client\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, \n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    # Response from the chat completion\n",
    "    text_response = response.choices[0].message.content\n",
    "\n",
    "    # Calculate the price for input tokens\n",
    "    input_tokens_price = response.usage.prompt_tokens * (0.5 / 1e6)\n",
    "\n",
    "    # Calculate the price for output tokens\n",
    "    output_tokens_price = response.usage.completion_tokens * (1.5 / 1e6)\n",
    "\n",
    "    # Calculate the total price\n",
    "    total_price = input_tokens_price + output_tokens_price\n",
    "\n",
    "    # Return the answer, input price, output price, and total price\n",
    "    return {\n",
    "        \"answer\": text_response,\n",
    "        \"input_price\": f\"$ {input_tokens_price}\",\n",
    "        \"output_price\": f\"$ {output_tokens_price}\",\n",
    "        \"total_price\": f\"$ {total_price}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price is calculated based on the pricing table I showed you earlier for the **gpt-3.5-turbo-0215** model. Let’s see how this function works by trying it out with a simple, short prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The capital of China is Beijing.',\n",
       " 'input_price': '$ 7e-06',\n",
       " 'output_price': '$ 1.0500000000000001e-05',\n",
       " 'total_price': '$ 1.7500000000000002e-05'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling our function with the prompt\n",
    "openai_chat(\"What is the capital of China?\", model_name, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pattern Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When extracting patterns like emails or numbers from text data, defining patterns using regex or other libraries is a common approach. However, the LLM has a significant advantage due to its training on large text data. Unlike regex, you just need to name the pattern you want to extract, such as email or phone number, making it more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input text contain several names, email, phone numbers, and more. Let's see how it can extract these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the user input\n",
    "user_input = \"\"\"\n",
    "In the bustling metropolis of New Alexandria, Detective Miller was hot on the trail of a notorious hacker known only as \"Cipher.\" A cryptic message intercepted by the cybercrime unit mentioned a meeting at \"The Crimson Cafe, 3pm, table 7,\" and listed two aliases: \"Silver Fox\" and \"Blackbird.\" Miller knew these were likely the hacker's accomplices. \n",
    "His informant, a nervous young man named Alex Turner, claimed Cipher frequented an online forum under the ShadowHunter1337 and often boasted about their exploits. Turner also provided a burner phone number, 555-987-2104, supposedly used by Cipher to contact their associates. \n",
    "Armed with this information, Miller headed to The Crimson Cafe. He arrived early, taking a seat across from table 7, his eyes scanning the room. At precisely 3pm, two individuals approached the table. One, a woman with silver hair and piercing blue eyes, exuded an air of confidence and cunning. The other, a man clad in black, remained silent and watchful. \n",
    "Miller approached them, flashing his badge. \"Excuse me, I'm Detective Miller. I believe you might have information regarding an individual known as Cipher.\" \n",
    "The woman, her lips curling into a sly smile, replied, \"Cipher? Never heard of them.\" \n",
    "\"\"\"\n",
    "\n",
    "# Patterns to detect the user input\n",
    "patterns = \"phone_numbers, person_names, location_names, time_expressions, aliases, usernames, physical_descriptions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined the patterns in a comma-separated format, this is one of the advantages of using the LLM, you are not bound to a specific format, you can define the patterns in any way you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a prompt template that the model can use to extract the desired pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the prompt template\n",
    "prompt_template = f'''\n",
    "Given the input text:\n",
    "user input: {user_input}\n",
    "\n",
    "extract following patterns from it: {patterns}\n",
    "\n",
    "output must be in this format:\n",
    "pattern_name: pattern_values\n",
    "...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt template is not difficult to understand, but one important thing to note is that the output format is the key here. The model will extract the patterns and return them in the same format as the output. If not defined properly, model may return the output in a different format on every run, which result in an error because we have to code to map the extracted values to the defined patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phone_numbers: 555-987-2104\n",
      "person_names: Detective Miller, Alex Turner\n",
      "location_names: New Alexandria, The Crimson Cafe\n",
      "time_expressions: 3pm\n",
      "aliases: Cipher, Silver Fox, Blackbird\n",
      "usernames: ShadowHunter1337\n",
      "physical_descriptions: woman with silver hair and piercing blue eyes, man clad in black\n"
     ]
    }
   ],
   "source": [
    "# Calling our function with the prompt\n",
    "extracted_patterns = openai_chat(prompt_template, model_name, temperature)\n",
    "\n",
    "# Printing the extracted patterns\n",
    "print(extracted_patterns['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has extracted the patterns successfully. But to make it reusable and more efficient, we can convert the output format into a more meaningful format such as dictionary or list etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phone_numbers': ['555-987-2104'],\n",
       " 'person_names': ['Detective Miller', 'Alex Turner'],\n",
       " 'location_names': ['New Alexandria', 'The Crimson Cafe'],\n",
       " 'time_expressions': ['3pm'],\n",
       " 'aliases': ['Cipher', 'Silver Fox', 'Blackbird'],\n",
       " 'usernames': ['ShadowHunter1337'],\n",
       " 'physical_descriptions': ['woman with silver hair and piercing blue eyes',\n",
       "  'man clad in black']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert extracted patterns to a dictionary\n",
    "extracted_patterns_list = extracted_patterns['answer'].split('\\n')\n",
    "extracted_patterns_dict = {}\n",
    "for pattern in extracted_patterns_list:\n",
    "    if pattern:\n",
    "        # Splitting the pattern into key and values\n",
    "        key, values = pattern.split(':')\n",
    "        key = key.strip()\n",
    "        # Splitting the values into a list\n",
    "        values = [value.strip() for value in values.split(',')]\n",
    "        # Adding the key and values to the dictionary\n",
    "        extracted_patterns_dict[key] = values\n",
    "\n",
    "\n",
    "# Printing the extracted patterns\n",
    "extracted_patterns_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just few lines of code we have changed the format of the extracted patterns in the same format which `SpaCy` or other NLP libraries have provided. The key point to note here is that some patterns names such as usernames, physical descriptions if working with core NLP libraries will be a hactic task, but with LLM it is just a matter of defining the pattern and extracting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where prompt engineering come in handy. We can define the prompt template in such a way that the model can correct the spelling of the given text. but output only those which needs correction and their corrected version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input text contain several spelling errors. Let's see how it can correct these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the user input\n",
    "user_input = \"\"\"She walkd alonng the breezy beech, feelin the kool breeze on her facee. The waves whispered soft melodies, soothin her troubled mind, as she strolld aimlesly alonng the shore.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a prompt template that the model can use to correct misspelled words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the prompt template\n",
    "prompt_template = f'''Given the input text:\n",
    "user input: {user_input}\n",
    "\n",
    "output must be in this format:\n",
    "misspelled_word:corrected_word\n",
    "...\n",
    "output must not contain anyother information\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the prompt template, we’ve explicitly defined the desired response format. The API should highlight misspelled words and provide their correct replacements. We’ve also specified that no additional information should be included in the response, as we’ll be coding to replace these words in the original **user_input**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walkd:walked\n",
      "alonnng:along\n",
      "beech:beach\n",
      "feelin:feeling\n",
      "kool:cool\n",
      "facee:face\n",
      "strolld:strolled\n",
      "aimlesly:aimlessly\n"
     ]
    }
   ],
   "source": [
    "# Calling our function with the prompt\n",
    "misspelled_words = openai_chat(prompt_template, model_name, temperature)\n",
    "\n",
    "# Printing the misspelled words\n",
    "print(misspelled_words['answer'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ve truncated the response here, but the language model successfully identified misspelled word and provided their correct form. Let’s see how much this task cost us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Price: $ 4.4999999999999996e-05\n",
      "Output Price: $ 7.05e-05\n",
      "Total Price: $ 0.0001155\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Price:\", misspelled_words['input_price'])\n",
    "print(\"Output Price:\", misspelled_words['output_price'])\n",
    "print(\"Total Price:\", misspelled_words['total_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost for this task comes to **$0.000115**, If we hadn’t used this prompt template, we would likely have spent more than this amount. This is because we would be sending all the text as input, and the output cost of $1.50 per million tokens would apply to the complete input text being returned.\n",
    "\n",
    "We have to code a bit to replace the misspelled words with their corrected form. But this is a one-time effort, and we can reuse this code for any other task that requires spell correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She walked alonng the breezy beach, feeling the cool breeze on her face. The waves whispered soft melodies, soothin her troubled mind, as she strolled aimlessly alonng the shore.\n"
     ]
    }
   ],
   "source": [
    "# Splitting the misspelled words and their corrected versions\n",
    "misspelled_words_list = misspelled_words['answer'].split(\"\\n\")\n",
    "result = [(line.split(\":\")[0], line.split(\":\")[1]) for line in misspelled_words_list if line.strip()]\n",
    "\n",
    "# Replace the misspelled words with the corrected words\n",
    "for misspelled_word, corrected_word in result:\n",
    "    user_input = user_input.replace(misspelled_word, corrected_word)\n",
    "\n",
    "# Print the corrected user input\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same prompt temaplate can be used to extract named entities from the text data. We just need to define the named entities we want to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input text contain several named entities. Let's see how it can extract these entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the user input\n",
    "user_input = \"\"\"In the heart of New York City, John Smith walked briskly down Fifth Avenue, clutching his briefcase tightly. He was headed to a meeting with Microsoft Corporation, where he would discuss the latest advancements in artificial intelligence. As he passed by Central Park, he couldn't help but admire the towering skyscrapers surrounding him. Suddenly, his phone buzzed with a notification from his friend Sarah, inviting him to dinner at her favorite Italian restaurant, Giovanni's. John quickly replied, agreeing to meet her there later that evening.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the NER tags\n",
    "ner_tags = f'''NER TAGS: FAC, CARDINAL, NUMBER, DEMONYM, QUANTITY, TITLE, PHONE_NUMBER, NATIONAL, JOB, PERSON, LOC, NORP, TIME, CITY, EMAIL, GPE, LANGUAGE, PRODUCT, ZIP_CODE, ADDRESS, MONEY, ORDINAL, DATE, EVENT, CRIMINAL_CHARGE, STATE_OR_PROVINCE, RELIGION, DURATION, URL, WORK_OF_ART, PERCENT, CAUSE_OF_DEATH, COUNTRY, ORG, LAW, NAME, COUNTRY, RELIGION, TIME'''\n",
    "\n",
    "# Generate the prompt template\n",
    "prompt_template = f'''Given the input text:\n",
    "user input: {user_input}\n",
    "\n",
    "perform NER detection on it.\n",
    "Following are the NER Tags: {ner_tags}\n",
    "\n",
    "output must be in the format\n",
    "tag:value\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC: New York City\n",
      "PERSON: John Smith\n",
      "ORG: Microsoft Corporation\n",
      "FAC: Central Park\n",
      "PERSON: Sarah\n",
      "FAC: Giovanni's\n",
      "TIME: later that evening\n"
     ]
    }
   ],
   "source": [
    "# Calling our function with the prompt\n",
    "extracted_entities = openai_chat(prompt_template, model_name, temperature)\n",
    "\n",
    "# Printing the extracted entities\n",
    "print(extracted_entities['answer'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code that we used to extract the named entities is the same as the one we used to extract the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': 'New York City',\n",
       " 'PERSON': 'Sarah',\n",
       " 'ORG': 'Microsoft Corporation',\n",
       " 'FAC': \"Giovanni's\",\n",
       " 'TIME': 'later that evening'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert extracted entities to a dictionary\n",
    "extracted_entities_list = extracted_entities['answer'].split('\\n')\n",
    "extracted_entities_dict = {}\n",
    "for entity in extracted_entities_list:\n",
    "    if entity:\n",
    "        # Splitting the entity into tag and value\n",
    "        tag, value = entity.split(':')\n",
    "        tag = tag.strip()\n",
    "        value = value.strip()\n",
    "        # Adding the tag and value to the dictionary\n",
    "        extracted_entities_dict[tag] = value\n",
    "\n",
    "# Printing the extracted entities\n",
    "extracted_entities_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER detection using OpenAI's GPT-3.5 model is a more powerful approach than using `SpaCy` or other NLP libraries. This is because the model has been trained on a large amount of text data and can extract named entities more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explored how to use the OpenAI GPT-3 model for NLP tasks. We used prompt engineering to guide the model to produce desired outputs, other NLP tasks such as POS tagging, sentiment analysis, and more can be performed using the same approach. This approach is cost-effective and efficient, as it reduces the number of tokens generated by the model, thereby reducing the cost of using the API.\n",
    "\n",
    "As a next step, you can try out different NLP tasks such as sentence segmentation where you can split the text into meaningful sentences by defining the prompt template accordingly.\n",
    "```python\n",
    "promtp_template = '''\n",
    "break the above text in meaningful information, each break contains a complete information and can have multiple sentences, dont provide sentences.\n",
    "\n",
    "provide output in this format:\n",
    "\n",
    "last word [break] first word\n",
    "last word [break] first word\n",
    "last word [break] first word\n",
    "'''\n",
    "```\n",
    "\n",
    "This way you wont be returning the complete text but only the break points which can be used to split the text into meaningful sentences and then using code to split the text into sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_basiclingua",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
