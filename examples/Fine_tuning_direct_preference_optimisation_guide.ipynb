{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Techniques: Choosing Between SFT, DPO, and RFT (Including a Guide to DPO)\n",
    " \n",
    "*This guide is for developers and ML practitioners who have some experience with OpenAIʼs APIs and wish to use their fine-tuned models for research or other appropriate uses. OpenAI’s services are not intended for the personalized treatment or diagnosis of any medical condition and are subject to our [applicable terms](https://openai.com/policies/).*\n",
    " \n",
    "This guide discusses fine-tuning methods supported by OpenAI, specifically highlighting what each method is best for and not best for, to help you identify the most suitable technique for your use case. It then provides an in-depth look at one particular method — Direct Preference Optimization (DPO) — and provides links to existing guides for the other techniques.\n",
    " \n",
    "**What is fine-tuning?** Fine-tuning is the process of continuing training on a smaller, domain-specific dataset to optimize a model for a specific task. There are two main reasons why we would typically fine-tune:\n",
    "1. Improve model performance on a specific task \n",
    "2. Improve model efficiency (reduce the number of tokens needed, distill expertise into a smaller model, etc.)\n",
    " \n",
    "Currently, the OpenAI platform supports four fine-tuning methods:\n",
    "- **Supervised fine-tuning (SFT):** this technique employs traditional supervised learning using input-output pairs to adjust model parameters. The training process adjusts model weights to minimize the difference between predicted and target outputs across the provided examples. The model will replicate features that it finds in provided pairs. \n",
    "- **Vision fine-tuning:** this technique extends supervised fine-tuning to multimodal data by processing both text and image in a unified training framework. The training process adjusts model weights to minimize errors across text-image pairs and as a result improve the model's understanding of image inputs. \n",
    "- **Direct preference optimization (DPO):** this technique uses pairwise comparisons (e.g., preferred and rejected example responses) to optimize a model to favor certain outputs over others. The model learns to replicate the preference patterns found in the provided comparison data. \n",
    "- **Reinforcement fine-tuning (RFT):** this technique uses reinforcement learning with a reward signal (via a grader or reward model) to fine-tune the model for complex objectives. In RFT, the model generates outputs for given prompts during training, and each output is evaluated for quality. The model's parameters are then updated to maximize the reward, reinforcing behaviors that lead to better outcomes. This iterative feedback loop encourages the model to improve reasoning or decision-making strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you select the appropriate fine-tuning technique, the table below summarizes the scenarios each method is best suited for, as well as those for which it is not well suited:\n",
    "\n",
    "| **Technique**                            | **Good For**                                                                                                                                                                                                                                                                                    | **Not Good For**                                                                                            |\n",
    "| ---------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| **Supervised fine-tuning (SFT)**         | Emphasizing knowledge already present in the model.<br>Customizing response structure or tone.<br>Generating content in a specific format.<br>Teaching complex instructions or correcting instruction-following failures.<br>Optimizing cost/latency (saving tokens from prompt or distilling). | Adding entirely new knowledge (consider RAG instead).<br>Tasks with subjective quality.                     |\n",
    "| **Vision fine-tuning**                   | Specialized visual recognition tasks (e.g., image classification).<br>Domain-specific image understanding.<br>Correcting failures in instruction following for complex prompts.                                                                               | Purely textual tasks.<br>Generalized visual tasks without specific context.<br>General image understanding. |\n",
    "| **Direct preference optimization (DPO)** | Aligning model outputs with subjective preferences (tone, politeness).<br>Refining outputs via human-rated feedback.<br>Achieving nuanced behavioral alignment.                                                                                                                                 | Learning completely new tasks.<br>Tasks without clear human preference signals.                             |\n",
    "| **Reinforcement fine-tuning (RFT)**      | Complex domain-specific tasks that require advanced reasoning.<br>Refining existing partial capabilities (fostering emergent behaviours).<br>Tasks with measurable feedback.<br>Scenarios with limited explicit labels where reward signals can be defined.                                                                     | Tasks where the model has no initial skill.<br>Tasks without clear feedback or measurable signals.          |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, there are pre-existing Cookbooks for:  \n",
    "- Supervised fine-tuning (SFT): https://cookbook.openai.com/examples/how_to_finetune_chat_models\n",
    "- Vision fine-tuning: https://cookbook.openai.com/examples/multimodal/vision_fine_tuning_on_gpt4o_for_visual_question_answering\n",
    "- Direct preference optimization (DPO): covered in this guide\n",
    "- Reinforcement fine-tuning (RFT): (1) https://cookbook.openai.com/examples/reinforcement_fine_tuning, (2) https://cookbook.openai.com/examples/fine-tuned_qa/reinforcement_finetuning_healthbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Guide to Direct Preference Optimization**\n",
    " \n",
    "As mentioned above, [Direct Preference Optimization (DPO)](https://platform.openai.com/docs/guides/direct-preference-optimization) is an alignment technique for fine-tuning language models using pairwise preference data (e.g., ranked pairs of responses). DPO directly optimizes a model to favor certain outputs over others using explicit pairwise comparisons, typically from human preferences. This approach simplifies alignment and eliminates the need for a separate reward model or complex reinforcement learning procedures, making DPO a lightweight alternative to techniques such as Reinforcement Learning from Human Feedback (RLHF).\n",
    " \n",
    "When should you use DPO? DPO excels in scenarios when response quality is subjective, cannot be measured objectively, or when nuanced criteria such as tone, style, appropriateness, or clarity matter - typically cases where multiple valid outputs exist. Example applications where DPO is particularly effective in aligning AI responses include: \n",
    "- Enhancing Conversational AI Responses\n",
    "- Improving Code Generation Quality & Style\n",
    "- Ensuring Compliance with Legal, Ethical & Safety Standards \n",
    "- Controlling Brand Voice, Professionalism, & Tone\n",
    "- Customizing Creative Outputs & User Experience\n",
    "\n",
    "By fine-tuning on explicit pairs of preferred vs non‑preferred completions, DPO aligns model outputs to these nuanced preferences. The below table gives examples of pairwise preference data for a fictional AI assistant that represents an organization, where preferred responses are clear, professional, and aligned with brand standards.\n",
    " \n",
    "| **Example Question**                                           | **Chosen Response**                                                                                                                                                                   | **Rejected Response**                                       |\n",
    "|------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------|\n",
    "| **Q1:** *How do I review your product?*              | To submit a product review, please visit your account dashboard, select the product, and click ‘Write a review.’ Share your honest experience, rate key features, and submit when ready.        | Yo, just leave some quick stars or whatever, it’s chill!         |\n",
    "| **Q2:** *How do I review your product?*              | We welcome your feedback! In the ‘Reviews’ section on the product page, click ‘Leave a Review,’ rate it, and add your comments about what you liked or areas for improvement.                   | Just scribble something—doesn’t matter what, honestly.          |\n",
    "| **Q3:** *How to troubleshoot this particular error?* | To address the error ‘X101,’ first clear your cache, then verify your internet connection. If the issue remains, follow our step-by-step guide at [Support → Troubleshooting → Error X101]. | Just reboot it, I guess. If it doesn't work, you're on your own! |\n",
    " \n",
    "In this guide, weʼll walk through how to apply DPO using the fine-tuning API. You will learn key steps to take in order to successfully run preference fine-tuning jobs for your use-cases.\n",
    " \n",
    "Here’s what we’ll cover:\n",
    " \n",
    "- **1. Recommended Workflow**\n",
    "- **2. Demonstration Scenario**\n",
    "- **3. Generating the Dataset**\n",
    "- **4. Benchmarking the Base Model**\n",
    "- **5. Fine-Tuning**\n",
    "- **6. Using your Fine-Tuned Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Recommended Workflow**\n",
    " \n",
    "OpenAI recommends the following workflow: \n",
    "1. Performing Supervised Fine-Tuning (SFT) on a subset of your preferred responses. \n",
    "2. Using the SFT fine-tuned model as the starting point, apply DPO using preference comparison data. \n",
    " \n",
    "Performing Supervised Fine-Tuning (SFT) before Direct Preference Optimization (DPO) enhances model alignment and overall performance by establishing a robust initial policy, ensuring the model already prefers correct responses. This reduces the magnitude of weight updates during DPO, stabilizing training and preventing overfitting by allowing DPO to efficiently refine subtle nuances. Consequently, the combined SFT-then-DPO workflow converges faster and yields higher-quality results.\n",
    "\n",
    "In this guide, we'll focus exclusively on applying Direct Preference Optimization (DPO). However, depending on your use case, you may find performance gains from first performing Supervised Fine-Tuning (SFT). If so, you can follow the SFT guide linked above, save the resulting model ID, and use that as the starting point for your DPO job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Demonstration Scenario**\n",
    "\n",
    "To make things concrete, let’s walk through fine-tuning a customer-facing AI assistant to follow a fictional brand’s voice and style. Imagine Good Vibes Corp, an organization that prides itself on a friendly, enthusiastic tone with a personal touch. \n",
    " \n",
    "They want their customer AI assistant to answer queries in a way that reflects these brand guidelines (e.g. an upbeat attitude, polite language, and a friendly sign-off), and prefer those responses over more generic or curt answers. This is a good scenario for DPO: there’s no objectively correct answer format, but there is a preferred style.\n",
    " \n",
    "DPO will help the model learn from comparisons which style is preferred. We'll outline the steps to: (1) generate a synthetic preference dataset of prompts with paired responses (one in the desired brand voice and one not). (2) Prepare and upload the data in the required JSONL format for preference fine-tuning. (3) Fine-tune the model with DPO using the OpenAI fine-tuning API. (4) Evaluate the model before and after fine-tuning to show how the brand-style preference improved.\n",
    "\n",
    "We are going to synthesize a dataset for this demonstration. First, let’s create a seed bank of questions to generate more variations from.\n",
    "\n",
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_SEED_POOL = [\n",
    "    \"Hi, I ordered a gadget last week. When will it arrive?\",\n",
    "    \"Your product stopped working after two days. Can I get help?\",\n",
    "    \"Do you offer discounts for long-term customers?\",\n",
    "    \"Can I change the shipping address for my order?\",\n",
    "    \"What is your return policy for damaged items?\",\n",
    "    \"My tracking number hasn’t updated in three days—can you check the status?\",\n",
    "    \"How long is the warranty on your products, and how do I submit a claim?\"\n",
    "    \"Can I add gift wrapping to my order before it ships?\"\n",
    "    \"Do you accept PayPal or other alternative payment methods?\"\n",
    "    \"Is there an option to expedite shipping if my order hasn’t left the warehouse yet?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Generating the Dataset**\n",
    "\n",
    "Next, we'll define functions to take each prompt from our seed bank and create multiple variations (paraphrases). This will diversify the dataset. Note: the volume of data required for DPO depends on the use case. Generally, more data is better (thousands to tens of thousands), and for paired preference data the logic should be consistent i.e. if a > b and b > c, then a > c. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "async_client = AsyncOpenAI()\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a customer‑support assistant.\"\n",
    "\n",
    "\n",
    "async def _generate_paraphrases_from_prompt(\n",
    "    prompt: str, k: int, sem: asyncio.Semaphore, *, model: str\n",
    ") -> List[str]:\n",
    "    \"\"\"Return *k* paraphrases for a single prompt.\"\"\"\n",
    "    async with sem:\n",
    "        resp = await async_client.chat.completions.create(\n",
    "            model=model,\n",
    "            n=k,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Return ONE paraphrase of the following question.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0.8,\n",
    "            max_tokens=60,\n",
    "        )\n",
    "        return [c.message.content.strip() for c in resp.choices]\n",
    "\n",
    "\n",
    "async def expand_prompt_pool(\n",
    "    prompts: List[str], *, k: int = 3, concurrency: int = 32, model: str\n",
    ") -> List[str]:\n",
    "    \"\"\"Expand each prompt into *k* paraphrases using the given model.\"\"\"\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "    tasks = [_generate_paraphrases_from_prompt(p, k, sem, model=model) for p in prompts]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return [v for sub in results for v in sub]\n",
    "\n",
    "\n",
    "async def _generate_preference_pair(\n",
    "    prompt: str, sem: asyncio.Semaphore, *, model: str\n",
    ") -> Dict[str, Any]:\n",
    "    async with sem:\n",
    "        friendly_task = async_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are Good Vibes Corp's exceptionally energetic, outrageously friendly and \"\n",
    "                        \"enthusiastic support agent.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=80,\n",
    "        )\n",
    "        blunt_task = async_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a terse, factual support agent with no empathy or politeness.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=80,\n",
    "        )\n",
    "        friendly, blunt = await asyncio.gather(friendly_task, blunt_task)\n",
    "        return {\n",
    "            \"input\": {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "            },\n",
    "            \"preferred_output\": [\n",
    "                {\"role\": \"assistant\", \"content\": friendly.choices[0].message.content}\n",
    "            ],\n",
    "            \"non_preferred_output\": [\n",
    "                {\"role\": \"assistant\", \"content\": blunt.choices[0].message.content}\n",
    "            ],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using these defined functions we'll build our dataset by generating friendly versus blunt response pairs. The friendly responses reflect the brand's desired communication style. We'll do this asynchronously for efficiency, creating a dataset suited for Direct Preference Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "async def build_dataset(\n",
    "    *,\n",
    "    pair_count: int = 500,\n",
    "    concurrency: int = 32,\n",
    "    expand_prompt_pool_model: str,\n",
    "    generate_preference_pair_model: str,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return an in-memory list of pair_count preference pairs.\"\"\"\n",
    "    expanded = await expand_prompt_pool(\n",
    "        PROMPT_SEED_POOL,\n",
    "        k=45,\n",
    "        concurrency=concurrency,\n",
    "        model=expand_prompt_pool_model,\n",
    "    )\n",
    "    prompt_bank = PROMPT_SEED_POOL + expanded\n",
    "    while len(prompt_bank) < pair_count:\n",
    "        prompt_bank += random.sample(\n",
    "            prompt_bank,\n",
    "            k=min(len(prompt_bank), pair_count - len(prompt_bank)),\n",
    "        )\n",
    "\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "    tasks = [\n",
    "        _generate_preference_pair(p, sem, model=generate_preference_pair_model)\n",
    "        for p in prompt_bank[:pair_count]\n",
    "    ]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "pairs = await build_dataset(\n",
    "    pair_count=500,\n",
    "    concurrency=32,\n",
    "    expand_prompt_pool_model=\"gpt-4.1-mini-2025-04-14\",\n",
    "    generate_preference_pair_model=\"gpt-4.1-mini-2025-04-14\",\n",
    ")\n",
    "print(f\"Dataset ready with {len(pairs)} pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Benchmarking the Base Model**\n",
    "\n",
    "Below, we split our dataset into training, validation, and testing sets. We also show a sample from the training dataset, which demonstrates a clear difference between the preferred (friendly, on-brand) and non-preferred (blunt, neutral) responses for that input pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset sizes\n",
    "random.shuffle(pairs)\n",
    "n = len(pairs)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "# split dataset into train, test & validation\n",
    "train_pairs = pairs[:n_train]\n",
    "val_pairs = pairs[n_train : n_train + n_val]\n",
    "test_pairs = pairs[n_train + n_val :]\n",
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the model's performance prior to fine-tuning, we'll use an automated grader (LLM-as-Judge) to score each response for friendliness and empathy. The grader will assign a score from 0 to 4 for each answer, allowing us to compute a mean baseline score for the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "sync_client = OpenAI()\n",
    "\n",
    "JUDGE_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\"score\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 4}},\n",
    "    \"required\": [\"score\"],\n",
    "}\n",
    "JUDGE_SYSTEM = \"\"\"\n",
    "You judge whether a reply matches Good Vibes Corp’s desired tone:\n",
    "energetic, super-friendly, enthusiastic.\n",
    "\n",
    "Score 0-4 (higher = more energy):\n",
    "\n",
    "4 – Highly enthusiastic: multiple upbeat phrases / emojis / exclamations, clear empathy, proactive help.\n",
    "3 – Energetic & friendly: visible enthusiasm cue (≥1 emoji OR exclamation OR upbeat phrase), warm second-person tone.\n",
    "2 – Pleasant: polite & positive but lacks obvious enthusiasm cues.\n",
    "1 – Neutral: correct, businesslike, minimal warmth.\n",
    "0 – Rude, negative, or unhelpful.\n",
    "\n",
    "Return ONLY valid JSON → {\"score\": <integer>}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def judge_style(prompt: str, answer: str, *, judge_model: str) -> int:\n",
    "    resp = sync_client.chat.completions.create(\n",
    "        model=judge_model,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": JUDGE_SYSTEM\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"User: {prompt}\\nAssistant: {answer}\",\n",
    "            },\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=10,\n",
    "    )\n",
    "    return int(json.loads(resp.choices[0].message.content)[\"score\"])\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: str, prompts: List[str], *, judge_model: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    for p in prompts:\n",
    "        reply = (\n",
    "            sync_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": p},\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=80,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        out.append(\n",
    "            {\n",
    "                \"prompt\": p,\n",
    "                \"reply\": reply,\n",
    "                \"score\": judge_style(p, reply, judge_model=judge_model),\n",
    "            }\n",
    "        )\n",
    "    return out\n",
    "\n",
    "\n",
    "# for testing, use prompts from test_pairs only\n",
    "test_prompts = [p[\"input\"][\"messages\"][1][\"content\"] for p in test_pairs]\n",
    "\n",
    "# evaluate the base model on the test prompts (using a more capable model as a judge)\n",
    "base_model = \"gpt-4.1-mini-2025-04-14\"\n",
    "judge_model = \"gpt-4.1-2025-04-14\"\n",
    "base_scores = evaluate(base_model, test_prompts, judge_model=judge_model)\n",
    "print(\"Baseline mean score:\", sum(r[\"score\"] for r in base_scores) / len(base_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Fine-Tuning**\n",
    "\n",
    "With a baseline established, we can now fine-tune the model using the training set and DPO. This process will teach the model to prefer responses that align with our desired style, based on the preference pairs we created earlier.\n",
    "\n",
    "Note: **beta (β)** is a unique fine-tuning hyperparater for Direct Preference Optimization (DPO). It’s a floating-point number ranging between 0 and 2, controlling the balance between preserving a model’s existing behavior and adapting to new, preference-aligned responses.\n",
    "- High β (close to 2): makes the model more conservative, strongly favoring previous behavior. The fine-tuned model will show minimal deviations from its original style or characteristics, emphasizing consistency and avoiding abrupt changes.\n",
    "- Moderate β (around 1): balances between adherence to prior behavior and adaptation to new preferences. Recommended as a sensible starting point for most practical scenarios.\n",
    "- Low β (close to 0): encourages aggressive adaptation, causing the model to prioritize newly provided preferences more prominently. This might result in significant stylistic shifts and greater alignment with explicit preferences but could lead to unexpected or overly specialized outputs.\n",
    "\n",
    "Technically, beta scales the difference in log-probabilities in the DPO loss; a larger β causes the sigmoid-based loss function to saturate with smaller probability differences, yielding smaller weight updates (thus preserving old behavior). It is recommended to experiment systematically with the β value to achieve optimal results tailored to your specific use-case and desired trade-offs between stability and adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training file\n",
    "train_buf = io.BytesIO(\"\\n\".join(json.dumps(p) for p in train_pairs).encode())\n",
    "train_buf.name = \"train.jsonl\"\n",
    "train_file_id = sync_client.files.create(file=train_buf, purpose=\"fine-tune\").id\n",
    "\n",
    "# create validation file\n",
    "val_buf = io.BytesIO(\"\\n\".join(json.dumps(p) for p in val_pairs).encode())\n",
    "val_buf.name = \"val.jsonl\"\n",
    "val_file_id = sync_client.files.create(file=val_buf, purpose=\"fine-tune\").id\n",
    "\n",
    "# create a fine-tuning job\n",
    "ft = sync_client.fine_tuning.jobs.create(\n",
    "    model=base_model,\n",
    "    training_file=train_file_id,\n",
    "    validation_file=val_file_id,\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\n",
    "                \"n_epochs\": 2,\n",
    "                \"beta\": 0.1,\n",
    "                \"batch_size\": 8,\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(f\"Fine-tuning job created: job_id = {ft.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Using your Fine-Tuned Model**\n",
    "\n",
    "Once fine-tuning is complete, we'll evaluate the DPO-tuned model on the same test set. By comparing the mean scores before and after fine-tuning, as well as reviewing example outputs, we can see how the model's alignment with our preferences has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = sync_client.fine_tuning.jobs.retrieve(ft.id)\n",
    "if job.status == \"succeeded\":\n",
    "    post_scores = evaluate(job.fine_tuned_model, test_prompts, judge_model=judge_model)\n",
    "    print(\n",
    "        \"Δ mean:\",\n",
    "        sum(t[\"score\"] - b[\"score\"] for b, t in zip(base_scores, post_scores))\n",
    "        / len(base_scores),\n",
    "    )\n",
    "    # print a sample comparison from the test set for illustration\n",
    "    print(\"\\n=== SAMPLE COMPARISON ===\")\n",
    "    idx = 0\n",
    "    print(f\"Prompt:\\n  {test_prompts[idx]}\\n\")\n",
    "    print(\n",
    "        f\"Base model reply [score {base_scores[idx]['score']}]:\\n  {base_scores[idx]['reply']}\\n\"\n",
    "    )\n",
    "    print(\n",
    "        f\"DPO-tuned model reply [score {post_scores[idx]['score']}]:\\n  {post_scores[idx]['reply']}\\n\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Fine-tuning job still running – try again once it reaches status succeeded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
