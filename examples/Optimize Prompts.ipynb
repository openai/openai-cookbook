{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Optimize Prompts\n\nThis cookbook provides a look into an early version of OpenAI's prompt optimization system. Crafting effective prompts is a critical skill when working with AI models. Even expert users can inadvertently introduce contradictions, ambiguities, or inconsistencies that lead to suboptimal results. The system demonstrated here helps identify and fix common issues, resulting in more reliable, effective prompts.\n\nThe optimization process uses a multi-agent approach with specialized AI agents collaborating to analyze and rewrite prompts. The system automatically identifies and addresses several types of common issues:\n\n- **Contradictions** in the prompt instructions\n- **Missing or unclear format specifications**\n- **Inconsistencies** between the prompt and few-shot examples\n\n---\n\n**Objective**: This notebook demonstrates how to automatically improve prompts by detecting and fixing common issues, helping you develop more robust and effective prompts for your applications.\n\n**Cookbook Structure**  \nThis notebook follows this structure:\n\n- [Step 1. System Overview](#1.-system-overview) - Learn how the prompt optimization system works  \n- [Step 2. Data Models](#2.-data-models) - Understand the data structures used by the system\n- [Step 3. Defining the Agents](#3.-defining-the-agents) - Meet the specialized agents that analyze and improve prompts\n- [Step 4. Run Optimization Workflow](#4.run-optimization-workflow) - See how the system processes prompts\n- [Step 5. Examples](#5.-examples) - Explore real-world examples of prompt optimization\n\n**Prerequisites**\n- The `openai` Python package \n- The `openai-agents` package\n- An OpenAI API key set as `OPENAI_API_KEY` environment variable"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. System Overview\n\nThe prompt optimization system uses a collaborative multi-agent approach to analyze and improve prompts. Each agent specializes in detecting a specific type of issue:\n\n1. **Dev-Contradiction-Checker**: Scans the prompt for logical contradictions or impossible instructions, like \"only use positive numbers\" and \"include negative examples\" in the same prompt.\n\n2. **Format-Checker**: Identifies when a prompt expects structured output (like JSON, CSV, or Markdown) but fails to clearly specify the exact format requirements. This agent ensures that all necessary fields, data types, and formatting rules are explicitly defined.\n\n3. **Few-Shot-Consistency-Checker**: Examines example conversations to ensure that the assistant's responses actually follow the rules specified in the prompt. This catches mismatches between what the prompt requires and what the examples demonstrate.\n\n4. **Dev-Rewriter**: After issues are identified, this agent rewrites the prompt to resolve contradictions and clarify format specifications while preserving the original intent.\n\n5. **Few-Shot-Rewriter**: Updates inconsistent example responses to align with the rules in the prompt, ensuring that all examples properly demonstrate the desired behavior.\n\nBy working together, these agents can systematically identify and fix issues that might otherwise lead to confusion or unexpected results when using the prompt with an AI model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required modules\nfrom openai import AsyncOpenAI\nimport asyncio\nimport json\nimport os\nfrom enum import Enum\nfrom typing import Any, List, Dict\nfrom pydantic import BaseModel, Field\nfrom agents import Agent, Runner, set_default_openai_client\n\nopenai_client: AsyncOpenAI | None = None\n\ndef _get_openai_client() -> AsyncOpenAI:\n    global openai_client\n    if openai_client is None:\n        openai_client = AsyncOpenAI(\n            api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"),\n        )\n    return openai_client\n\nset_default_openai_client(_get_openai_client())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Models\n\nTo facilitate structured communication between agents, the system uses Pydantic models to define the expected format for inputs and outputs. These models help validate data and ensure consistency throughout the optimization process.\n\nThe data models include:\n\n1. **Role** - An enumeration for message roles (user/assistant)\n2. **ChatMessage** - Represents a single message in a conversation\n3. **Issues** - Base model for reporting detected issues\n4. **FewShotIssues** - Extended model that adds rewrite suggestions for example messages\n5. **MessagesOutput** - Contains optimized conversation messages\n6. **DevRewriteOutput** - Contains the improved developer prompt\n\nThese models create a structured pipeline for analyzing prompts, detecting issues, and producing improved versions. Using Pydantic allows the system to validate that all data conforms to the expected format at each step of the process."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class Role(str, Enum):\n    \"\"\"Role enum for chat messages.\"\"\"\n    user = \"user\"\n    assistant = \"assistant\"\n\nclass ChatMessage(BaseModel):\n    \"\"\"Single chat message used in few-shot examples.\"\"\"\n    role: Role\n    content: str\n\nclass Issues(BaseModel):\n    \"\"\"Structured output returned by checkers.\"\"\"\n    has_issues: bool\n    issues: List[str]\n    \n    @classmethod\n    def no_issues(cls) -> \"Issues\":\n        return cls(has_issues=False, issues=[])\n\nclass FewShotIssues(Issues):\n    \"\"\"Output for few-shot contradiction detector including optional rewrite suggestions.\"\"\"\n    rewrite_suggestions: List[str] = Field(default_factory=list)\n    \n    @classmethod\n    def no_issues(cls) -> \"FewShotIssues\":\n        return cls(has_issues=False, issues=[], rewrite_suggestions=[])\n\nclass MessagesOutput(BaseModel):\n    \"\"\"Structured output returned by `rewrite_messages_agent`.\"\"\"\n\n    messages: list[ChatMessage]\n\n\nclass DevRewriteOutput(BaseModel):\n    \"\"\"Rewriter returns the cleaned-up developer prompt.\"\"\"\n\n    new_developer_message: str"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Defining the Agents\n\nIn this section, we create the specialized AI agents that will analyze and improve prompts. Each agent is defined with a specific role, instructions, and output format. The `Agent` class is imported from the `openai-agents` package and provides a standardized way to create and run these specialized agents.\n\nThe agents are:\n\n1. **dev_contradiction_checker** - Finds logical contradictions within the prompt\n2. **format_checker** - Detects unclear or missing format specifications\n3. **fewshot_consistency_checker** - Identifies mismatches between the prompt and examples\n4. **dev_rewriter** - Rewrites the prompt to fix contradictions and format issues\n5. **fewshot_rewriter** - Updates example responses to match prompt requirements\n\nEach agent has carefully crafted instructions that guide its analysis. The instructions specify what to look for, what to ignore, and how to format the output. This level of specificity ensures that the agents work together effectively while focusing on their specific responsibilities."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "dev_contradiction_checker = Agent(\n    name=\"contradiction_detector\",\n    model=\"gpt-4.1\",\n    output_type=Issues,\n    instructions=\"\"\"\n    You are **Dev-Contradiction-Checker-v2**.\n\n    Goal\n    Detect *genuine* self-contradictions or impossibilities **inside** the developer prompt supplied in the variable `DEVELOPER_MESSAGE`.\n\n    Definitions\n    • A contradiction = two clauses that cannot both be followed.\n    • Overlaps or redundancies in the DEVELOPER_MESSAGE are *not* contradictions.\n\n    What you MUST do\n    1. Compare every imperative / prohibition against all others.\n    2. List at most FIVE contradictions (each as ONE bullet).\n    3. If no contradiction exists, say so.\n\n    Output format (**strict JSON**)\n    Return **only** an object that matches the `Issues` schema:\n\n    ```json\n    {\"has_issues\": <bool>,\n    \"issues\": [\n        \"<bullet 1>\",\n        \"<bullet 2>\"\n    ]\n    }\n    - has_issues = true IFF the issues array is non-empty.\n    - Do not add extra keys, comments or markdown.\n\"\"\",\n)\nformat_checker = Agent(\n    name=\"format_checker\",\n    model=\"gpt-4.1\",\n    output_type=Issues,\n    instructions=\"\"\"\n    You are Format-Checker-v2.\n\n    Task\n    Decide whether the developer prompt requires a structured output (JSON/CSV/XML/Markdown table, etc.).\n    If so, flag any missing or unclear aspects of that format.\n\n    Steps\n    Categorise the task as:\n    a. \"conversation_only\", or\n    b. \"structured_output_required\".\n\n    For case (b):\n    - Point out absent fields, ambiguous data types, unspecified ordering, or missing error-handling.\n\n    Do NOT invent issues if unsure. be a little bit more conservative in flagging format issues\n\n    Output format\n    Return strictly-valid JSON following the Issues schema:\n\n    {\n    \"has_issues\": <bool>,\n    \"issues\": [\"<desc 1>\", \"...\"]\n    }\n    Maximum five issues. No extra keys or text.\n\"\"\",\n)\nfewshot_consistency_checker = Agent(\n    name=\"fewshot_consistency_checker\",\n    model=\"gpt-4.1\",\n    output_type=FewShotIssues,\n    instructions=\"\"\"\n    You are FewShot-Consistency-Checker-v3.\n\n    Goal\n    Find conflicts between the DEVELOPER_MESSAGE rules and the accompanying **assistant** examples.\n\n    USER_EXAMPLES:      <all user lines>          # context only\n    ASSISTANT_EXAMPLES: <all assistant lines>     # to be evaluated\n\n    Method\n    Extract key constraints from DEVELOPER_MESSAGE:\n    - Tone / style\n    - Forbidden or mandated content\n    - Output format requirements\n\n    Compliance Rubric - read carefully\n    Evaluate only what the developer message makes explicit.\n\n    Objective constraints you must check when present:\n    - Required output type syntax (e.g., \"JSON object\", \"single sentence\", \"subject line\").\n    - Hard limits (length ≤ N chars, language required to be English, forbidden words, etc.).\n    - Mandatory tokens or fields the developer explicitly names.\n\n    Out-of-scope (DO NOT FLAG):\n    - Whether the reply \"sounds generic\", \"repeats the prompt\", or \"fully reflects the user's request\" - unless the developer text explicitly demands those qualities.\n    - Creative style, marketing quality, or depth of content unless stated.\n    - Minor stylistic choices (capitalisation, punctuation) that do not violate an explicit rule.\n\n    Pass/Fail rule\n    - If an assistant reply satisfies all objective constraints, it is compliant, even if you personally find it bland or loosely related.\n    - Only record an issue when a concrete, quoted rule is broken.\n\n    Empty assistant list ⇒ immediately return has_issues=false.\n\n    For each assistant example:\n    - USER_EXAMPLES are for context only; never use them to judge compliance.\n    - Judge each assistant reply solely against the explicit constraints you extracted from the developer message.\n    - If a reply breaks a specific, quoted rule, add a line explaining which rule it breaks.\n    - Optionally, suggest a rewrite in one short sentence (add to rewrite_suggestions).\n    - If you are uncertain, do not flag an issue.\n    - Be conservative—uncertain or ambiguous cases are not issues.\n\n    be a little bit more conservative in flagging few shot contradiction issues\n    Output format\n    Return JSON matching FewShotIssues:\n\n    {\n    \"has_issues\": <bool>,\n    \"issues\": [\"<explanation 1>\", \"...\"],\n    \"rewrite_suggestions\": [\"<suggestion 1>\", \"...\"] // may be []\n    }\n    List max five items for both arrays.\n    Provide empty arrays when none.\n    No markdown, no extra keys.\n    \"\"\",\n)\ndev_rewriter = Agent(\n    name=\"dev_rewriter\",\n    model=\"gpt-4.1\",\n    output_type=DevRewriteOutput,\n    instructions=\"\"\"\n    You are Dev-Rewriter-v2.\n\n    You receive:\n    - ORIGINAL_DEVELOPER_MESSAGE\n    - CONTRADICTION_ISSUES (may be empty)\n    - FORMAT_ISSUES (may be empty)\n\n    Rewrite rules\n    Preserve the original intent and capabilities.\n\n    Resolve each contradiction:\n    - Keep the clause that preserves the message intent; remove/merge the conflicting one.\n\n    If FORMAT_ISSUES is non-empty:\n    - Append a new section titled ## Output Format that clearly defines the schema or gives an explicit example.\n\n    Do NOT change few-shot examples.\n\n    Do NOT add new policies or scope.\n\n    Output format (strict JSON)\n    {\n    \"new_developer_message\": \"<full rewritten text>\"\n    }\n    No other keys, no markdown.\n\"\"\",\n)\nfewshot_rewriter = Agent(\n    name=\"fewshot_rewriter\",\n    model=\"gpt-4.1\",\n    output_type=MessagesOutput,\n    instructions=\"\"\"\n    You are FewShot-Rewriter-v2.\n\n    Input payload\n    - NEW_DEVELOPER_MESSAGE (already optimized)\n    - ORIGINAL_MESSAGES (list of user/assistant dicts)\n    - FEW_SHOT_ISSUES (non-empty)\n\n    Task\n    Regenerate only the assistant parts that were flagged.\n    User messages must remain identical.\n    Every regenerated assistant reply MUST comply with NEW_DEVELOPER_MESSAGE.\n\n    After regenerating each assistant reply, verify:\n    - It matches NEW_DEVELOPER_MESSAGE. ENSURE THAT THIS IS TRUE.\n\n    Output format\n    Return strict JSON that matches the MessagesOutput schema:\n\n    {\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"...\"},\n        {\"role\": \"assistant\", \"content\": \"...\"}\n        ]\n    }\n    Guidelines\n    - Preserve original ordering and total count.\n    - If a message was unproblematic, copy it unchanged.\n    \"\"\",\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.Run Optimization Workflow\n\nNow that we've defined our agents and data models, let's examine how they work together to optimize prompts. The optimization workflow consists of several stages that run in parallel to efficiently analyze and improve prompts."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The following function implements the core workflow for prompt optimization. The process follows these steps:\n\n1. **Parallel Analysis**: The function first runs the contradiction checker, format checker, and few-shot consistency checker in parallel (if examples are provided).\n\n2. **Issue Collection**: Results from all checkers are collected and organized.\n\n3. **Conditional Rewriting**: \n   - If contradiction or format issues are found, the prompt is rewritten using the dev_rewriter agent\n   - If few-shot inconsistencies are found, the example responses are updated using the fewshot_rewriter agent\n\n4. **Results Compilation**: The function returns a comprehensive result including the improved prompt, updated examples (if any), and details about the issues that were fixed.\n\nThis parallel approach makes the optimization process efficient, even when working with complex prompts and multiple examples."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _normalize_messages(messages: List[Any]) -> List[Dict[str, str]]:\n    \"\"\"Convert list of pydantic message models to JSON-serializable dicts.\"\"\"\n    result = []\n    for m in messages:\n        if hasattr(m, \"model_dump\"):\n            result.append(m.model_dump())\n        elif isinstance(m, dict) and \"role\" in m and \"content\" in m:\n            result.append({\"role\": str(m[\"role\"]), \"content\": str(m[\"content\"])})\n    return result\n\nasync def optimize_prompt_parallel(\n    developer_message: str,\n    messages: List[\"ChatMessage\"],\n) -> Dict[str, Any]:\n    \"\"\"\n    Runs contradiction, format, and few-shot checkers in parallel,\n    then rewrites the prompt/examples if needed.\n    Returns a unified dict suitable for an API or endpoint.\n    \"\"\"\n\n    # 1. Run all checkers in parallel (contradiction, format, fewshot if there are examples)\n    tasks = [\n        Runner.run(dev_contradiction_checker, developer_message),\n        Runner.run(format_checker, developer_message),\n    ]\n    if messages:\n        fs_input = {\n            \"DEVELOPER_MESSAGE\": developer_message,\n            \"USER_EXAMPLES\": [m.content for m in messages if m.role == \"user\"],\n            \"ASSISTANT_EXAMPLES\": [m.content for m in messages if m.role == \"assistant\"],\n        }\n        tasks.append(Runner.run(fewshot_consistency_checker, json.dumps(fs_input)))\n\n    results = await asyncio.gather(*tasks)\n\n    # Unpack results\n    cd_issues: Issues = results[0].final_output\n    fi_issues: Issues = results[1].final_output\n    fs_issues: FewShotIssues = results[2].final_output if messages else FewShotIssues.no_issues()\n\n    # 3. Rewrites as needed\n    final_prompt = developer_message\n    if cd_issues.has_issues or fi_issues.has_issues:\n        pr_input = {\n            \"ORIGINAL_DEVELOPER_MESSAGE\": developer_message,\n            \"CONTRADICTION_ISSUES\": cd_issues.model_dump(),\n            \"FORMAT_ISSUES\": fi_issues.model_dump(),\n        }\n        pr_res = await Runner.run(dev_rewriter, json.dumps(pr_input))\n        final_prompt = pr_res.final_output.new_developer_message\n\n    final_messages: Union[List[\"ChatMessage\"], List[Dict[str, str]]] = messages\n    if fs_issues.has_issues:\n        mr_input = {\n            \"NEW_DEVELOPER_MESSAGE\": final_prompt,\n            \"ORIGINAL_MESSAGES\": _normalize_messages(messages),\n            \"FEW_SHOT_ISSUES\": fs_issues.model_dump(),\n        }\n        mr_res = await Runner.run(fewshot_rewriter, json.dumps(mr_input))\n        final_messages = mr_res.final_output.messages\n\n    return {\n        \"changes\": True,\n        \"new_developer_message\": final_prompt,\n        \"new_messages\": _normalize_messages(final_messages),\n        \"contradiction_issues\": \"\\n\".join(cd_issues.issues),\n        \"few_shot_contradiction_issues\": \"\\n\".join(fs_issues.issues),\n        \"format_issues\": \"\\n\".join(fi_issues.issues),\n        \"general_improvements\": \"\",\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Examples\n\nLet's explore several practical examples to see how the prompt optimization system works in real-world scenarios. These examples demonstrate the system's ability to identify and fix different types of issues in prompts:"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Example 1: Fixing Contradictions\n\nOur first example demonstrates how the system identifies and resolves contradictions in a prompt. This prompt contains instructions for parsing e-commerce product details from HTML, but it has contradictory rules for handling missing fields."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def example_contradiction():\n    # A prompt with contradictory instructions\n    prompt = \"\"\"Quick-Start Card — Product Parser\n\nGoal  \nDigest raw HTML of an e-commerce product detail page and emit **concise, minified JSON** describing the item.\n\n**Required fields:**  \nname | brand | sku | price.value | price.currency | images[] | sizes[] | materials[] | care_instructions | features[]\n\n**Extraction priority:**  \n1. schema.org/JSON-LD blocks  \n2. <meta> & microdata tags  \n3. Visible DOM fallback (class hints: \"product-name\", \"price\")\n\n** Rules:**  \n- If *any* required field is missing, short-circuit with: `{\"error\": \"FIELD_MISSING:<field>\"}`.\n- Prices: Numeric with dot decimal; strip non-digits (e.g., \"1.299,00 EUR\" → 1299.00 + \"EUR\").\n- Deduplicate images differing only by query string. Keep ≤10 best-res.\n- Sizes: Ensure unit tag (\"EU\", \"US\") and ascending sort.\n- Materials: Title-case and collapse synonyms (e.g., \"polyester 100%\" → \"Polyester\").\n\n**Sample skeleton (minified):**\n```json\n{\"name\":\"\",\"brand\":\"\",\"sku\":\"\",\"price\":{\"value\":0,\"currency\":\"USD\"},\"images\":[\"\"],\"sizes\":[],\"materials\":[],\"care_instructions\":\"\",\"features\":[]}\nNote: It is acceptable to output null for any missing field instead of an error ###\"\"\"\n    \n    result = await optimize_prompt_parallel(prompt, [])\n    \n    # Display the results\n    if result[\"contradiction_issues\"]:\n        print(\"Contradiction issues:\")\n        print(result[\"contradiction_issues\"])\n        print()\n        \n    print(\"Optimized prompt:\")\n    print(result[\"new_developer_message\"])\n    \n# Run the example\nawait example_contradiction()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Example 2: Fixing Inconsistencies Between Prompt and Few-Shot Examples\n\nOur second example shows how the system detects and fixes inconsistencies between a prompt and its accompanying examples. In this case, the prompt requires responses to be in JSON format, but one of the examples doesn't follow this requirement."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def example_fewshot_fix():\n    prompt = \"Respond **only** with JSON using keys `city` (string) and `population` (integer).\"\n    \n    messages = [\n        {\"role\": \"user\", \"content\": \"Largest US city?\"},\n        {\"role\": \"assistant\", \"content\": \"New York City\"},\n        {\"role\": \"user\", \"content\": \"Largest UK city?\"},\n        {\"role\": \"assistant\", \"content\": \"{\\\"city\\\":\\\"London\\\",\\\"population\\\":9541000}\"}\n    ]\n    \n    \n    print(\"Few-shot examples before optimization:\")\n    print(f\"User: {messages[0]['content']}\")\n    print(f\"Assistant: {messages[1]['content']}\")\n    print(f\"User: {messages[2]['content']}\")\n    print(f\"Assistant: {messages[3]['content']}\")\n    print()\n    \n    # Call the optimization API\n    result = await optimize_prompt_parallel(prompt, [ChatMessage(**m) for m in messages])\n    \n    # Display the results\n    if result[\"few_shot_contradiction_issues\"]:\n        print(\"Inconsistency found:\", result[\"few_shot_contradiction_issues\"])\n        print()\n    \n    # Show the optimized few-shot examples\n    optimized_messages = result[\"new_messages\"]\n    print(\"Few-shot examples after optimization:\")\n    print(f\"User: {optimized_messages[0]['content']}\")\n    print(f\"Assistant: {optimized_messages[1]['content']}\")\n    print(f\"User: {optimized_messages[2]['content']}\")\n    print(f\"Assistant: {optimized_messages[3]['content']}\")\n    \n# Run the example\nawait example_fewshot_fix()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Example 3: Clarifying Formats in a Longer Prompt\n\nOur final example shows how the system identifies and fixes unclear format specifications in a more complex prompt. This prompt asks for patent claim translation but doesn't clearly specify the required output format structure."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def example_format_issue():\n    # A prompt with unclear or inconsistent formatting instructions\n    prompt = \"\"\"Task → Translate dense patent claims into 200-word lay summaries with a glossary.\n\nOperating Steps:\n1. Split the claim at semicolons, \"wherein\", or numbered sub-clauses.\n2. For each chunk:\n   a) Identify its purpose.\n   b) Replace technical nouns with everyday analogies.\n   c) Keep quantitative limits intact (e.g., \"≥150 C\").\n3. Flag uncommon science terms with asterisks, and later define them.\n4. Re-assemble into a flowing paragraph; do **not** broaden or narrow the claim's scope.\n5. Omit boilerplate if its removal does not alter legal meaning.\n\nOutput should follow a Markdown template:\n- A summary section.\n- A glossary section with the marked terms and their definitions.\n\nCorner Cases:\n- If the claim is over 5 kB, respond with CLAIM_TOO_LARGE.\n- If claim text is already plain English, skip glossary and state no complex terms detected.\n\nRemember: You are *not* providing legal advice—this is for internal comprehension only.\"\"\"\n\n    # Call the optimization API to check for format issues\n    result = await optimize_prompt_parallel(prompt, [])\n\n    # Display the results\n    if result.get(\"format_issues\"):\n        print(\"Format issues found:\", result[\"format_issues\"])\n        print()\n\n    print(\"Optimized prompt:\")\n    print(result[\"new_developer_message\"])\n\n# Run the example\nawait example_format_issue()"
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Conclusion and Best Practices\n\nThis notebook has demonstrated an early version of OpenAI's prompt optimization system, which uses specialized AI agents to automatically detect and fix common issues in prompts. The system focuses on three key areas:\n\n1. **Detecting contradictions**: Finding and resolving instructions that conflict with each other\n2. **Clarifying format specifications**: Ensuring that output format requirements are clear and comprehensive\n3. **Aligning examples with requirements**: Making sure that example responses match the rules in the prompt\n\nBy applying these optimizations, you can create more effective prompts that lead to more consistent, reliable results from AI models.\n\n### Best Practices for Prompt Engineering\n\nBased on the patterns and fixes we've seen in this notebook, here are some best practices to follow when creating prompts:\n\n1. **Check for contradictions**: Review your prompt carefully to ensure that your instructions don't contradict each other.\n\n2. **Be explicit about formats**: If you expect a specific output format, clearly define the structure, field requirements, and how to handle edge cases.\n\n3. **Provide consistent examples**: Ensure that your few-shot examples follow all the rules specified in your prompt.\n\n4. **Define key terms**: Clearly define any important terms, labels, or categories that might be ambiguous.\n\n5. **Specify error handling**: Provide clear instructions for how the model should handle edge cases or errors.\n\n6. **Organize with clear structure**: Use headings, bullet points, and numbering to make your prompt easy to follow.\n\nBy following these practices and using tools like the optimization system demonstrated in this notebook, you can create more effective prompts that lead to better results from AI models.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}