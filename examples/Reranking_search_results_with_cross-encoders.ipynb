{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f30b8b2",
   "metadata": {},
   "source": [
    "# Reranking search results with cross-encoders\n",
    "\n",
    "This notebook takes you through examples of using a cross-encoder to re-rank search results.\n",
    "\n",
    "This is a common use case with our customers, where you've implemented semantic search using embeddings (produced using a [Bi-encoder](https://www.sbert.net/examples/applications/retrieve_rerank/README.html#retrieval-bi-encoder)) but the results are not exactly what you need. A possible cause is that there is some business rule you can use to rerank the documents such as how recent or how popular a document is. \n",
    "\n",
    "However, often there are subtle domain-specific rules that help determine relevancy, and this is where a cross-encoder can be useful. Cross-encoders are more accurate than bi-encoders but they don't scale well, so using them to re-order a shortened list returned by semantic search is the ideal use case.\n",
    "\n",
    "We'll use text-davinci-003 with logprobs to build a cross-encoder here.\n",
    "\n",
    "This notebook drew on this great [article](https://weaviate.io/blog/cross-encoders-as-reranker) by Weaviate, and this [excellent explanation](https://www.sbert.net/examples/applications/cross-encoder/README.html) of bi-encoders vs. cross-encoders from Sentence Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arxiv\n",
    "!pip install tenacity\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90f3b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from math import exp\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdada886",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "We'll use the arXiv search service for this example, but this step could be performed by any search service you have. The key item to consider is over-fetching slightly to capture all the potentially relevant documents, before re-sorting them.\n",
    "\n",
    "To illustrate our point we'll sort by \"SubmittedDate\" rather than \"Relevance\" so we get some irrelevant results in the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf16c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how do bi-encoders work for sentence embeddings\"\n",
    "search = arxiv.Search(\n",
    "    query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b020a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "\n",
    "for result in search.results():\n",
    "    result_dict = {}\n",
    "\n",
    "    result_dict.update({\"title\": result.title})\n",
    "    result_dict.update({\"summary\": result.summary})\n",
    "\n",
    "    # Taking the first url provided\n",
    "    result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "    result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "    result_list.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fdce882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ModuleFormer: Learning Modular Large Language Models From Uncurated Data',\n",
       " 'summary': 'Large Language Models (LLMs) have achieved remarkable results. But existing\\nmodels are expensive to train and deploy, and it is also difficult to expand\\ntheir knowledge beyond pre-training data without forgetting previous knowledge.\\nThis paper proposes a new neural network architecture, ModuleFormer, that\\nleverages modularity to improve the efficiency and flexibility of large\\nlanguage models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).\\nUnlike the previous SMoE-based modular language model [Gururangan et al.,\\n2021], which requires domain-labeled data to learn domain-specific experts,\\nModuleFormer can induce modularity from uncurated data with its new load\\nbalancing and load concentration losses. ModuleFormer is a modular architecture\\nthat includes two different types of modules, new stick-breaking attention\\nheads, and feedforward experts. Different modules are sparsely activated\\nconditions on the input token during training and inference. In our experiment,\\nwe found that the modular architecture enables three important abilities for\\nlarge pre-trained language models: 1) Efficiency, since ModuleFormer only\\nactivates a subset of its modules for each input token, thus it could achieve\\nthe same performance as dense LLMs with more than two times throughput; 2)\\nExtendability, ModuleFormer is more immune to catastrophic forgetting than\\ndense LLMs and can be easily extended with new modules to learn new knowledge\\nthat is not included in the training data; 3) Specialisation, finetuning\\nModuleFormer could specialize a subset of modules to the finetuning task, and\\nthe task-unrelated modules could be easily pruned for a lightweight deployment.',\n",
       " 'article_url': 'http://arxiv.org/abs/2306.04640v1',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2306.04640v1'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e6abb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ModuleFormer: Learning Modular Large Language Models From Uncurated Data\n",
      "2: Lopsidedness as a tracer of early galactic assembly history\n",
      "3: Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection\n",
      "4: GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image Translation\n",
      "5: A New Paradigm Unifying the Concepts in Particle Abrasion and Breakage\n",
      "6: On the Reliability of Watermarks for Large Language Models\n",
      "7: Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion\n",
      "8: Designing a Better Asymmetric VQGAN for StableDiffusion\n",
      "9: On P^1-stabilization in unstable motivic homotopy theory\n",
      "10: One-loop Integrals from Volumes of Orthoschemes\n"
     ]
    }
   ],
   "source": [
    "for i, result in enumerate(result_list):\n",
    "    print(f\"{i + 1}: {result['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5727678",
   "metadata": {},
   "source": [
    "## Cross-encoder\n",
    "\n",
    "We'll create a cross-encoder using the ```Completions``` endpoint - the key factors to consider here are:\n",
    "- Make your examples domain-specific - the strength of cross-encoders comes when you fine-tune them to your domain.\n",
    "- There is a trade-off between how many potential examples to re-rank vs. processing speed. Consider batching and parallel processing cross-encoder requests to aid user experience.\n",
    "\n",
    "The steps here are:\n",
    "- Build a prompt to assess relevance and provide few-shot examples to tune it to your domain\n",
    "- Return the classification of yes/no as well as the ```logprobs```\n",
    "- Rerank the results by the ```logprobs``` keyed on Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4fdf8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "You are an Assistant responsible for helping detect whether the retrieved document is relevant to the query. For a given input, you need to output a single token: \"Yes\" or \"No\" indicating the retrieved document is relevant to the query.\n",
    "\n",
    "Examples:\n",
    "\"\"\"\n",
    "Query: How to plant a tree?\n",
    "Retrieved document: Cars were invented in 1886, when German inventor Carl Benz patented his Benz Patent-Motorwagen.[3][4][5] Cars became widely available during the 20th century. One of the first cars affordable by the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the US, where they replaced horse-drawn carriages.[6] In Europe and other parts of the world, demand for automobiles did not increase until after World War II.[7] The car is considered an essential part of the developed economy.\n",
    "Relevant to the query?\n",
    "Assistant: No\n",
    "\n",
    "Query: Has the coronavirus vaccine been approved?\n",
    "Retrieved document: The Pfizer-BioNTech COVID-19 vaccine was approved for emergency use in the United States on December 11, 2020.\n",
    "Relevant to the query?\n",
    "Assistant: Yes\n",
    "\n",
    "Query: What is the capital of France?\n",
    "Retrieved document: Paris, France's capital, is a major European city and a global center for art, fashion, gastronomy and culture. Its 19th-century cityscape is crisscrossed by wide boulevards and the River Seine. Beyond such landmarks as the Eiffel Tower and the 12th-century, Gothic Notre-Dame cathedral, the city is known for its cafe culture and designer boutiques along the Rue du Faubourg Saint-Honoré. \n",
    "Relevant to the query?\n",
    "Assistant: Yes\n",
    "\n",
    "Query: What are some papers to learn about PPO reinforcement learning?\n",
    "Retrieved document: Proximal Policy Optimization and its Dynamic Version for Sequence Generation: In sequence generation task, many works use policy gradient for model optimization to tackle the intractable backpropagation issue when maximizing the non-differentiable evaluation metrics or fooling the discriminator in adversarial learning. In this paper, we replace policy gradient with proximal policy optimization (PPO), which is a proved more efficient reinforcement learning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We demonstrate the efficacy of PPO and PPO-dynamic on conditional sequence generation tasks including synthetic experiment and chit-chat chatbot. The results show that PPO and PPO-dynamic can beat policy gradient by stability and performance.\n",
    "Relevant to the query?\n",
    "Assistant: Yes\n",
    "\n",
    "Query: Explain sentence embeddings\n",
    "Retrieved document: Inside the bubble: exploring the environments of reionisation-era Lyman-α emitting galaxies with JADES and FRESCO: We present a study of the environments of 16 Lyman-α emitting galaxies (LAEs) in the reionisation era (5.8<z<8) identified by JWST/NIRSpec as part of the JWST Advanced Deep Extragalactic Survey (JADES). Unless situated in sufficiently (re)ionised regions, Lyman-α emission from these galaxies would be strongly absorbed by neutral gas in the intergalactic medium (IGM). We conservatively estimate sizes of the ionised regions required to reconcile the relatively low Lyman-α velocity offsets (ΔvLyα<300kms−1) with moderately high Lyman-α escape fractions (fesc,Lyα>5%) observed in our sample of LAEs, indicating the presence of ionised ``bubbles'' with physical sizes of the order of 0.1pMpc≲Rion≲1pMpc in a patchy reionisation scenario where the bubbles are embedded in a fully neutral IGM. Around half of the LAEs in our sample are found to coincide with large-scale galaxy overdensities seen in FRESCO at z∼5.8-5.9 and z∼7.3, suggesting Lyman-α transmission is strongly enhanced in such overdense regions, and underlining the importance of LAEs as tracers of the first large-scale ionised bubbles. Considering only spectroscopically confirmed galaxies, we find our sample of UV-faint LAEs (MUV≳−20mag) and their direct neighbours are generally not able to produce the required ionised regions based on the Lyman-α transmission properties, suggesting lower-luminosity sources likely play an important role in carving out these bubbles. These observations demonstrate the combined power of JWST multi-object and slitless spectroscopy in acquiring a unique view of the early stages of Cosmic Reionisation via the most distant LAEs.\n",
    "Relevant to the query?\n",
    "Assistant: No\n",
    "\"\"\"\n",
    "End of Examples\n",
    "\n",
    "Begin!\n",
    "\n",
    "Query: {query}\n",
    "Retrieved document: {document}\n",
    "Relevant to the query?\n",
    "Assistant:\n",
    "'''\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def rerank_document(query, document, output_list):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt.format(query=query, document=content),\n",
    "        temperature=0,\n",
    "        logprobs=1,\n",
    "    )\n",
    "\n",
    "    output_list.append(\n",
    "        (\n",
    "            query,\n",
    "            document,\n",
    "            response[\"choices\"][0][\"text\"],\n",
    "            response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][0],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "753cd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = result_list[0][\"title\"] + \": \" + result_list[0][\"summary\"]\n",
    "\n",
    "# Set logprobs to 1 so we return the most likely token with the likelihood\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prompt.format(query=query, document=content),\n",
    "    temperature=0,\n",
    "    logprobs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7efef2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result was Yes\n",
      "Logprobs was -0.0013786046\n",
      "\n",
      "Below is the full logprobs object\n",
      "\n",
      "\n",
      "{\n",
      "  \"text_offset\": [\n",
      "    6216\n",
      "  ],\n",
      "  \"token_logprobs\": [\n",
      "    -0.0013786046\n",
      "  ],\n",
      "  \"tokens\": [\n",
      "    \"Yes\"\n",
      "  ],\n",
      "  \"top_logprobs\": [\n",
      "    {\n",
      "      \"Yes\": -0.0013786046\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = response[\"choices\"][0]\n",
    "print(f\"Result was {result['text']}\")\n",
    "print(f\"Logprobs was {result['logprobs']['token_logprobs'][0]}\")\n",
    "print(\"\\nBelow is the full logprobs object\\n\\n\")\n",
    "print(result[\"logprobs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7683b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "for x in result_list:\n",
    "    content = x[\"title\"] + \": \" + x[\"summary\"]\n",
    "\n",
    "    try:\n",
    "        rerank_document(query, document=content, output_list=output_list)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57576313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how do bi-encoders work for sentence embeddings',\n",
       "  'ModuleFormer: Learning Modular Large Language Models From Uncurated Data: Large Language Models (LLMs) have achieved remarkable results. But existing\\nmodels are expensive to train and deploy, and it is also difficult to expand\\ntheir knowledge beyond pre-training data without forgetting previous knowledge.\\nThis paper proposes a new neural network architecture, ModuleFormer, that\\nleverages modularity to improve the efficiency and flexibility of large\\nlanguage models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).\\nUnlike the previous SMoE-based modular language model [Gururangan et al.,\\n2021], which requires domain-labeled data to learn domain-specific experts,\\nModuleFormer can induce modularity from uncurated data with its new load\\nbalancing and load concentration losses. ModuleFormer is a modular architecture\\nthat includes two different types of modules, new stick-breaking attention\\nheads, and feedforward experts. Different modules are sparsely activated\\nconditions on the input token during training and inference. In our experiment,\\nwe found that the modular architecture enables three important abilities for\\nlarge pre-trained language models: 1) Efficiency, since ModuleFormer only\\nactivates a subset of its modules for each input token, thus it could achieve\\nthe same performance as dense LLMs with more than two times throughput; 2)\\nExtendability, ModuleFormer is more immune to catastrophic forgetting than\\ndense LLMs and can be easily extended with new modules to learn new knowledge\\nthat is not included in the training data; 3) Specialisation, finetuning\\nModuleFormer could specialize a subset of modules to the finetuning task, and\\nthe task-unrelated modules could be easily pruned for a lightweight deployment.',\n",
       "  'Yes',\n",
       "  -0.0013881554),\n",
       " ('how do bi-encoders work for sentence embeddings',\n",
       "  'Lopsidedness as a tracer of early galactic assembly history: Large-scale asymmetries (i.e. lopsidedness) are a common feature in the\\nstellar density distribution of nearby disk galaxies both in low- and\\nhigh-density environments. In this work, we characterize the present-day\\nlopsidedness in a sample of 1435 disk-like galaxies selected from the TNG50\\nsimulation. We find that the percentage of lopsided galaxies (10%-30%) is in\\ngood agreement with observations if we use similar radial ranges to the\\nobservations. However, the percentage (58%) significantly increases if we\\nextend our measurement to larger radii. We find a mild or lack of correlation\\nbetween lopsidedness amplitude and environment at z=0 and a strong correlation\\nbetween lopsidedness and galaxy morphology regardless of the environment.\\nPresent-day galaxies with more extended disks, flatter inner galactic regions\\nand lower central stellar mass density (i.e. late-type disk galaxies) are\\ntypically more lopsided than galaxies with smaller disks, rounder inner\\ngalactic regions and higher central stellar mass density (i.e. early-type disk\\ngalaxies). Interestingly, we find that lopsided galaxies have, on average, a\\nvery distinct star formation history within the last 10 Gyr, with respect to\\ntheir symmetric counterparts. Symmetric galaxies have typically assembled at\\nearly times (~8-6 Gyr ago) with relatively short and intense bursts of central\\nstar formation, while lopsided galaxies have assembled on longer timescales and\\nwith milder initial bursts of star formation, continuing building up their mass\\nuntil z=0. Overall, these results indicate that lopsidedness in present-day\\ndisk galaxies is connected to the specific evolutionary histories of the\\ngalaxies that shaped their distinct internal properties.',\n",
       "  'No',\n",
       "  -0.0008327981),\n",
       " ('how do bi-encoders work for sentence embeddings',\n",
       "  \"Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection: Neural sequence models based on the transformer architecture have\\ndemonstrated remarkable \\\\emph{in-context learning} (ICL) abilities, where they\\ncan perform new tasks when prompted with training and test examples, without\\nany parameter update to the model. This work first provides a comprehensive\\nstatistical theory for transformers to perform ICL. Concretely, we show that\\ntransformers can implement a broad class of standard machine learning\\nalgorithms in context, such as least squares, ridge regression, Lasso, learning\\ngeneralized linear models, and gradient descent on two-layer neural networks,\\nwith near-optimal predictive power on various in-context data distributions.\\nUsing an efficient implementation of in-context gradient descent as the\\nunderlying mechanism, our transformer constructions admit mild size bounds, and\\ncan be learned with polynomially many pretraining sequences.\\n  Building on these ``base'' ICL algorithms, intriguingly, we show that\\ntransformers can implement more complex ICL procedures involving\\n\\\\emph{in-context algorithm selection}, akin to what a statistician can do in\\nreal life -- A \\\\emph{single} transformer can adaptively select different base\\nICL algorithms -- or even perform qualitatively different tasks -- on different\\ninput sequences, without any explicit prompting of the right algorithm or task.\\nWe both establish this in theory by explicit constructions, and also observe\\nthis phenomenon experimentally. In theory, we construct two general mechanisms\\nfor algorithm selection with concrete examples: pre-ICL testing, and post-ICL\\nvalidation. As an example, we use the post-ICL validation mechanism to\\nconstruct a transformer that can perform nearly Bayes-optimal ICL on a\\nchallenging task -- noisy linear models with mixed noise levels.\\nExperimentally, we demonstrate the strong in-context algorithm selection\\ncapabilities of standard transformer architectures.\",\n",
       "  'Yes',\n",
       "  -0.001507777)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29a4dc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>query</th>\n",
       "      <th>document</th>\n",
       "      <th>prediction</th>\n",
       "      <th>logprobs</th>\n",
       "      <th>probability</th>\n",
       "      <th>cleaned_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>ModuleFormer: Learning Modular Large Language ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.001388</td>\n",
       "      <td>0.998613</td>\n",
       "      <td>0.998613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>Lopsidedness as a tracer of early galactic ass...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000833</td>\n",
       "      <td>0.999168</td>\n",
       "      <td>0.000832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>Transformers as Statisticians: Provable In-Con...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.001508</td>\n",
       "      <td>0.998493</td>\n",
       "      <td>0.998493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>GP-UNIT: Generative Prior for Versatile Unsupe...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>A New Paradigm Unifying the Concepts in Partic...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.999175</td>\n",
       "      <td>0.000825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            query  \\\n",
       "0      0  how do bi-encoders work for sentence embeddings   \n",
       "1      1  how do bi-encoders work for sentence embeddings   \n",
       "2      2  how do bi-encoders work for sentence embeddings   \n",
       "3      3  how do bi-encoders work for sentence embeddings   \n",
       "4      4  how do bi-encoders work for sentence embeddings   \n",
       "\n",
       "                                            document prediction  logprobs  \\\n",
       "0  ModuleFormer: Learning Modular Large Language ...        Yes -0.001388   \n",
       "1  Lopsidedness as a tracer of early galactic ass...         No -0.000833   \n",
       "2  Transformers as Statisticians: Provable In-Con...        Yes -0.001508   \n",
       "3  GP-UNIT: Generative Prior for Versatile Unsupe...         No -0.000779   \n",
       "4  A New Paradigm Unifying the Concepts in Partic...         No -0.000825   \n",
       "\n",
       "   probability  cleaned_probability  \n",
       "0     0.998613             0.998613  \n",
       "1     0.999168             0.000832  \n",
       "2     0.998493             0.998493  \n",
       "3     0.999221             0.000779  \n",
       "4     0.999175             0.000825  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = pd.DataFrame(\n",
    "    output_list, columns=[\"query\", \"document\", \"prediction\", \"logprobs\"]\n",
    ").reset_index()\n",
    "# Use exp() to convert logprobs into probability\n",
    "output_df[\"probability\"] = output_df[\"logprobs\"].apply(exp)\n",
    "# Reorder based on likelihood of being Yes\n",
    "output_df[\"cleaned_probability\"] = output_df.apply(\n",
    "    lambda x: x[\"probability\"] * -1 + 1\n",
    "    if x[\"prediction\"] == \"No\"\n",
    "    else x[\"probability\"],\n",
    "    axis=1,\n",
    ")\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a647f120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>query</th>\n",
       "      <th>document</th>\n",
       "      <th>prediction</th>\n",
       "      <th>logprobs</th>\n",
       "      <th>probability</th>\n",
       "      <th>cleaned_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>ModuleFormer: Learning Modular Large Language ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.001388</td>\n",
       "      <td>0.998613</td>\n",
       "      <td>0.998613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>Transformers as Statisticians: Provable In-Con...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.001508</td>\n",
       "      <td>0.998493</td>\n",
       "      <td>0.998493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>On P^1-stabilization in unstable motivic homot...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000883</td>\n",
       "      <td>0.999117</td>\n",
       "      <td>0.000883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>Designing a Better Asymmetric VQGAN for Stable...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000837</td>\n",
       "      <td>0.999164</td>\n",
       "      <td>0.000836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>Lopsidedness as a tracer of early galactic ass...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000833</td>\n",
       "      <td>0.999168</td>\n",
       "      <td>0.000832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>A New Paradigm Unifying the Concepts in Partic...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.999175</td>\n",
       "      <td>0.000825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>GP-UNIT: Generative Prior for Versatile Unsupe...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>Contrastive Lift: 3D Object Instance Segmentat...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000775</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>0.000775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>One-loop Integrals from Volumes of Orthoscheme...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000648</td>\n",
       "      <td>0.999352</td>\n",
       "      <td>0.000648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>how do bi-encoders work for sentence embeddings</td>\n",
       "      <td>On the Reliability of Watermarks for Large Lan...</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.000585</td>\n",
       "      <td>0.999415</td>\n",
       "      <td>0.000585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            query  \\\n",
       "0      0  how do bi-encoders work for sentence embeddings   \n",
       "2      2  how do bi-encoders work for sentence embeddings   \n",
       "8      8  how do bi-encoders work for sentence embeddings   \n",
       "7      7  how do bi-encoders work for sentence embeddings   \n",
       "1      1  how do bi-encoders work for sentence embeddings   \n",
       "4      4  how do bi-encoders work for sentence embeddings   \n",
       "3      3  how do bi-encoders work for sentence embeddings   \n",
       "6      6  how do bi-encoders work for sentence embeddings   \n",
       "9      9  how do bi-encoders work for sentence embeddings   \n",
       "5      5  how do bi-encoders work for sentence embeddings   \n",
       "\n",
       "                                            document prediction  logprobs  \\\n",
       "0  ModuleFormer: Learning Modular Large Language ...        Yes -0.001388   \n",
       "2  Transformers as Statisticians: Provable In-Con...        Yes -0.001508   \n",
       "8  On P^1-stabilization in unstable motivic homot...         No -0.000883   \n",
       "7  Designing a Better Asymmetric VQGAN for Stable...         No -0.000837   \n",
       "1  Lopsidedness as a tracer of early galactic ass...         No -0.000833   \n",
       "4  A New Paradigm Unifying the Concepts in Partic...         No -0.000825   \n",
       "3  GP-UNIT: Generative Prior for Versatile Unsupe...         No -0.000779   \n",
       "6  Contrastive Lift: 3D Object Instance Segmentat...         No -0.000775   \n",
       "9  One-loop Integrals from Volumes of Orthoscheme...         No -0.000648   \n",
       "5  On the Reliability of Watermarks for Large Lan...         No -0.000585   \n",
       "\n",
       "   probability  cleaned_probability  \n",
       "0     0.998613             0.998613  \n",
       "2     0.998493             0.998493  \n",
       "8     0.999117             0.000883  \n",
       "7     0.999164             0.000836  \n",
       "1     0.999168             0.000832  \n",
       "4     0.999175             0.000825  \n",
       "3     0.999221             0.000779  \n",
       "6     0.999225             0.000775  \n",
       "9     0.999352             0.000648  \n",
       "5     0.999415             0.000585  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return reranked results\n",
    "output_df.sort_values(by=[\"cleaned_probability\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372d311",
   "metadata": {},
   "source": [
    "## Consolidation\n",
    "\n",
    "We've used the ```Completions``` endpoint from OpenAI to build our cross-encoder, but this area is well-served by the open-source community. [Here](https://huggingface.co/cross-encoder/mmarco-mMiniLMv2-L12-H384-v1) is an example from HuggingFace, for example.\n",
    "\n",
    "We hope you find this useful for tuning your search use cases, and look forward to seeing what you build."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_test",
   "language": "python",
   "name": "openai_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
