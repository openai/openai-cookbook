{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b66af9",
   "metadata": {},
   "source": [
    "# Building an **AIÂ ResearchÂ Assistant** with the OpenAIÂ AgentsÂ SDK\n",
    "\n",
    "This notebook provides a reference patterns for implementing a multiâ€‘agent AI Research Assistant that can plan, search, curate, and draft highâ€‘quality reports with citations.\n",
    "\n",
    "While the Deep Research feature is avaialble in ChatGPT, however, individual and companies may want to implement their own API based solution for a more finegrained control over the output.\n",
    "\n",
    "With support for Agents, and built-in tools such as Code Interpreter, Web Search, and File Search, - Responses API makes building your own Research Assistant fast and easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd3942",
   "metadata": {},
   "source": [
    "## TableÂ ofÂ Contents\n",
    "1. [Overview](#overview)\n",
    "2. [SolutionÂ Workflow](#workflow)\n",
    "3. [Highâ€‘LevelÂ Architecture](#architecture)\n",
    "4. [AgentÂ DefinitionsÂ (PseudoÂ Code)](#agents)\n",
    "    * ResearchÂ PlanningÂ Agent\n",
    "    * WebÂ SearchÂ Agent\n",
    "    * KnowledgeÂ AssistantÂ Agent\n",
    "    * ReportÂ CreationÂ Agent\n",
    "    * DataÂ AnalysisÂ Agent (optional)\n",
    "    * Imageâ€‘GenÂ Agent (optional)\n",
    "5. [GuardrailsÂ &Â BestÂ Practices](#best-practices)\n",
    "6. [RisksÂ &Â Mitigation](#risks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e358e",
   "metadata": {},
   "source": [
    "### 1Â â€”Â Overview <a id='overview'></a>\n",
    "The AI Research Assistant helps drives better research quality and faster turnaround for knowledge content.\n",
    "\n",
    "1. **Performs autonomous Internet research** to gather the most recent sources.\n",
    "2. **Incorporates internal data sources** such as a Company's proprietery knowledge sources. \n",
    "3. **Reduces analyst effort from days to minutes** by automating search, curation and firstâ€‘draft writing.\n",
    "4. **Produces draft reports with citations** and builtâ€‘in hallucination detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb6ce3",
   "metadata": {},
   "source": [
    "### 2Â â€”Â SolutionÂ Workflow <a id='workflow'></a>\n",
    "The typical workflow consists of five orchestrated steps: \n",
    "\n",
    "|Â StepÂ |Â PurposeÂ |Â ModelÂ |\n",
    "|------|---------|-------|\n",
    "|Â **QueryÂ Expansion**Â | Draft multiâ€‘facet prompts / hypotheses | `o4-mini` |\n",
    "|Â **Searchâ€‘TermÂ Generation**Â | Expand/clean user query into rich keyword list | `gptâ€‘4.1` |\n",
    "|Â **ConductÂ Research**Â | Run web & internal searches, rank & summarise results | `gptâ€‘4.1` + tools |\n",
    "|Â **DraftÂ Report**Â | Produce first narrative with reasoning & inline citations | `o3` |\n",
    "|Â **ReportÂ Expansion**Â | Polish formatting, add charts / images / appendix | `gptâ€‘4.1` + tools |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4e6dc",
   "metadata": {},
   "source": [
    "### 3Â â€”Â Highâ€‘LevelÂ Architecture <a id='architecture'></a>\n",
    "The following diagram groups agents and tools:\n",
    "\n",
    "* **ResearchÂ PlanningÂ Agent** â€“ interprets the user request and produces a research plan/agenda.\n",
    "* **KnowledgeÂ AssistantÂ Agent** â€“ orchestrates parallel web & file searches via builtâ€‘in tools, curates shortâ€‘term memory.\n",
    "* **WebÂ SearchÂ Agent(s)** â€“ perform Internet queries, deduplicate, rank and summarise pages.\n",
    "* **ReportÂ CreationÂ Agent** â€“ consumes curated corpus and drafts the structured report.\n",
    "* **(Optional) DataÂ AnalysisÂ Agent** â€“ executes code for numeric/CSV analyses via the CodeÂ Interpreter tool.\n",
    "* **(Optional) Imageâ€‘GenÂ Agent** â€“ generates illustrative figures.\n",
    "\n",
    "Input/output guardrails wrap user prompts and final content for policy, safety and citation checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3464739",
   "metadata": {},
   "source": [
    "### 4Â â€”Â Pre-requisites <a id='pre-requisites'></a>\n",
    "\n",
    "Create a virual environment  \n",
    "\n",
    "Install dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a16ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai openai-agents --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69135215",
   "metadata": {},
   "source": [
    "### 5Â â€”Â Agents (PseudoÂ Code) <a id='agents'></a>\n",
    "Below are skeletal class definitions illustrating how each agentâ€™s policy and toolâ€‘usage might look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3062e",
   "metadata": {},
   "source": [
    "#### Step 1 - Query Expansion\n",
    "\n",
    "The query expansion step ensures the subsequent agents conducting research have sufficient context of user's inquiry. \n",
    "\n",
    "The first step is to understand user's intent, and make sure the user has provided sufficinet details for subsequent agents to search the web, build a knowledge repository, and prepare a deepdive report. The `query_expansion_agent.py` accomplishes this with the prompt that outlines minimum information needed from the user to generate a report. This could include timeframe, industry, target audience, etc. The prompt can be tailored to the need of your deepresearch assistant. The agent will put a `is_task_clear` yes or no, when its no, it would prompt the user with additional questions, if sufficent information is available, it would output the expanded prompt. \n",
    "\n",
    "This is also an opportunity to enforce input guardrails for any research topics that you'd like to restrict the user from reserarching based on your usage policies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2618f60",
   "metadata": {},
   "source": [
    "##### Input Guardrails with Agents SDK \n",
    "Let's assume our ficticious guardrail is to prevent the user from generating a non-AI releated topic report. For this we will define a guardrail agent. The guardrail agent `topic_guradrail.py` checks whether the topic is related to AI, if not, it raises an execption. The function `ai_topic_guardrail` is passed to the `QueryExpansionAgent()` as `input_guardrails`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "620f9e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš« Guardrail tripped â€“ not an AI topic: The user's request focuses on the luxury goods market, which pertains to market trends in the luxury sector rather than artificial intelligence. Therefore, it is not about AI.\n"
     ]
    }
   ],
   "source": [
    "from ai_research_assistant_resources.agents_tools_registry.query_expansion_agent import QueryExpansionAgent\n",
    "from agents import InputGuardrailTripwireTriggered\n",
    "\n",
    "query_expansion_agent_guardrail_check = QueryExpansionAgent()\n",
    "\n",
    "try:\n",
    "\n",
    "    result = await query_expansion_agent_guardrail_check.task(\"Write a research report on the latest trends in luxury goods market\")\n",
    "\n",
    "except InputGuardrailTripwireTriggered as e:\n",
    "    reason = e.guardrail_result.output.output_info.reasoning\n",
    "    #            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "    #            GuardrailFunctionOutput\n",
    "    print(\"ğŸš« Guardrail tripped â€“ not an AI topic:\", reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77364239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The task is not clear. The agent asks:\n",
      " 1. What timeframe should the report cover (e.g., the past year, the past five years, up to current date)?\n",
      "2. Should the report focus on specific AI subfields (e.g., natural language processing, computer vision, reinforcement learning) or provide a general overview?\n",
      "3. Are there particular industries or application domains (e.g., healthcare, finance, manufacturing) you want the report to emphasize?\n",
      "4. What length or depth do you expect for the report (e.g., a brief summary, a detailed 20-page analysis)?\n",
      "5. Who is the target audience for the report (e.g., technical researchers, business executives, policymakers)?\n",
      "\n",
      "\n",
      "user input:  5 years, AI in healthcare, exec summary \n",
      "\n",
      "Expanded query:\n",
      " Draft an executive summary research report on the latest trends in AI developments in healthcare over the past five years. Summarize key advancements across major subfields such as diagnostic imaging, predictive analytics, natural language processing for clinical documentation, and personalized medicine. Highlight impactful case studies, emerging technologies, regulatory considerations, and potential challenges. Provide high-level insights on market adoption, ROI metrics, and strategic recommendations for healthcare executives.\n"
     ]
    }
   ],
   "source": [
    "from ai_research_assistant_resources.agents_tools_registry.query_expansion_agent import QueryExpansionAgent\n",
    "\n",
    "query_expansion_agent = QueryExpansionAgent()\n",
    "\n",
    "# Initial prompt to the agent\n",
    "prompt: str = \"Draft a research report on the latest trends in AI developments\"\n",
    "expanded_query = \"\" \n",
    "\n",
    "try: \n",
    "\n",
    "    while True:\n",
    "        # Execute the agent with the current prompt\n",
    "        result = await query_expansion_agent.task(prompt)\n",
    "\n",
    "        # When the task is clear, show the expanded query and exit.\n",
    "        if result.is_task_clear == \"yes\":\n",
    "            expanded_query = result.expanded_query\n",
    "            print(\"\\nExpanded query:\\n\", expanded_query)\n",
    "            break\n",
    "\n",
    "        # Otherwise, display the clarifying questions and ask the user for input.\n",
    "        print(\"\\nThe task is not clear. The agent asks:\\n\", result.questions)\n",
    "        prompt = input(\"Please provide the missing details so I can refine the query: \")\n",
    "        print(\"\\n\")\n",
    "        print(\"user input: \", prompt)\n",
    "        \n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Non-AI topic guardrail tripped!\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b10e7",
   "metadata": {},
   "source": [
    "#### Step 2 - Web Search Terms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725969cb",
   "metadata": {},
   "source": [
    "Conducting Web search is typically an integral part of the deep research process. First we generate web search terms relevant to the research report. In the next step we will search the web and build a knowledge repository of the data.\n",
    "\n",
    "The `WebSearchTermsGenerationAgent` takes as input the the expanded prompt, and generates succient search terms. You can structure the search term generation prompt according to your user's typical requirements such as include adjacent industries in the search terms, include competitors, etc. Additionally, you can also control how much data you want to gather e.g., number of search terms to generate. In our case, we will limit to 3 search terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15e0c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Latest AI trends in healthcare 2025 report\n",
      "2. Advancements in AI for diagnostic imaging and predictive analytics 2020-2025\n",
      "3. Impactful AI case studies in healthcare and market adoption analysis 2025\n"
     ]
    }
   ],
   "source": [
    "from ai_research_assistant_resources.agents_tools_registry.web_search_terms_generation_agent import WebSearchTermsGenerationAgent\n",
    "\n",
    "search_terms_agent = WebSearchTermsGenerationAgent(3)\n",
    "\n",
    "result = await search_terms_agent.task(expanded_query)\n",
    "\n",
    "search_terms_raw = result\n",
    "\n",
    "for i, query in enumerate(search_terms_raw.Search_Queries, start=1):\n",
    "    print(f\"{i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feeaae8",
   "metadata": {},
   "source": [
    "#### Step 3 - Scroll the Web build a inventory of data sources \n",
    "\n",
    "We will use custom web search to identify and knowledge content to form the baseline for our report. You can learn more about building custom web search and retreival here. [Building a Bring Your Own Browser (BYOB) Tool for Web Browsing and Summarization](https://cookbook.openai.com/examples/third_party/web_search_with_google_api_bring_your_own_browser_tool). You will also need a Google Custom Search API key and Custom Search Engine ID (CSE ID) in a .env file at the root. \n",
    "\n",
    "NOTE: The reason for using custom web search is provide more finegrained control over which information is retreived, and guardrails such as excluding competitor's content from your report. \n",
    "\n",
    "This is a 3 step process: \n",
    "\n",
    "1. Obtain the search results (top 10 pages)\n",
    "2. Scroll the pages, and summarize the key points \n",
    "3. Output guardrails to weedout irrelevant or undesirable results (e.g., the timeframe of the content doesn't align with user's need, or mentions a competitor)\n",
    "\n",
    "prerequisite pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b7260c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Query 1: Latest AI trends in healthcare 2025 report\n",
      "Search Query 2: Advancements in AI for diagnostic imaging and predictive analytics 2020-2025\n",
      "Search Query 3: Impactful AI case studies in healthcare and market adoption analysis 2025\n",
      "Results written to research_results.json\n"
     ]
    }
   ],
   "source": [
    "from ai_research_assistant_resources.utils.web_search_and_util import get_results_for_search_term\n",
    "import json\n",
    "\n",
    "research_results = []\n",
    "\n",
    "for idx, query in enumerate(search_terms_raw.Search_Queries, 1):\n",
    "    print(f\"Search Query {idx}: {query}\")\n",
    "    research_results.append(get_results_for_search_term(query))\n",
    "\n",
    "if research_results:                       \n",
    "    with open(\"research_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(research_results, f, indent=2, ensure_ascii=False)\n",
    "    print(\"Results written to research_results.json\")\n",
    "else:\n",
    "    print(\"No results returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9758743",
   "metadata": {},
   "source": [
    "### Step-4: Create a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076a6f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Report written to REPORT_DRAFT.md\n"
     ]
    }
   ],
   "source": [
    "from ai_research_assistant_resources.agents_tools_registry.report_writing_agent import (\n",
    "    ReportWritingAgent,\n",
    ")\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Load research results\n",
    "# ------------------------------------------------------------------\n",
    "with open(\"research_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    research_results = f.read()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Draft the report\n",
    "# ------------------------------------------------------------------\n",
    "outline = \"\"\" Draft a comprehensive research report analyzing the latest trends in artificial intelligence (AI) developments within the healthcare industry over the past five years. The report should evaluate advancements in machine learning, deep learning, natural language processing, medical imaging, and other relevant AI applications, while also examining regulatory, ethical, and operational impacts on healthcare delivery. Include detailed case studies, emerging research areas, and recommendations for future innovation in the industry.\"\"\"  # â† customise as needed\n",
    "\n",
    "report_agent = ReportWritingAgent(research_resources=research_results)\n",
    "\n",
    "draft_md = await report_agent.task(outline)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Persist to file\n",
    "# ------------------------------------------------------------------\n",
    "Path(\"REPORT_DRAFT.md\").write_text(draft_md, encoding=\"utf-8\")\n",
    "print(\"âœ… Report written to REPORT_DRAFT.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f4f1a8",
   "metadata": {},
   "source": [
    "### Step-5: Report Expansion or Scouting for additional data points (OPTIONAL)\n",
    "\n",
    "If you have a large corpus of data, you may have a secondary report expansion agent review each section of the report, and add content that may have been overlooked in the first pass by the report writer. This can be selectively done for a section, or for all sections based on your use case. \n",
    "\n",
    "While it is beyond the purview of this Cookbook, the overall architecture is as follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ee533",
   "metadata": {},
   "source": [
    "### Step-6: Organize the with References and Table of Content \n",
    "\n",
    "We let the LLM focus on generating the content, the content formatting such as creating a Table of Content upfront, and move references to the end.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad9315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def update_references(file_path, search_results_json):\n",
    "    \"\"\"\n",
    "    Update the references in a Markdown file by extracting unique URLs from <source></source> tags and creating a\n",
    "    References section at the end of the file.\n",
    "\n",
    "    :param file_path: The path to the Markdown file.\n",
    "    \"\"\"\n",
    "    global content\n",
    "    # Read the markdown_content of the MD file\n",
    "\n",
    "    global url_to_title\n",
    "    # Load the search results\n",
    "    # Create a dictionary for quick lookup of titles by URL\n",
    "    url_to_title = {entry[\"URL\"]: entry[\"title\"] for entry in search_results_json}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    # Remove the existing References section if it exists\n",
    "    content = re.sub(r'\\n## References[\\s\\S]*', '', content)\n",
    "    content = re.sub(r'\\n### References[\\s\\S]*', '', content)\n",
    "\n",
    "    # Find all <source></source> tags and extract the URLs\n",
    "    sources = re.findall(r'<source>(.*?)</source>', content)\n",
    "    # Eliminate duplicates while maintaining order\n",
    "    unique_sources = []\n",
    "    unique_references = {}\n",
    "    for source in sources:\n",
    "        if source not in unique_sources:\n",
    "            unique_sources.append(source)\n",
    "        unique_references[source] = unique_sources.index(source) + 1\n",
    "    # Create the References section\n",
    "    references_section = \"\\n\\n## References\\n\"\n",
    "\n",
    "    for i, source in enumerate(unique_sources, start=1):\n",
    "        title = url_to_title.get(source, \"Source not found in the search results\")\n",
    "        references_section += f\"{i}. [{title}]({source})\\n\"\n",
    "        # references_section += f\"{i}. {source}\\n\"\n",
    "\n",
    "    # Replace <source></source> tags with [reference #]\n",
    "    for source, reference_number in unique_references.items():\n",
    "        # markdown_content = markdown_content.replace(f'<source>{source}</source>', f'[reference {reference_number}]')\n",
    "        content = content.replace(f'<source>{source}</source>', f'<sup>[[{reference_number}]({source})]</sup>')\n",
    "    # Append the References section to the markdown_content\n",
    "    content += references_section\n",
    "    # Save the modified markdown_content back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "        \n",
    "        \n",
    "def add_toc_to_markdown(file_path):\n",
    "    \"\"\"\n",
    "    Add a Table of Contents (TOC) to a Markdown file by generating links to the headings in the file.\n",
    "\n",
    "    :param file_path: The path to the Markdown file.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_toc_line(line):\n",
    "        level = line.count('#') - 2\n",
    "        heading = line.strip().lstrip('#').strip()\n",
    "        link = heading.lower().replace(' ', '-').replace('.', '').replace(',', '')\n",
    "        return f\"{'  ' * level}- [{heading}](#{link})\\n\"\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    toc_lines = []\n",
    "    content_start_index = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('## '):\n",
    "            content_start_index = i\n",
    "            break\n",
    "\n",
    "    for line in lines[content_start_index:]:\n",
    "        if line.startswith('## ') or line.startswith('### '):\n",
    "            toc_lines.append(generate_toc_line(line))\n",
    "\n",
    "    toc_content = \"# Table of Contents\\n\" + ''.join(toc_lines) + \"\\n---\\n\\n\"\n",
    "    new_content = toc_content + ''.join(lines)\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(new_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a890c80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "255e0a06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb69c797",
   "metadata": {},
   "source": [
    "### 5Â â€”Â GuardrailsÂ &Â BestÂ Practices <a id='best-practices'></a>\n",
    "* **Crawlâ€¯â†’â€¯Walkâ€¯â†’â€¯Run**: start with a single agent, then expand into a swarm. \n",
    "* **Expose intermediate reasoning** (â€œshow the mathâ€) to build user trust.  \n",
    "* **Parameterise UX** so analysts can tweak report format and source mix. \n",
    "* **Native OpenAI tools first** (web browsing, file ingestion) before reinventing lowâ€‘level retrieval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdcab82",
   "metadata": {},
   "source": [
    "### 6Â â€”Â RisksÂ &Â Mitigation <a id='risks'></a>\n",
    "|Â PitfallÂ |Â MitigationÂ |\n",
    "|---------|------------|\n",
    "| Scopeâ€‘creep & endless roadmap | Narrow MVP & SMART milestones | îˆ€fileciteîˆ‚turn1file4îˆ‚L23-L24îˆ |\n",
    "| Hallucinations & weak guardrails | Goldenâ€‘set evals, RAG with citation checks | îˆ€fileciteîˆ‚turn1file4îˆ‚L25-L26îˆ |\n",
    "| Runâ€‘away infra costs | Cost curve modelling; efficient models + autoscaling | îˆ€fileciteîˆ‚turn1file4îˆ‚L27-L28îˆ |\n",
    "| Talent gaps | Upskill & leverage AgentsÂ SDK to offload core reasoning | îˆ€fileciteîˆ‚turn1file4îˆ‚L29-L30îˆ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40dcf3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
