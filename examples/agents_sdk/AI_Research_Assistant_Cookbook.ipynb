{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b66af9",
   "metadata": {},
   "source": [
    "# Build a **Multi‑Agent AI Research Assistant** with the OpenAI Agents SDK & Responses API\n",
    "\n",
    "This notebook provides a reference patterns for implementing a multi‑agent AI Research Assistant that can plan, search, curate, and draft high‑quality reports with citations.\n",
    "\n",
    "While the Deep Research feature is available in ChatGPT, however, individual and companies may want to implement their own API based solution for a more fine grained control over the output.\n",
    "\n",
    "With support for Agents, and built-in tools such as Code Interpreter, Web Search, and File Search, - Responses API makes building your own Research Assistant fast and easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd3942",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Solution Workflow](#workflow)\n",
    "3. [High‑Level Architecture](#architecture)\n",
    "4. [Agent Definitions (Pseudo Code)](#agents)\n",
    "    * Research Planning Agent\n",
    "    * Web Search Agent\n",
    "    * Knowledge Assistant Agent\n",
    "    * Report Creation Agent\n",
    "    * Data Analysis Agent (optional)\n",
    "    * Image‑Gen Agent (optional)\n",
    "5. [Guardrails & Best Practices](#best-practices)\n",
    "6. [Risks & Mitigation](#risks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e358e",
   "metadata": {},
   "source": [
    "### 1 — Overview <a id='overview'></a>\n",
    "The AI Research Assistant helps drives better research quality and faster turnaround for knowledge content.\n",
    "\n",
    "1. **Performs autonomous Internet research** to gather the most recent sources.\n",
    "2. **Incorporates internal data sources** such as a Company's proprietery knowledge sources. \n",
    "3. **Reduces analyst effort from days to minutes** by automating search, curation and first‑draft writing.\n",
    "4. **Produces draft reports with citations** and built‑in hallucination detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb6ce3",
   "metadata": {},
   "source": [
    "### 2 — Solution Workflow <a id='workflow'></a>\n",
    "The typical workflow consists of five orchestrated steps: \n",
    "\n",
    "| Step | Purpose | Model |\n",
    "|------|---------|-------|\n",
    "| **Query Expansion** | Draft multi‑facet prompts / hypotheses | `o4-mini` |\n",
    "| **Search‑Term Generation** | Expand/clean user query into rich keyword list | `gpt‑4.1` |\n",
    "| **Conduct Research** | Run web & internal searches, rank & summarize results | `gpt‑4.1` + tools |\n",
    "| **Draft Report** | Produce first narrative with reasoning & inline citations | `o3` |\n",
    "| **Report Expansion** | Polish formatting, add charts / images / appendix | `gpt‑4.1` + tools |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4e6dc",
   "metadata": {},
   "source": [
    "### 3 — High‑Level Architecture <a id='architecture'></a>\n",
    "The following diagram groups agents and tools:\n",
    "\n",
    "* **Research Planning Agent** – interprets the user request and produces a research plan/agenda.\n",
    "* **Knowledge Assistant Agent** – orchestrates parallel web & file searches via built‑in tools, curates short‑term memory.\n",
    "* **Web Search Agent(s)** – perform Internet queries, deduplicate, rank and summarize pages.\n",
    "* **Report Creation Agent** – consumes curated corpus and drafts the structured report.\n",
    "* **(Optional) Data Analysis Agent** – executes code for numeric/CSV analyses via the Code Interpreter tool.\n",
    "* **(Optional) Image‑Gen Agent** – generates illustrative figures.\n",
    "\n",
    "Input/output guardrails wrap user prompts and final content for policy, safety and citation checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3464739",
   "metadata": {},
   "source": [
    "### 4 — Pre-requisites <a id='pre-requisites'></a>\n",
    "\n",
    "Create a virual environment  \n",
    "\n",
    "Install dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a16ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai openai-agents --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69135215",
   "metadata": {},
   "source": [
    "### 5 — Agents (Pseudo Code) <a id='agents'></a>\n",
    "Below are skeletal class definitions illustrating how each agent’s policy and tool‑usage might look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3062e",
   "metadata": {},
   "source": [
    "#### Step 1 - Query Expansion\n",
    "\n",
    "The query expansion step ensures the subsequent agents conducting research have sufficient context of user's inquiry. \n",
    "\n",
    "The first step is to understand user's intent, and make sure the user has provided sufficient details for subsequent agents to search the web, build a knowledge repository, and prepare a deep dive report. The `query_expansion_agent.py` accomplishes this with the prompt that outlines minimum information needed from the user to generate a report. This could include timeframe, industry, target audience, etc. The prompt can be tailored to the need of your deep research assistant. The agent will put a `is_task_clear` yes or no, when its no, it would prompt the user with additional questions, if sufficient information is available, it would output the expanded prompt. \n",
    "\n",
    "This is also an opportunity to enforce input guardrails for any research topics that you'd like to restrict the user from researching based on your usage policies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2618f60",
   "metadata": {},
   "source": [
    "##### Input Guardrails with Agents SDK \n",
    "Let's assume our fictitious guardrail is to prevent the user from generating a non-AI related topic report. For this we will define a guardrail agent. The guardrail agent `topic_content_guardrail.py` checks whether the topic is related to AI, if not, it raises an exception. The function `ai_topic_guardrail` is passed to the `QueryExpansionAgent()` as `input_guardrails`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "620f9e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚫 Guardrail tripped – not an AI topic: The user's request focuses on the luxury goods market, which pertains to market trends in the luxury sector rather than artificial intelligence. Therefore, it is not about AI.\n"
     ]
    }
   ],
   "source": [
    "from ai_research_assistant_resources.agents_tools_registry.query_expansion_agent import QueryExpansionAgent\n",
    "from agents import InputGuardrailTripwireTriggered\n",
    "\n",
    "query_expansion_agent_guardrail_check = QueryExpansionAgent()\n",
    "\n",
    "try:\n",
    "\n",
    "    result = await query_expansion_agent_guardrail_check.task(\"Write a research report on the latest trends in luxury goods market\")\n",
    "\n",
    "except InputGuardrailTripwireTriggered as e:\n",
    "    reason = e.guardrail_result.output.output_info.reasoning\n",
    "    #            └─────┬─────┘\n",
    "    #            GuardrailFunctionOutput\n",
    "    print(\"🚫 Guardrail tripped – not an AI topic:\", reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77364239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The task is not clear. The agent asks:\n",
      " 1. What timeframe should the report cover (e.g., the past year, the past five years, up to current date)?\n",
      "2. Should the report focus on specific AI subfields (e.g., natural language processing, computer vision, reinforcement learning) or provide a general overview?\n",
      "3. Are there particular industries or application domains (e.g., healthcare, finance, manufacturing) you want the report to emphasize?\n",
      "4. What length or depth do you expect for the report (e.g., a brief summary, a detailed 20-page analysis)?\n",
      "5. Who is the target audience for the report (e.g., technical researchers, business executives, policymakers)?\n",
      "\n",
      "\n",
      "user input:  5 years, AI in healthcare, exec summary \n",
      "\n",
      "Expanded query:\n",
      " Draft an executive summary research report on the latest trends in AI developments in healthcare over the past five years. Summarize key advancements across major subfields such as diagnostic imaging, predictive analytics, natural language processing for clinical documentation, and personalized medicine. Highlight impactful case studies, emerging technologies, regulatory considerations, and potential challenges. Provide high-level insights on market adoption, ROI metrics, and strategic recommendations for healthcare executives.\n"
     ]
    }
   ],
   "source": [
    "from ai_research_assistant_resources.agents_tools_registry.query_expansion_agent import QueryExpansionAgent\n",
    "\n",
    "query_expansion_agent = QueryExpansionAgent()\n",
    "\n",
    "# Initial prompt to the agent\n",
    "prompt: str = \"Draft a research report on the latest trends in AI developments\"\n",
    "expanded_query = \"\" \n",
    "\n",
    "try: \n",
    "\n",
    "    while True:\n",
    "        # Execute the agent with the current prompt\n",
    "        result = await query_expansion_agent.task(prompt)\n",
    "\n",
    "        # When the task is clear, show the expanded query and exit.\n",
    "        if result.is_task_clear == \"yes\":\n",
    "            expanded_query = result.expanded_query\n",
    "            print(\"\\nExpanded query:\\n\", expanded_query)\n",
    "            break\n",
    "\n",
    "        # Otherwise, display the clarifying questions and ask the user for input.\n",
    "        print(\"\\nThe task is not clear. The agent asks:\\n\", result.questions)\n",
    "        prompt = input(\"Please provide the missing details so I can refine the query: \")\n",
    "        print(\"\\n\")\n",
    "        print(\"user input: \", prompt)\n",
    "        \n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Non-AI topic guardrail tripped!\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b10e7",
   "metadata": {},
   "source": [
    "#### Step 2 - Web Search Terms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725969cb",
   "metadata": {},
   "source": [
    "Conducting Web search is typically an integral part of the deep research process. First we generate web search terms relevant to the research report. In the next step we will search the web and build a knowledge repository of the data.\n",
    "\n",
    "The `WebSearchTermsGenerationAgent` takes as input the the expanded prompt, and generates succinct search terms. You can structure the search term generation prompt according to your user's typical requirements such as include adjacent industries in the search terms, include competitors, etc. Additionally, you can also control how much data you want to gather e.g., number of search terms to generate. In our case, we will limit to 3 search terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15e0c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Latest AI trends in healthcare 2025 report\n",
      "2. Advancements in AI for diagnostic imaging and predictive analytics 2020-2025\n",
      "3. Impactful AI case studies in healthcare and market adoption analysis 2025\n"
     ]
    }
   ],
   "source": [
    "from ai_research_assistant_resources.agents_tools_registry.web_search_terms_generation_agent import WebSearchTermsGenerationAgent\n",
    "\n",
    "search_terms_agent = WebSearchTermsGenerationAgent(3)\n",
    "\n",
    "result = await search_terms_agent.task(expanded_query)\n",
    "\n",
    "search_terms_raw = result\n",
    "\n",
    "for i, query in enumerate(search_terms_raw.Search_Queries, start=1):\n",
    "    print(f\"{i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feeaae8",
   "metadata": {},
   "source": [
    "### Step 3 - Web Search: Build an Inventory of Data Sources\n",
    "\n",
    "In this step, we will use the OpenAI web search tool that is integrated into the `responses` API to identify and collect knowledge content that will form the baseline for our report. This tool allows you to search the web and retrieve relevant information and citations directly within your workflow, without needing to set up any external search APIs or browser automation.\n",
    "\n",
    "You can learn more about the OpenAI web search tool here: [OpenAI Web Search Tool Documentation](https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses).\n",
    "\n",
    "The OpenAI web search tool is a convenient, out-of-the-box solution for most research use cases. However, if you require more fine-grained control over the information retrieved (for example, to exclude certain sources, apply custom filters, or use a specific search engine), you can also build and use your own browser-based or Google Custom Search integration. For an example of building a custom web search and retrieval pipeline, see [Building a Bring Your Own Browser (BYOB) Tool for Web Browsing and Summarization](https://cookbook.openai.com/examples/third_party/web_search_with_google_api_bring_your_own_browser_tool).\n",
    "\n",
    "The process for building your research data inventory using the OpenAI web search tool is as follows:\n",
    "\n",
    "1. Obtain the search results (e.g., top 10 relevant pages) for each search term.\n",
    "2. Extract and summarize the key points from each result.\n",
    "3. Optionally, apply output guardrails to filter out irrelevant or undesirable results (for example, based on publication date, source, or content).\n",
    "#\n",
    "If you choose to implement your own custom search or browser-based retrieval, you may need additional setup such as API keys or environment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b7260c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Query 1: Latest AI trends in healthcare 2025 report\n",
      "Search Query 2: Advancements in AI for diagnostic imaging and predictive analytics 2020-2025\n",
      "Search Query 3: Impactful AI case studies in healthcare and market adoption analysis 2025\n",
      "Results written to research_results.json\n"
     ]
    }
   ],
   "source": [
    "from ai_research_assistant_resources.utils.web_search_and_util import get_results_for_search_term\n",
    "import json\n",
    "\n",
    "research_results = []\n",
    "\n",
    "for idx, query in enumerate(search_terms_raw.Search_Queries, 1):\n",
    "    print(f\"Search Query {idx}: {query}\")\n",
    "    research_results.append(get_results_for_search_term(query))\n",
    "\n",
    "if research_results:                       \n",
    "    with open(\"research_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(research_results, f, indent=2, ensure_ascii=False)\n",
    "    print(\"Results written to research_results.json\")\n",
    "else:\n",
    "    print(\"No results returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9758743",
   "metadata": {},
   "source": [
    "### Step-4: Create a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076a6f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Report written to REPORT_DRAFT.md\n"
     ]
    }
   ],
   "source": [
    "from ai_research_assistant_resources.agents_tools_registry.report_writing_agent import (\n",
    "    ReportWritingAgent,\n",
    ")\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Load research results\n",
    "# ------------------------------------------------------------------\n",
    "with open(\"research_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    research_results = f.read()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Draft the report\n",
    "# ------------------------------------------------------------------\n",
    "outline = \"\"\" Draft a comprehensive research report analyzing the latest trends in artificial intelligence (AI) developments within the healthcare industry over the past five years. The report should evaluate advancements in machine learning, deep learning, natural language processing, medical imaging, and other relevant AI applications, while also examining regulatory, ethical, and operational impacts on healthcare delivery. Include detailed case studies, emerging research areas, and recommendations for future innovation in the industry.\"\"\"  # ← customize as needed\n",
    "\n",
    "report_agent = ReportWritingAgent(research_resources=research_results)\n",
    "\n",
    "draft_md = await report_agent.task(outline)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Persist to file\n",
    "# ------------------------------------------------------------------\n",
    "Path(\"REPORT_DRAFT.md\").write_text(draft_md, encoding=\"utf-8\")\n",
    "print(\"✅ Report written to REPORT_DRAFT.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f4f1a8",
   "metadata": {},
   "source": [
    "### Step-5: Report Expansion or Scouting for additional data points (OPTIONAL)\n",
    "\n",
    "If you have a large corpus of data, you may have a secondary report expansion agent review each section of the report, and add content that may have been overlooked in the first pass by the report writer. This can be selectively done for a section, or for all sections based on your use case. \n",
    "\n",
    "While it is beyond the purview of this Cookbook, the overall architecture is as follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ee533",
   "metadata": {},
   "source": [
    "### Step-6: Organize the with References and Table of Content \n",
    "\n",
    "We let the LLM focus on generating the content, the content formatting such as creating a Table of Content upfront, and move references to the end.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad9315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def update_references(file_path, search_results_json):\n",
    "    \"\"\"\n",
    "    Update the references in a Markdown file by extracting unique URLs from <source></source> tags and creating a\n",
    "    References section at the end of the file.\n",
    "\n",
    "    :param file_path: The path to the Markdown file.\n",
    "    \"\"\"\n",
    "    global content\n",
    "    # Read the markdown_content of the MD file\n",
    "\n",
    "    global url_to_title\n",
    "    # Load the search results\n",
    "    # Create a dictionary for quick lookup of titles by URL\n",
    "    url_to_title = {entry[\"URL\"]: entry[\"title\"] for entry in search_results_json}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    # Remove the existing References section if it exists\n",
    "    content = re.sub(r'\\n## References[\\s\\S]*', '', content)\n",
    "    content = re.sub(r'\\n### References[\\s\\S]*', '', content)\n",
    "\n",
    "    # Find all <source></source> tags and extract the URLs\n",
    "    sources = re.findall(r'<source>(.*?)</source>', content)\n",
    "    # Eliminate duplicates while maintaining order\n",
    "    unique_sources = []\n",
    "    unique_references = {}\n",
    "    for source in sources:\n",
    "        if source not in unique_sources:\n",
    "            unique_sources.append(source)\n",
    "        unique_references[source] = unique_sources.index(source) + 1\n",
    "    # Create the References section\n",
    "    references_section = \"\\n\\n## References\\n\"\n",
    "\n",
    "    for i, source in enumerate(unique_sources, start=1):\n",
    "        title = url_to_title.get(source, \"Source not found in the search results\")\n",
    "        references_section += f\"{i}. [{title}]({source})\\n\"\n",
    "        # references_section += f\"{i}. {source}\\n\"\n",
    "\n",
    "    # Replace <source></source> tags with [reference #]\n",
    "    for source, reference_number in unique_references.items():\n",
    "        # markdown_content = markdown_content.replace(f'<source>{source}</source>', f'[reference {reference_number}]')\n",
    "        content = content.replace(f'<source>{source}</source>', f'<sup>[[{reference_number}]({source})]</sup>')\n",
    "    # Append the References section to the markdown_content\n",
    "    content += references_section\n",
    "    # Save the modified markdown_content back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "        \n",
    "        \n",
    "def add_toc_to_markdown(file_path):\n",
    "    \"\"\"\n",
    "    Add a Table of Contents (TOC) to a Markdown file by generating links to the headings in the file.\n",
    "\n",
    "    :param file_path: The path to the Markdown file.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_toc_line(line):\n",
    "        level = line.count('#') - 2\n",
    "        heading = line.strip().lstrip('#').strip()\n",
    "        link = heading.lower().replace(' ', '-').replace('.', '').replace(',', '')\n",
    "        return f\"{'  ' * level}- [{heading}](#{link})\\n\"\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    toc_lines = []\n",
    "    content_start_index = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('## '):\n",
    "            content_start_index = i\n",
    "            break\n",
    "\n",
    "    for line in lines[content_start_index:]:\n",
    "        if line.startswith('## ') or line.startswith('### '):\n",
    "            toc_lines.append(generate_toc_line(line))\n",
    "\n",
    "    toc_content = \"# Table of Contents\\n\" + ''.join(toc_lines) + \"\\n---\\n\\n\"\n",
    "    new_content = toc_content + ''.join(lines)\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(new_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69c797",
   "metadata": {},
   "source": [
    "### 5 — Guardrails & Best Practices <a id='best-practices'></a>\n",
    "* **Crawl → Walk → Run**: start with a single agent, then expand into a swarm. \n",
    "* **Expose intermediate reasoning** (“show the math”) to build user trust.  \n",
    "* **Parameterize UX** so analysts can tweak report format and source mix. \n",
    "* **Native OpenAI tools first** (web browsing, file ingestion) before reinventing low‑level retrieval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdcab82",
   "metadata": {},
   "source": [
    "### 6 — Risks & Mitigation <a id='risks'></a>\n",
    "| Pitfall | Mitigation |\n",
    "|---------|------------|\n",
    "| Scope‑creep & endless roadmap | Narrow MVP & SMART milestones | fileciteturn1file4L23-L24 |\n",
    "| Hallucinations & weak guardrails | Golden‑set evals, RAG with citation checks | fileciteturn1file4L25-L26 |\n",
    "| Run‑away infra costs | Cost curve modelling; efficient models + autoscaling | fileciteturn1file4L27-L28 |\n",
    "| Talent gaps | Upskill & leverage Agents SDK to offload core reasoning | fileciteturn1file4L29-L30 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40dcf3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
