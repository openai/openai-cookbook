{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcfc573",
   "metadata": {},
   "source": [
    "# Context Engineering: Short-Term Memory Management with Sessions from OpenAI Agents SDK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab798a",
   "metadata": {},
   "source": [
    "In this cookbook, we’ll explore how to **manage context effectively using the `Session` object from the [OpenAI Agents SDK](https://github.com/openai/openai-agents-python)**.\n",
    "\n",
    "AI agents often operate in **long-running, multi-turn interactions**, where keeping the right balance of context is critical. If too much is carried forward, the model risks distraction, inefficiency, or outright failure. If too little is preserved, the agent loses coherence. This guide focuses on two proven context management techniques—**trimming** and **compression**—to keep agents fast, reliable, and cost-efficient.\n",
    "\n",
    "\n",
    "#### Why Context Management Matters\n",
    "\n",
    "* **Sustained coherence across long threads** – Keep the agent anchored to the latest user goal without dragging along stale details. Session-level trimming and summaries prevent “yesterday’s plan” from overriding today’s ask.\n",
    "* **Higher tool-call accuracy** – Focused context improves function selection and argument filling, reducing retries, timeouts, and cascading failures during multi-tool runs.\n",
    "* **Lower latency & cost** – Smaller, sharper prompts cut tokens per turn and attention load.\n",
    "* **Error & hallucination containment** – Summaries act as “clean rooms” that correct or omit prior mistakes; trimming avoids amplifying bad facts (“context poisoning”) turn after turn.\n",
    "* **Easier debugging & observability** – Stable summaries and bounded histories make logs comparable: you can diff summaries, attribute regressions, and reproduce failures reliably.\n",
    "* **Multi-issue and handoff resilience** – In multi-problem chats, per-issue mini-summaries let the agent pause/resume, escalate to humans, or hand off to another agent while staying consistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e1913",
   "metadata": {},
   "source": [
    "![Memory Comparison in AI Agents](../../images/memory_comparison.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068564c",
   "metadata": {},
   "source": [
    "\n",
    "#### Real-World Scenario\n",
    "\n",
    "We’ll ground the techniques in a practical example for one of the common long-running tasks, such as:\n",
    "\n",
    "* **Multi-turn Customer Service Conversations**\n",
    "In extended conversations about tech products—spanning both hardware and software—customers often surface multiple issues over time. The agent must stay consistent and goal-focused while retaining only the essentials rather than hauling along every past detail.\n",
    "\n",
    "#### Techniques Covered\n",
    "\n",
    "To address these challenges, we introduce two concrete approaches using OpenAI Agents SDK:\n",
    "\n",
    "1. **Trimming Messages** – dropping older turns while keeping the last N turns.\n",
    "2. **Summarizing Messages** – compressing prior exchanges into structured, shorter representations.\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc613968",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this cookbook, you must set up the following accounts and complete a few setup actions. These prerequisites are essential to interact with the APIs used in this project.\n",
    "\n",
    "#### Step0: OpenAI Account\n",
    "\n",
    "- **Purpose:**  \n",
    "  You need an OpenAI account to access language models and use the Agents SDK featured in this cookbook.\n",
    "\n",
    "- **Action:**  \n",
    "  [Sign up for an OpenAI account](https://openai.com) if you don’t already have one. Once you have an account, create an API key by visiting the [OpenAI API Keys page](https://platform.openai.com/api-keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9a109",
   "metadata": {},
   "source": [
    "#### Step1: Install the Required Libraries\n",
    "\n",
    "Below we install the `openai-agents` library (the [OpenAI Agents SDK](https://github.com/openai/openai-agents-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87818100",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai-agents nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6348e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b39f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import set_tracing_disabled\n",
    "set_tracing_disabled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b7952",
   "metadata": {},
   "source": [
    "Let's test the installed libraries by defining and running an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe54469a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating AI agents ensures reliability, safety, ethical alignment, performance accuracy, and helps avoid biases, improving overall trust and effectiveness.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents import Agent, Runner\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"Reply very concisely.\",\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent, \"Tell me why it is important to evaluate AI agents.\")\n",
    "print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39dab9d",
   "metadata": {},
   "source": [
    "### Define Agents\n",
    "\n",
    "We can start by defining the necessary components from Agents SDK Library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2725d9d",
   "metadata": {},
   "source": [
    "#### Customer Service Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4451ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_agent = Agent(\n",
    "    name=\"Customer Support Assistant\",\n",
    "    model=\"gpt-5\",\n",
    "    instructions=(\n",
    "        \"You are a patient, step-by-step IT support assistant. \"\n",
    "        \"Your role is to help customers troubleshoot and resolve issues with devices and software. \"\n",
    "        \"Guidelines:\\n\"\n",
    "        \"- Be concise and use numbered steps where possible.\\n\"\n",
    "        \"- Ask only one focused, clarifying question at a time before suggesting next actions.\\n\"\n",
    "        \"- Track and remember multiple issues across the conversation; update your understanding as new problems emerge.\\n\"\n",
    "        \"- When a problem is resolved, briefly confirm closure before moving to the next.\\n\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8074e05",
   "metadata": {},
   "source": [
    "## 1. Context Trimming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8cf80",
   "metadata": {},
   "source": [
    "#### Implement Custom Session Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993098b",
   "metadata": {},
   "source": [
    "We are using [Session](https://openai.github.io/openai-agents-python/sessions/) object from [OpenAI Agents Python SDK](https://openai.github.io/openai-agents-python/). Here’s a `MyCustomSession` implementation that **keeps only the last N turns** (a “turn” = one user message and everything until the next user message—including the assistant reply and any tool calls/results). It’s in-memory and trims automatically on every write and read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b468c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "from collections import deque\n",
    "from typing import Any, Deque, Dict, List, Tuple, cast\n",
    "\n",
    "from agents.memory.session import SessionABC\n",
    "from agents.items import TResponseInputItem  # typically a dict-like item\n",
    "\n",
    "\n",
    "def _is_user_msg(item: TResponseInputItem) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: treat items with role=='user' as user messages.\n",
    "    Falls back to type=='message' and author checks if your SDK uses a different shape.\n",
    "    Adjust this if your item schema differs.\n",
    "    \"\"\"\n",
    "    if isinstance(item, dict):\n",
    "        role = cast(Dict[str, Any], item).get(\"role\")\n",
    "        if role == \"user\":\n",
    "            return True\n",
    "        # Some SDKs encode messages as {\"type\": \"message\", \"role\": \"...\"}\n",
    "        if item.get(\"type\") == \"message\" and item.get(\"role\") == \"user\":\n",
    "            return True\n",
    "    # Extend here if you carry custom classes with .role attribute:\n",
    "    role_attr = getattr(item, \"role\", None)\n",
    "    return role_attr == \"user\"\n",
    "\n",
    "\n",
    "class TrimmingSession(SessionABC):\n",
    "    \"\"\"\n",
    "    Custom session that keeps only the last N user-turns.\n",
    "    A 'turn' is defined as a user message and all subsequent items\n",
    "    (assistant/tool calls/results) up to—but not including—the next user message.\n",
    "\n",
    "    Works entirely in memory. If you need persistence, replace the in-memory\n",
    "    deque with your storage of choice (SQLite/Redis/etc.), preserving the\n",
    "    trimming logic in `_trim_to_last_turns`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, session_id: str, max_turns: int = 8):\n",
    "        self.session_id = session_id\n",
    "        self.max_turns = max(1, max_turns)\n",
    "        self._items: Deque[TResponseInputItem] = deque()  # full chronological log\n",
    "        self._lock = asyncio.Lock()\n",
    "\n",
    "    # ---- SessionABC API ----\n",
    "\n",
    "    async def get_items(self, limit: int | None = None) -> List[TResponseInputItem]:\n",
    "        \"\"\"\n",
    "        Return the history trimmed to the last N turns.\n",
    "        If `limit` is provided, return at most that many most-recent items\n",
    "        from within the trimmed history.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            trimmed = self._trim_to_last_turns(list(self._items))\n",
    "            if limit is not None and limit >= 0:\n",
    "                return trimmed[-limit:]\n",
    "            return trimmed\n",
    "\n",
    "    async def add_items(self, items: List[TResponseInputItem]) -> None:\n",
    "        \"\"\"\n",
    "        Append new items, then trim to last N turns.\n",
    "        \"\"\"\n",
    "        if not items:\n",
    "            return\n",
    "        async with self._lock:\n",
    "            self._items.extend(items)\n",
    "            # Trim in place by rebuilding from trimmed list\n",
    "            trimmed = self._trim_to_last_turns(list(self._items))\n",
    "            self._items.clear()\n",
    "            self._items.extend(trimmed)\n",
    "\n",
    "    async def pop_item(self) -> TResponseInputItem | None:\n",
    "        \"\"\"\n",
    "        Remove and return the most recent item (post-trim).\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            if not self._items:\n",
    "                return None\n",
    "            return self._items.pop()\n",
    "\n",
    "    async def clear_session(self) -> None:\n",
    "        \"\"\"\n",
    "        Remove all items for this session.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            self._items.clear()\n",
    "\n",
    "    # ---- Helpers ----\n",
    "\n",
    "    def _trim_to_last_turns(self, items: List[TResponseInputItem]) -> List[TResponseInputItem]:\n",
    "        \"\"\"\n",
    "        Keep only the suffix of `items` that contains the last `max_turns` user messages\n",
    "        and everything after the earliest of those user messages.\n",
    "\n",
    "        Algorithm:\n",
    "          1) Scan from the end to find indices of the last `max_turns` user messages.\n",
    "          2) Cut history to start from the earliest of those (inclusive).\n",
    "        Edge cases:\n",
    "          - If there are fewer than `max_turns` user messages, keep entire history.\n",
    "          - If there are no user messages yet, treat all existing items as a single turn and keep them.\n",
    "        \"\"\"\n",
    "        if not items:\n",
    "            return items\n",
    "\n",
    "        # Find indices of user messages scanning from the end\n",
    "        user_indices: List[int] = []\n",
    "        for idx in range(len(items) - 1, -1, -1):\n",
    "            if _is_user_msg(items[idx]):\n",
    "                user_indices.append(idx)\n",
    "                if len(user_indices) >= self.max_turns:\n",
    "                    break\n",
    "\n",
    "        if not user_indices:\n",
    "            # No user messages yet; keep everything\n",
    "            return items\n",
    "\n",
    "        # The earliest index among the last N user messages\n",
    "        cut_from = min(user_indices)  # since we collected from the end\n",
    "        return items[cut_from:]\n",
    "\n",
    "    # ---- Optional convenience API (not part of SessionABC) ----\n",
    "\n",
    "    async def set_max_turns(self, max_turns: int) -> None:\n",
    "        async with self._lock:\n",
    "            self.max_turns = max(1, int(max_turns))\n",
    "            trimmed = self._trim_to_last_turns(list(self._items))\n",
    "            self._items.clear()\n",
    "            self._items.extend(trimmed)\n",
    "\n",
    "    async def raw_items(self) -> List[TResponseInputItem]:\n",
    "        \"\"\"Return the untrimmed in-memory log (for debugging).\"\"\"\n",
    "        async with self._lock:\n",
    "            return list(self._items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd5b28",
   "metadata": {},
   "source": [
    "Let's define the custom session object we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ad6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the last 8 turns (user + assistant/tool interactions)\n",
    "session = TrimmingSession(\"my_session\", max_turns=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab99cd",
   "metadata": {},
   "source": [
    "**How to choose the right `max_turns`?**\n",
    "\n",
    "Determining this parameter usually requires experimentation with your conversation history. One approach is to extract the total number of turns across conversations and analyze their distribution. Another option is to use an LLM to evaluate conversations—identifying how many tasks or issues each one contains and calculating the average number of turns needed per issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c59d40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"There is a red light on the dashboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03b15552",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(\n",
    "    support_agent,\n",
    "    message,\n",
    "    session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c94beb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = await session.get_items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "626a6e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'There is a red light on the dashboard.', 'role': 'user'},\n",
       " {'id': 'rs_68ba0a2abf3c8196adc68a947b457ae0001b849827e34d0e',\n",
       "  'summary': [],\n",
       "  'type': 'reasoning',\n",
       "  'content': []},\n",
       " {'id': 'msg_68ba0a3480748196b7059d0e23d87350001b849827e34d0e',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'Which device or system is the dashboard on (e.g., car, printer, router, software)?',\n",
       "    'type': 'output_text',\n",
       "    'logprobs': []}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "395c1bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ca19df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('user', 'There is a red light on the dashboard.')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation[0]['role'], conversation[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51f60675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example flow\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Hi, my router won't connect.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Let's check your firmware version.\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Firmware v1.0.3; still failing.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Try a factory reset.\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Reset done; error 42 now.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"test1\"}])\n",
    "# At this point, with max_turns=3, everything *before* the earliest of the last 3 user\n",
    "# messages is summarized into a synthetic pair, and the last 3 turns remain verbatim.\n",
    "\n",
    "history = await session.get_items()\n",
    "# Pass `history` into your agent runner / responses call as the conversation context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6dc29a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07430b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Firmware v1.0.3; still failing.'},\n",
       " {'role': 'assistant', 'content': 'Try a factory reset.'},\n",
       " {'role': 'user', 'content': 'Reset done; error 42 now.'},\n",
       " {'role': 'assistant', 'content': 'test1'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e8de9a",
   "metadata": {},
   "source": [
    "Below, you can see how the trimming session works for max_turns=3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42471c87",
   "metadata": {},
   "source": [
    "![Context Trimming in Session](../../images/trimingSession.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8dd9c5",
   "metadata": {},
   "source": [
    "**What counts as a “turn”**\n",
    "\n",
    "* A **turn** = one **user** message **plus everything that follows it** (assistant replies, reasoning, tool calls, tool results) **until the next user message**.\n",
    "\n",
    "**When trimming happens**\n",
    "\n",
    "* On **write**: `add_items(...)` appends the new items, then immediately trims the stored history.\n",
    "* On **read**: `get_items(...)` returns a **trimmed** view (so even if you bypassed a write, reads won’t leak old turns).\n",
    "\n",
    "**How it decides what to keep**\n",
    "\n",
    "1. Treat any item with `role == \"user\"` as a **user message** (via `_is_user_msg`).\n",
    "2. Scan the history **backwards** and collect the indices of the last **N** user messages (`max_turns`).\n",
    "3. Find the **earliest** index among those N user messages.\n",
    "4. **Keep everything from that index to the end**; drop everything before it.\n",
    "\n",
    "That preserves each complete turn boundary: if the earliest kept user message is at index `k`, you also keep all assistant/tool items that came after `k`.\n",
    "\n",
    "**Edge cases**\n",
    "\n",
    "* **Fewer than N user messages**: keep **everything** (no trimming yet).\n",
    "* **No user messages**: keep **everything** (treat as a single in-progress turn).\n",
    "* **`limit` in `get_items(limit=…)`**: applied **after** trimming; returns only the last `limit` items of the already-trimmed slice.\n",
    "\n",
    "**Tiny example**\n",
    "\n",
    "History (old → new):\n",
    "\n",
    "```\n",
    "0: user(\"Hi\")\n",
    "1: assistant(\"Hello!\")\n",
    "2: tool_call(\"lookup\")\n",
    "3: tool_result(\"…\")\n",
    "4: user(\"It didn't work\")\n",
    "5: assistant(\"Try rebooting\")\n",
    "6: user(\"Rebooted, now error 42\")\n",
    "7: assistant(\"On it\")\n",
    "```\n",
    "\n",
    "With `max_turns = 2`, the last two user messages are at indices **4** and **6**.\n",
    "Earliest of those is **4** → keep items **4..7**, drop **0..3**.\n",
    "\n",
    "**Why this works well**\n",
    "\n",
    "* You always keep **complete** turns, so the assistant retains the immediate context it needs (both the user’s last asks and the assistant/tool steps in between).\n",
    "* It prevents context bloat by discarding older turns wholesale, not just messages.\n",
    "\n",
    "**Customization knobs**\n",
    "\n",
    "* Change `max_turns` at init or via `set_max_turns(...)`.\n",
    "* Adjust `_is_user_msg(...)` if your item schema differs.\n",
    "* If you’d rather cap by **message count** or **tokens**, replace `_trim_to_last_turns(...)` or add a second pass that measures tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa349f",
   "metadata": {},
   "source": [
    "## 2. Context Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c453e",
   "metadata": {},
   "source": [
    "Once the history exceeds `max_turns`. It keeps the most recent N user turns intact, **summarizes everything older into two synthetic messages**:\n",
    "\n",
    "* `user`: *\"Summarize the conversation we had so far.\"*\n",
    "* `assistant`: *{generated summary}*\n",
    "\n",
    "The shadow prompt from the user to request the summarization added to keep natural flow of the conversation without confusing the chat flow between user and assistant. Final version of the generated summary injected to assistant message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b8d8c",
   "metadata": {},
   "source": [
    "**Summarization Prompt**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380492d",
   "metadata": {},
   "source": [
    "A well-crafted summarization prompt is essential for preserving the context of a conversation, and it should always be tailored to the specific use case. Think of it like **being a customer support agent handing off a case to the next agent**. What concise yet critical details would they need to continue smoothly? The prompt should strike the right balance: not overloaded with unnecessary information, but not so sparse that key context is lost. Achieving this balance requires careful design and ongoing experimentation to fine-tune the level of detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7be8b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = \"\"\"\n",
    "You are a senior customer-support assistant for tech devices, setup, and software issues.\n",
    "Compress the earlier conversation into a precise, reusable snapshot for future turns.\n",
    "\n",
    "Before you write (do this silently):\n",
    "- Contradiction check: compare user claims with system instructions and tool definitions/logs; note any conflicts or reversals.\n",
    "- Temporal ordering: sort key events by time; the most recent update wins. If timestamps exist, keep them.\n",
    "- Hallucination control: if any fact is uncertain/not stated, mark it as UNVERIFIED rather than guessing.\n",
    "\n",
    "Write a structured, factual summary ≤ 200 words using the sections below (use the exact headings):\n",
    "\n",
    "• Product & Environment:\n",
    "  - Device/model, OS/app versions, network/context if mentioned.\n",
    "\n",
    "• Reported Issue:\n",
    "  - Single-sentence problem statement (latest state).\n",
    "\n",
    "• Steps Tried & Results:\n",
    "  - Chronological bullets (include tool calls + outcomes, errors, codes).\n",
    "\n",
    "• Identifiers:\n",
    "  - Ticket #, device serial/model, account/email (only if provided).\n",
    "\n",
    "• Timeline Milestones:\n",
    "  - Key events with timestamps or relative order (e.g., 10:32 install → 10:41 error).\n",
    "\n",
    "• Tool Performance Insights:\n",
    "  - What tool calls worked/failed and why (if evident).\n",
    "\n",
    "• Current Status & Blockers:\n",
    "  - What’s resolved vs pending; explicit blockers preventing progress.\n",
    "\n",
    "• Next Recommended Step:\n",
    "  - One concrete action (or two alternatives) aligned with policies/tools.\n",
    "\n",
    "Rules:\n",
    "- Be concise, no fluff; use short bullets, verbs first.\n",
    "- Do not invent new facts; quote error strings/codes exactly when available.\n",
    "- If previous info was superseded, note “Superseded:” and omit details unless critical.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f7b15",
   "metadata": {},
   "source": [
    "**Key Principles for Designing Memory Summarization Prompts**\n",
    "\n",
    "* **Milestones:** Highlight important events in the conversation—for example, when an issue is resolved, valuable information is uncovered, or all necessary details have been collected.\n",
    "\n",
    "* **Contradiction Check:** Ensure the summary does not conflict with itself, system instructions or tool definitions. This is especially critical for reasoning models, which are more prone to conflicts in the context.\n",
    "\n",
    "* **Timestamps & Temporal Flow:** Incorporate timing of events in the summary. This helps the model reason about updates in sequence and reduces confusion when forgetting or remembering the latest memory over a timeline.\n",
    "\n",
    "* **Chunking:** Organize details into categories or sections rather than long paragraphs. Structured grouping improves an LLM’s ability to understand relationships between pieces of information.\n",
    "\n",
    "* **Tool Performance Insights:** Capture lessons learned from multi-turn, tool-enabled interactions—for example, noting which tools worked effectively for specific queries and why. These insights are valuable for guiding future steps.\n",
    "\n",
    "* **Guidance & Examples:** Steer the summary with clear guidance. Where possible, extract concrete examples from the conversation history to make future turns more grounded and context-rich.\n",
    "\n",
    "* **Hallucination Control:** Be precise in what you include. Even minor hallucinations in a summary can propagate forward, contaminating future context with inaccuracies.\n",
    "\n",
    "* **Use Case Specificity:** Tailor the compression prompt to the specific use case. Think about how a human would track and recall information in working memory while solving the same task.\n",
    "\n",
    "* **Model Choice:** Select a summarizer model based on use case requirements, summary length, and tradeoffs between latency and cost. In some cases, using the same model as the agent itself can be advantageous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8f3f811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMSummarizer:\n",
    "    def __init__(self, client, model=\"gpt-4o\", max_tokens=400, tool_trim_limit=600):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tool_trim_limit = tool_trim_limit\n",
    "\n",
    "    async def summarize(self, messages: List[Item]) -> Tuple[str, str]:\n",
    "        user_shadow = \"Summarize the conversation we had so far.\"\n",
    "        # Map history into a compact prompt\n",
    "        history_snippets = []\n",
    "        for m in messages:\n",
    "            role = m.get(\"role\", \"assistant\")\n",
    "            content = (m.get(\"content\") or \"\").strip()\n",
    "            if not content:\n",
    "                continue\n",
    "            # trim very long tool blobs\n",
    "            if role in (\"tool\", \"tool_result\") and len(content) > self.tool_trim_limit:\n",
    "                content = content[:self.tool_trim_limit] + \" …\"\n",
    "            history_snippets.append(f\"{role.upper()}: {content}\")\n",
    "            #print(history_snippets)\n",
    "        # Example using Responses; adapt if you use SDK Agents runs instead\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": SUMMARY_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\".join(history_snippets)}\n",
    "        ]\n",
    "        print(len(prompt_messages))\n",
    "        resp = await asyncio.to_thread(\n",
    "                    self.client.responses.create,\n",
    "                    model=self.model,\n",
    "                    input=prompt_messages,\n",
    "                    max_output_tokens=self.max_tokens\n",
    "                )      \n",
    "        \n",
    "        summary = resp.output_text\n",
    "\n",
    "        await asyncio.sleep(0)  # yield control\n",
    "        return user_shadow, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0d8bd4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import itertools\n",
    "from collections import deque\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "\n",
    "class SummarizingSession:\n",
    "    \"\"\"\n",
    "    Keeps the last N *user turns* verbatim (keep_last_n_turns).\n",
    "    A turn = one real user message + everything that follows it (assistant replies,\n",
    "    reasoning, tool calls, tool results) until the next real user message.\n",
    "    Summarizes everything before that into a synthetic user→assistant pair.\n",
    "\n",
    "    Summarization is triggered once the number of *real* user turns\n",
    "    (non-synthetic 'user' messages) exceeds `context_limit`.\n",
    "\n",
    "    Internally stores (message, metadata) records. Exposes:\n",
    "      - get_items(): model-safe messages only (no metadata)\n",
    "      - get_full_history(): [{ \"message\": msg, \"metadata\": meta }, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # Only these keys are sent to the model. Everything else goes to metadata.\n",
    "    _ALLOWED_MSG_KEYS = {\"role\", \"content\", \"name\"}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keep_last_n_turns: int = 3,\n",
    "        context_limit: int = 3,\n",
    "        summarizer: Optional[\"Summarizer\"] = None,\n",
    "        session_id: Optional[str] = None,\n",
    "    ):\n",
    "        assert context_limit >= 1\n",
    "        assert keep_last_n_turns >= 0\n",
    "        assert keep_last_n_turns <= context_limit, \"keep_last_n_turns should not be greater than context_limit\"\n",
    "        self.keep_last_n_turns = keep_last_n_turns\n",
    "        self.context_limit = context_limit\n",
    "        # Each record: {\"msg\": {...}, \"meta\": {...}}\n",
    "        self._records: deque[Dict[str, Dict[str, Any]]] = deque()\n",
    "        self._lock = asyncio.Lock()\n",
    "        self.session_id = session_id or \"default\"\n",
    "        self.summarizer = summarizer\n",
    "\n",
    "    # --------- public API used by your runner ---------\n",
    "\n",
    "    async def get_items(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Returns messages in a model-safe shape (no metadata).\n",
    "        Runner.run(..., session=self) should call this.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            data = list(self._records)\n",
    "        msgs = [self._sanitize_for_model(rec[\"msg\"]) for rec in data]\n",
    "        return msgs[-limit:] if limit else msgs\n",
    "\n",
    "    async def add_items(self, items: List[Dict[str, Any]]) -> None:\n",
    "        async with self._lock:\n",
    "            for it in items:\n",
    "                msg, meta = self._split_msg_and_meta(it)\n",
    "                self._records.append({\"msg\": msg, \"meta\": meta})\n",
    "            need_summary, boundary_idx = self._should_summarize_locked()\n",
    "\n",
    "        if need_summary:\n",
    "            async with self._lock:\n",
    "                prefix_records = list(itertools.islice(self._records, 0, boundary_idx))\n",
    "                prefix_msgs = [r[\"msg\"] for r in prefix_records]\n",
    "\n",
    "            user_shadow, assistant_summary = await self._summarize(prefix_msgs)\n",
    "\n",
    "            async with self._lock:\n",
    "                need_summary_now, boundary_idx_now = self._should_summarize_locked()\n",
    "                if not need_summary_now:\n",
    "                    # normalize anyway if summarization got skipped\n",
    "                    self._normalize_synthetic_flags_locked()\n",
    "                    return\n",
    "\n",
    "                suffix_records = list(itertools.islice(self._records, boundary_idx_now, None))\n",
    "                self._records.clear()\n",
    "\n",
    "                # Synthetic summary pair keeps synthetic=True\n",
    "                self._records.extend([\n",
    "                    {\n",
    "                        \"msg\": {\"role\": \"user\", \"content\": user_shadow},\n",
    "                        \"meta\": {\n",
    "                            \"synthetic\": True,\n",
    "                            \"kind\": \"history_summary_prompt\",\n",
    "                            \"summary_for_turns\": f\"< all before idx {boundary_idx_now} >\",\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"msg\": {\"role\": \"assistant\", \"content\": assistant_summary},\n",
    "                        \"meta\": {\n",
    "                            \"synthetic\": True,\n",
    "                            \"kind\": \"history_summary\",\n",
    "                            \"summary_for_turns\": f\"< all before idx {boundary_idx_now} >\",\n",
    "                        },\n",
    "                    },\n",
    "                ])\n",
    "                self._records.extend(suffix_records)\n",
    "\n",
    "                # ✅ Ensure all real messages explicitly have synthetic=False\n",
    "                self._normalize_synthetic_flags_locked()\n",
    "        else:\n",
    "            # ✅ Even when we don't summarize, enforce the invariant\n",
    "            async with self._lock:\n",
    "                self._normalize_synthetic_flags_locked()\n",
    "\n",
    "    async def pop_item(self) -> Optional[Dict[str, Any]]:\n",
    "        async with self._lock:\n",
    "            if not self._records:\n",
    "                return None\n",
    "            rec = self._records.pop()\n",
    "            return dict(rec[\"msg\"])  # model-safe\n",
    "\n",
    "    async def clear_session(self) -> None:\n",
    "        async with self._lock:\n",
    "            self._records.clear()\n",
    "\n",
    "    def set_max_turns(self, n: int) -> None:\n",
    "        \"\"\"\n",
    "        Back-compat: interpret as updating context_limit.\n",
    "        Ensures keep_last_n_turns <= context_limit.\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.context_limit = n\n",
    "        if self.keep_last_n_turns > self.context_limit:\n",
    "            self.keep_last_n_turns = self.context_limit\n",
    "\n",
    "    # --------- full-history (for debugging/analytics/observability) ---------\n",
    "\n",
    "    # ✅ Backfill safeguard for older records that might lack the flag\n",
    "    def _normalize_synthetic_flags_locked(self) -> None:\n",
    "        for rec in self._records:\n",
    "            role = rec[\"msg\"].get(\"role\")\n",
    "            if role in (\"user\", \"assistant\") and \"synthetic\" not in rec[\"meta\"]:\n",
    "                rec[\"meta\"][\"synthetic\"] = False\n",
    "\n",
    "    \n",
    "    async def get_full_history(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Returns combined history where each entry is:\n",
    "          { \"message\": {role, content[, name]}, \"metadata\": {...} }\n",
    "        This is NOT sent to the model; it's for your logs/UI/debugging.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            data = list(self._records)\n",
    "        out = [{\"message\": dict(rec[\"msg\"]), \"metadata\": dict(rec[\"meta\"])} for rec in data]\n",
    "        return out[-limit:] if limit else out\n",
    "\n",
    "    # Backwards-compatible alias if you were using this name before\n",
    "    async def get_items_with_metadata(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "        return await self.get_full_history(limit)\n",
    "\n",
    "    # --------- helpers ---------\n",
    "\n",
    "    def _split_msg_and_meta(self, it: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "        msg = {k: v for k, v in it.items() if k in self._ALLOWED_MSG_KEYS}\n",
    "        extra = {k: v for k, v in it.items() if k not in self._ALLOWED_MSG_KEYS}\n",
    "        meta = dict(extra.pop(\"metadata\", {}))\n",
    "        meta.update(extra)\n",
    "\n",
    "        if \"role\" not in msg or \"content\" not in msg:\n",
    "            msg.setdefault(\"role\", \"user\")\n",
    "            msg.setdefault(\"content\", str(it))\n",
    "\n",
    "        # ✅ Default synthetic flag for real (non-summarized) messages\n",
    "        role = msg.get(\"role\")\n",
    "        if role in (\"user\", \"assistant\") and \"synthetic\" not in meta:\n",
    "            meta[\"synthetic\"] = False\n",
    "        return msg, meta\n",
    "\n",
    "    def _sanitize_for_model(self, msg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Strictly keep only allowed keys for model input.\n",
    "        \"\"\"\n",
    "        return {k: v for k, v in msg.items() if k in self._ALLOWED_MSG_KEYS}\n",
    "\n",
    "    def _is_user(self, rec: Dict[str, Dict[str, Any]]) -> bool:\n",
    "        return rec[\"msg\"].get(\"role\") == \"user\"\n",
    "\n",
    "    def _should_summarize_locked(self) -> Tuple[bool, int]:\n",
    "        \"\"\"\n",
    "        Trigger summarization if the number of *real* user turns exceeds `context_limit`.\n",
    "\n",
    "        Keep the last `keep_last_n_turns` *turns* verbatim:\n",
    "        find the earliest index among the last `keep_last_n_turns` real user messages;\n",
    "        everything before that index becomes the summarization prefix.\n",
    "\n",
    "        Returns: (need_summary: bool, boundary_idx: int)\n",
    "        \"\"\"\n",
    "        # Collect indices of real user messages (turn starts)\n",
    "        user_idxs: List[int] = []\n",
    "        for i, rec in enumerate(self._records):\n",
    "            if self._is_user(rec) and not rec[\"meta\"].get(\"synthetic\", False):\n",
    "                user_idxs.append(i)\n",
    "\n",
    "        real_turns = len(user_idxs)\n",
    "        if real_turns <= self.context_limit:\n",
    "            return False, -1\n",
    "\n",
    "        # Determine boundary according to \"turns\"\n",
    "        if self.keep_last_n_turns == 0:\n",
    "            # summarize everything; keep no turns verbatim\n",
    "            boundary = len(self._records)\n",
    "        else:\n",
    "            if len(user_idxs) < self.keep_last_n_turns:\n",
    "                return False, -1  # defensive; should not happen due to the check above\n",
    "            # earliest index among the last N real user-turn starts\n",
    "            boundary = user_idxs[-self.keep_last_n_turns]\n",
    "\n",
    "        # If boundary is 0 and we intend to keep >=1 turn, there's nothing before to summarize.\n",
    "        if boundary <= 0 and self.keep_last_n_turns > 0:\n",
    "            return False, -1\n",
    "\n",
    "        return True, boundary\n",
    "\n",
    "    async def _summarize(self, prefix_msgs: List[Dict[str, Any]]) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Adapter to your summarizer. Provide *model-safe* messages only.\n",
    "        \"\"\"\n",
    "        if not self.summarizer:\n",
    "            # Fallback summary if no summarizer is configured\n",
    "            return (\"Summarize the conversation we had so far.\", \"Summary unavailable.\")\n",
    "        # Only send role/content/name to the summarizer as well\n",
    "        clean_prefix = [self._sanitize_for_model(m) for m in prefix_msgs]\n",
    "        return await self.summarizer.summarize(clean_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214228c8",
   "metadata": {},
   "source": [
    "![Contxt Trimming in Session](../../images/SummarizingSession.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b12761",
   "metadata": {},
   "source": [
    "**High‑level idea**\n",
    "\n",
    "* **A turn** = one **real user** message **plus everything that follows it** (assistant replies, tool calls/results, etc.) **until the next real user message**.\n",
    "* You configure two knobs:\n",
    "\n",
    "  * **`context_limit`**: the maximum number of **real user turns** allowed in the raw history before we summarize.\n",
    "  * **`keep_last_n_turns`**: how many of the most recent **turns** to keep verbatim when we do summarize.\n",
    "\n",
    "    * Invariant: `keep_last_n_turns <= context_limit`.\n",
    "* When the number of **real** user turns exceeds `context_limit`, the session:\n",
    "\n",
    "  1. **Summarizes** everything **before** the earliest of the last `keep_last_n_turns` turn starts,\n",
    "  2. Injects a **synthetic user→assistant pair** at the top of the kept region:\n",
    "\n",
    "     * `user`: `\"Summarize the conversation we had so far.\"` (shadow prompt)\n",
    "     * `assistant`: `{generated summary}`\n",
    "  3. **Keeps** the last `keep_last_n_turns` turns **verbatim**.\n",
    "\n",
    "This guarantees the last `keep_last_n_turns` turns are preserved exactly as they occurred, while all earlier content is compressed into the two synthetic messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6d867c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SummarizingSession(\n",
    "    keep_last_n_turns=2,\n",
    "    context_limit=4,\n",
    "    summarizer=LLMSummarizer(client)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d22531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example flow\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Hi, my router won't connect. by the way, I am using Windows 10. I tried troubleshooting via your FAQs but I didn't get anywhere. This is my third tiem calling you. I am based in the US and one of Premium customers.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Let's check your firmware version.\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Firmware v1.0.3; still failing.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Try a factory reset.\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Reset done; error 42 now.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Try to install a new firmware.\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"I tried but I got another error now.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Can you please provide me with the error code?\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"It says 404 not found when I try to access the page.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Are you connected to the internet?\"}])\n",
    "# At this point, with context_limit=4, everything *before* the earliest of the last 4 turns\n",
    "# is summarized into a synthetic pair, and the last 2 turns remain verbatim.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = await session.get_items()\n",
    "# Pass `history` into your agent runner / responses call as the conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9f229de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Summarize the conversation we had so far.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"• Product & Environment:\\n  - Router with firmware v1.0.3, Windows 10, US-based, Premium customer.\\n\\n• Reported Issue:\\n  - Router won't connect to the internet.\\n\\n• Steps Tried & Results:\\n  - Followed FAQs for troubleshooting; no resolution.\\n  - Checked firmware version: v1.0.3; issue persists.\\n  - Performed factory reset; encountered error 42.\\n\\n• Identifiers:\\n  - None provided.\\n\\n• Timeline Milestones:\\n  - Initial troubleshooting via FAQs → Firmware check → Factory reset → Error 42.\\n\\n• Tool Performance Insights:\\n  - Factory reset unsuccessful in resolving connection issue; led to error 42.\\n\\n• Current Status & Blockers:\\n  - Connection issue unresolved; error 42 after reset is blocking progress.\\n\\n• Next Recommended Step:\\n  - Install new firmware version compatible with device.\"},\n",
       " {'role': 'user', 'content': 'I tried but I got another error now.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Can you please provide me with the error code?'},\n",
       " {'role': 'user',\n",
       "  'content': 'It says 404 not found when I try to access the page.'},\n",
       " {'role': 'assistant', 'content': 'Are you connected to the internet?'}]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "afc57803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Product & Environment:\n",
      "  - Windows 10, router (specific model UNVERIFIED).\n",
      "\n",
      "• Reported Issue:\n",
      "  - Router won't connect.\n",
      "\n",
      "• Steps Tried & Results:\n",
      "  - Used FAQs for troubleshooting; no resolution.\n",
      "\n",
      "• Identifiers:\n",
      "  - Premium customer; based in the US (no specific identifiers provided).\n",
      "\n",
      "• Timeline Milestones:\n",
      "  - Third interaction reported by user.\n",
      "\n",
      "• Tool Performance Insights:\n",
      "  - User FAQs insufficient for resolution.\n",
      "\n",
      "• Current Status & Blockers:\n",
      "  - Connection issue unresolved; firmware version not yet checked.\n",
      "\n",
      "• Next Recommended Step:\n",
      "  - Verify and update the router firmware version.\n"
     ]
    }
   ],
   "source": [
    "print(history[1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6004db",
   "metadata": {},
   "source": [
    "You can use the get_items_with_metadata method to get the full history of the session including the metadata for debugging and analysis purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "5448ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_history = await session.get_items_with_metadata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ddec0fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'message': {'role': 'user',\n",
       "   'content': 'Summarize the conversation we had so far.'},\n",
       "  'metadata': {'synthetic': True,\n",
       "   'kind': 'history_summary_prompt',\n",
       "   'summary_for_turns': '< all before idx 6 >'}},\n",
       " {'message': {'role': 'assistant',\n",
       "   'content': '**Product & Environment:**\\n- Device: Router\\n- OS: Windows 10\\n- Firmware: v1.0.3\\n\\n**Reported Issue:**\\n- Router fails to connect to the internet, now showing error 42.\\n\\n**Steps Tried & Results:**\\n- Checked FAQs: No resolution.\\n- Firmware version checked: v1.0.3.\\n- Factory reset performed: Resulted in error 42.\\n\\n**Identifiers:**\\n- UNVERIFIED\\n\\n**Timeline Milestones:**\\n- User attempted FAQ troubleshooting.\\n- Firmware checked after initial advice.\\n- Factory reset led to error 42.\\n\\n**Tool Performance Insights:**\\n- FAQs and basic reset process did not resolve the issue.\\n\\n**Current Status & Blockers:**\\n- Error 42 unresolved; firmware update needed.\\n\\n**Next Recommended Step:**\\n- Install the latest firmware update and check for resolution.'},\n",
       "  'metadata': {'synthetic': True,\n",
       "   'kind': 'history_summary',\n",
       "   'summary_for_turns': '< all before idx 6 >'}},\n",
       " {'message': {'role': 'user',\n",
       "   'content': 'I tried but I got another error now.'},\n",
       "  'metadata': {'synthetic': False}},\n",
       " {'message': {'content': 'I still have a problem with my router.',\n",
       "   'role': 'user'},\n",
       "  'metadata': {'synthetic': False}},\n",
       " {'message': {'content': [], 'role': 'user'},\n",
       "  'metadata': {'id': 'rs_68ba192de700819dbed28ad768a9c48205277fe33200f1e3',\n",
       "   'summary': [],\n",
       "   'type': 'reasoning',\n",
       "   'synthetic': False}},\n",
       " {'message': {'content': [{'annotations': [],\n",
       "     'text': 'Sorry you’re still stuck. What is the exact error code/message you see now during the firmware update, and does it appear in the router’s web UI or elsewhere?\\n\\nWhile you check that, try these quick, safe steps:\\n1) Verify the firmware file exactly matches your router’s model and hardware revision (check the label on the router) and region.\\n2) Re‑download the firmware from the vendor site and verify its checksum (MD5/SHA256) if provided.\\n3) Use a wired Ethernet connection to a LAN port, disable Wi‑Fi on the PC, and try a different browser with extensions disabled.\\n4) Ensure you’re uploading the correct file type (e.g., .bin/.img), not a ZIP; don’t rename the file.\\n5) Reboot the router and your PC, then retry the upload; after starting the update, wait at least 10 minutes and don’t power off.\\n\\nNote: “Error 42” meanings vary by brand; once you share the exact current error text and where it appears, I’ll give brand‑specific steps (including recovery options if needed).',\n",
       "     'type': 'output_text',\n",
       "     'logprobs': []}],\n",
       "   'role': 'assistant'},\n",
       "  'metadata': {'id': 'msg_68ba19400060819db38bcb891e9aec7605277fe33200f1e3',\n",
       "   'status': 'completed',\n",
       "   'type': 'message',\n",
       "   'synthetic': False}}]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "64d555ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Product & Environment:**\n",
      "- Device: Router\n",
      "- OS: Windows 10\n",
      "- Firmware: v1.0.3\n",
      "\n",
      "**Reported Issue:**\n",
      "- Router fails to connect to the internet, now showing error 42.\n",
      "\n",
      "**Steps Tried & Results:**\n",
      "- Checked FAQs: No resolution.\n",
      "- Firmware version checked: v1.0.3.\n",
      "- Factory reset performed: Resulted in error 42.\n",
      "\n",
      "**Identifiers:**\n",
      "- UNVERIFIED\n",
      "\n",
      "**Timeline Milestones:**\n",
      "- User attempted FAQ troubleshooting.\n",
      "- Firmware checked after initial advice.\n",
      "- Factory reset led to error 42.\n",
      "\n",
      "**Tool Performance Insights:**\n",
      "- FAQs and basic reset process did not resolve the issue.\n",
      "\n",
      "**Current Status & Blockers:**\n",
      "- Error 42 unresolved; firmware update needed.\n",
      "\n",
      "**Next Recommended Step:**\n",
      "- Install the latest firmware update and check for resolution.\n"
     ]
    }
   ],
   "source": [
    "print(history[1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744c6d8",
   "metadata": {},
   "source": [
    "### Notes & design choices\n",
    "\n",
    "* **Turn boundary preserved at the “fresh” side**: the **`keep_last_n_turns` user turns** remain verbatim; everything older is compressed.\n",
    "* **Two-message summary block**: easy for downstream tooling to detect or display (`metadata.synthetic == True`).\n",
    "* **Async + lock discipline**: we **release the lock** while the (potentially slow) summarization runs; then re-check the condition before applying the summary to avoid racey merges.\n",
    "* **Idempotent behavior**: if more messages arrive during summarization, the post-await recheck prevents stale rewrites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730afac9",
   "metadata": {},
   "source": [
    "## Evals\n",
    "\n",
    "At the end of the day, **evals is all you need** for context engineering. The key question to ask is: *how do we know the model isn’t “losing context” or \"confusing context\"?*\n",
    "\n",
    "While a full cookbook around memory could stand on its own in the future, here are some lightweight evaluation harness ideas to start with:\n",
    "\n",
    "* **Baseline & Deltas:** Continue running your core eval sets and compare before/after experiments to measure memory improvements.\n",
    "* **LLM-as-Judge:** Use a model with a carefully designed grader prompt to evaluate summarization quality. Focus on whether it captures the most important details in the correct format.\n",
    "* **Transcript Replay:** Re-run long conversations and measure next-turn accuracy with and without context trimming. Metrics could include exact match on entities/IDs and rubric-based scoring on reasoning quality.\n",
    "* **Error Regression Tracking:** Watch for common failure modes—unanswered questions, dropped constraints, or unnecessary/repeated tool calls.\n",
    "* **Token Pressure Checks:** Flag cases where token limits force dropping protected context. Log before/after token counts to detect when critical details are being pruned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2843701",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
