{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcfc573",
   "metadata": {},
   "source": [
    "# Context Engineering: Short-Term Memory Management with Sessions from OpenAI Agents SDK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab798a",
   "metadata": {},
   "source": [
    "In this cookbook, we’ll explore how to **manage context effectively using the `Session` object from the [OpenAI Agents SDK](https://github.com/openai/openai-agents-python)**.\n",
    "\n",
    "AI agents often operate in **long-running, multi-turn interactions**, where keeping the right balance of context is critical. If too much is carried forward, the model risks distraction, inefficiency, or outright failure. If too little is preserved, the agent loses coherence. This guide focuses on two proven context management techniques—**trimming** and **compression**—used by engineering teams to keep agents fast, reliable, and cost-efficient.\n",
    "\n",
    "#### Why Context Management Matters\n",
    "\n",
    "* **Sustained coherence across long threads** – Keep the agent anchored to the latest user goal without dragging along stale details. Session-level trimming and summaries prevent “yesterday’s plan” from overriding today’s ask.\n",
    "* **Higher tool-call accuracy** – Focused context improves function selection and argument filling, reducing retries, timeouts, and cascading failures during multi-tool runs.\n",
    "* **Lower latency & cost** – Smaller, sharper prompts cut tokens per turn and attention load.\n",
    "* **Error & hallucination containment** – Summaries act as “clean rooms” that correct or omit prior mistakes; trimming avoids amplifying bad facts (“context poisoning”) turn after turn.\n",
    "* **Easier debugging & observability** – Stable summaries and bounded histories make logs comparable: you can diff summaries, attribute regressions, and reproduce failures reliably.\n",
    "* **Multi-issue and handoff resilience** – In multi-problem chats, per-issue mini-summaries let the agent pause/resume, escalate to humans, or hand off to another agent while staying consistent.\n",
    "\n",
    "#### Real-World Scenario\n",
    "\n",
    "We’ll ground the techniques in a practical example for one of the common long-running tasks, such as:\n",
    "\n",
    "* **Multi-turn Customer Service Conversations**\n",
    "In extended conversations about tech products—spanning both hardware and software—customers often surface multiple issues over time. The agent must stay consistent and goal-focused while retaining only the essentials rather than hauling along every past detail.\n",
    "\n",
    "#### Techniques Covered\n",
    "\n",
    "To address these challenges, we introduce two concrete approaches using OpenAI Agents SDK:\n",
    "\n",
    "1. **Trimming Messages** – dropping older turns while keeping the last N turns.\n",
    "2. **Summarizing Messages** – compressing prior exchanges into structured, shorter representations.\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc613968",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this cookbook, you must set up the following accounts and complete a few setup actions. These prerequisites are essential to interact with the APIs used in this project.\n",
    "\n",
    "#### Step0: OpenAI Account\n",
    "\n",
    "- **Purpose:**  \n",
    "  You need an OpenAI account to access language models and use the Agents SDK featured in this cookbook.\n",
    "\n",
    "- **Action:**  \n",
    "  [Sign up for an OpenAI account](https://openai.com) if you don’t already have one. Once you have an account, create an API key by visiting the [OpenAI API Keys page](https://platform.openai.com/api-keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9a109",
   "metadata": {},
   "source": [
    "#### Step1: Install the Required Libraries\n",
    "\n",
    "Below we install the `openai-agents` library (the [OpenAI Agents SDK](https://github.com/openai/openai-agents-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87818100",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai-agents nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6348e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b39f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import set_tracing_disabled\n",
    "set_tracing_disabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e8e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating AI agents is crucial to ensure accuracy, fairness, safety, security, and alignment with human values and objectives.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents import Agent, Runner\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Assistant\",\n",
    "        instructions=\"Reply very concisely.\",\n",
    "    )\n",
    "\n",
    "    result = await Runner.run(agent, \"Tell me why it is important to evaluate AI agents.\")\n",
    "    print(result.final_output)\n",
    "\n",
    "loop = asyncio.get_running_loop()\n",
    "await loop.create_task(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39dab9d",
   "metadata": {},
   "source": [
    "### Define Agents\n",
    "\n",
    "We can start by defining the necessary components from Agents SDK Library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2725d9d",
   "metadata": {},
   "source": [
    "#### Customer Service Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4451ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_agent = Agent(\n",
    "    name=\"Customer Support Assistant\",\n",
    "    model=\"gpt-5\",\n",
    "    instructions=(\n",
    "        \"You are a patient, step-by-step IT support assistant. \"\n",
    "        \"Your role is to help customers troubleshoot and resolve issues with devices and software. \"\n",
    "        \"Guidelines:\\n\"\n",
    "        \"- Be concise and use numbered steps where possible.\\n\"\n",
    "        \"- Ask only one focused, clarifying question at a time before suggesting next actions.\\n\"\n",
    "        \"- Track and remember multiple issues across the conversation; update your understanding as new problems emerge.\\n\"\n",
    "        \"- When a problem is resolved, briefly confirm closure before moving to the next.\\n\"\n",
    "        \"- If the session grows long, proactively summarize solved and open issues to maintain clarity.\\n\"\n",
    "        \"- Prefer verified knowledge base information; avoid speculation.\\n\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8074e05",
   "metadata": {},
   "source": [
    "## 1. Context Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8cf80",
   "metadata": {},
   "source": [
    "#### Implement Custom Session Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993098b",
   "metadata": {},
   "source": [
    "We are using [Session](https://openai.github.io/openai-agents-python/sessions/) object from [OpenAI Agents Python SDK](https://openai.github.io/openai-agents-python/). Here’s a `MyCustomSession` implementation that **keeps only the last N turns** (a “turn” = one user message and everything until the next user message—including the assistant reply and any tool calls/results). It’s in-memory and trims automatically on every write and read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1b468c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "from collections import deque\n",
    "from typing import Any, Deque, Dict, Iterable, List, Optional, Tuple, Union, cast\n",
    "\n",
    "from agents.memory.session import SessionABC\n",
    "from agents.items import TResponseInputItem  # typically a dict-like item\n",
    "\n",
    "\n",
    "def _is_user_msg(item: TResponseInputItem) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: treat items with role=='user' as user messages.\n",
    "    Falls back to type=='message' and author checks if your SDK uses a different shape.\n",
    "    Adjust this if your item schema differs.\n",
    "    \"\"\"\n",
    "    if isinstance(item, dict):\n",
    "        role = cast(Dict[str, Any], item).get(\"role\")\n",
    "        if role == \"user\":\n",
    "            return True\n",
    "        # Some SDKs encode messages as {\"type\": \"message\", \"role\": \"...\"}\n",
    "        if item.get(\"type\") == \"message\" and item.get(\"role\") == \"user\":\n",
    "            return True\n",
    "    # Extend here if you carry custom classes with .role attribute:\n",
    "    role_attr = getattr(item, \"role\", None)\n",
    "    return role_attr == \"user\"\n",
    "\n",
    "\n",
    "class MyCustomSession(SessionABC):\n",
    "    \"\"\"\n",
    "    Custom session that keeps only the last N user-turns.\n",
    "    A 'turn' is defined as a user message and all subsequent items\n",
    "    (assistant/tool calls/results) up to—but not including—the next user message.\n",
    "\n",
    "    Works entirely in memory. If you need persistence, replace the in-memory\n",
    "    deque with your storage of choice (SQLite/Redis/etc.), preserving the\n",
    "    trimming logic in `_trim_to_last_turns`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, session_id: str, max_turns: int = 8):\n",
    "        self.session_id = session_id\n",
    "        self.max_turns = max(1, max_turns)\n",
    "        self._items: Deque[TResponseInputItem] = deque()  # full chronological log\n",
    "        self._lock = asyncio.Lock()\n",
    "\n",
    "    # ---- SessionABC API ----\n",
    "\n",
    "    async def get_items(self, limit: int | None = None) -> List[TResponseInputItem]:\n",
    "        \"\"\"\n",
    "        Return the history trimmed to the last N turns.\n",
    "        If `limit` is provided, return at most that many most-recent items\n",
    "        from within the trimmed history.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            trimmed = self._trim_to_last_turns(list(self._items))\n",
    "            if limit is not None and limit >= 0:\n",
    "                return trimmed[-limit:]\n",
    "            return trimmed\n",
    "\n",
    "    async def add_items(self, items: List[TResponseInputItem]) -> None:\n",
    "        \"\"\"\n",
    "        Append new items, then trim to last N turns.\n",
    "        \"\"\"\n",
    "        if not items:\n",
    "            return\n",
    "        async with self._lock:\n",
    "            self._items.extend(items)\n",
    "            # Trim in place by rebuilding from trimmed list\n",
    "            trimmed = self._trim_to_last_turns(list(self._items))\n",
    "            self._items.clear()\n",
    "            self._items.extend(trimmed)\n",
    "\n",
    "    async def pop_item(self) -> TResponseInputItem | None:\n",
    "        \"\"\"\n",
    "        Remove and return the most recent item (post-trim).\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            if not self._items:\n",
    "                return None\n",
    "            return self._items.pop()\n",
    "\n",
    "    async def clear_session(self) -> None:\n",
    "        \"\"\"\n",
    "        Remove all items for this session.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            self._items.clear()\n",
    "\n",
    "    # ---- Helpers ----\n",
    "\n",
    "    def _trim_to_last_turns(self, items: List[TResponseInputItem]) -> List[TResponseInputItem]:\n",
    "        \"\"\"\n",
    "        Keep only the suffix of `items` that contains the last `max_turns` user messages\n",
    "        and everything after the earliest of those user messages.\n",
    "\n",
    "        Algorithm:\n",
    "          1) Scan from the end to find indices of the last `max_turns` user messages.\n",
    "          2) Cut history to start from the earliest of those (inclusive).\n",
    "        Edge cases:\n",
    "          - If there are fewer than `max_turns` user messages, keep entire history.\n",
    "          - If there are no user messages yet, treat all existing items as a single turn and keep them.\n",
    "        \"\"\"\n",
    "        if not items:\n",
    "            return items\n",
    "\n",
    "        # Find indices of user messages scanning from the end\n",
    "        user_indices: List[int] = []\n",
    "        for idx in range(len(items) - 1, -1, -1):\n",
    "            if _is_user_msg(items[idx]):\n",
    "                user_indices.append(idx)\n",
    "                if len(user_indices) >= self.max_turns:\n",
    "                    break\n",
    "\n",
    "        if not user_indices:\n",
    "            # No user messages yet; keep everything\n",
    "            return items\n",
    "\n",
    "        # The earliest index among the last N user messages\n",
    "        cut_from = min(user_indices)  # since we collected from the end\n",
    "        return items[cut_from:]\n",
    "\n",
    "    # ---- Optional convenience API (not part of SessionABC) ----\n",
    "\n",
    "    async def set_max_turns(self, max_turns: int) -> None:\n",
    "        async with self._lock:\n",
    "            self.max_turns = max(1, int(max_turns))\n",
    "            trimmed = self._trim_to_last_turns(list(self._items))\n",
    "            self._items.clear()\n",
    "            self._items.extend(trimmed)\n",
    "\n",
    "    async def raw_items(self) -> List[TResponseInputItem]:\n",
    "        \"\"\"Return the untrimmed in-memory log (for debugging).\"\"\"\n",
    "        async with self._lock:\n",
    "            return list(self._items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd5b28",
   "metadata": {},
   "source": [
    "Let's define the custom session object we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "951ad6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the last 8 turns (user + assistant/tool interactions)\n",
    "session = MyCustomSession(\"my_session\", max_turns=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab99cd",
   "metadata": {},
   "source": [
    "**How to choose the right `max_turns`?**\n",
    "\n",
    "Determining this parameter usually requires experimentation with your conversation history. One approach is to extract the total number of turns across conversations and analyze their distribution. Another option is to use an LLM to evaluate conversations—identifying how many tasks or issues each one contains and calculating the average number of turns needed per issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c59d40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"There is a red light on the dashboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "03b15552",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(\n",
    "    support_agent,\n",
    "    message,\n",
    "    session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c94beb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A red light usually indicates an issue with your internet connection. Is the red light on your modem or router?'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.raw_responses[0].output[0].content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50382117",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await session.raw_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "395c1bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f60675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example flow\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Hi, my router won't connect.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Let's check your firmware version.\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Firmware v1.0.3; still failing.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Try a factory reset.\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Reset done; error 42 now.\"}])\n",
    "# At this point, with max_turns=3, everything *before* the earliest of the last 3 user\n",
    "# messages is summarized into a synthetic pair, and the last 3 turns remain verbatim.\n",
    "\n",
    "history = await session.get_items()\n",
    "# Pass `history` into your agent runner / responses call as the conversation context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8dd9c5",
   "metadata": {},
   "source": [
    "**What counts as a “turn”**\n",
    "\n",
    "* A **turn** = one **user** message **plus everything that follows it** (assistant replies, tool calls, tool results) **until the next user message**.\n",
    "\n",
    "** When trimming happens\n",
    "\n",
    "* On **write**: `add_items(...)` appends the new items, then immediately trims the stored history.\n",
    "* On **read**: `get_items(...)` returns a **trimmed** view (so even if you bypassed a write, reads won’t leak old turns).\n",
    "\n",
    "**How it decides what to keep**\n",
    "\n",
    "1. Treat any item with `role == \"user\"` as a **user message** (via `_is_user_msg`).\n",
    "2. Scan the history **backwards** and collect the indices of the last **N** user messages (`max_turns`).\n",
    "3. Find the **earliest** index among those N user messages.\n",
    "4. **Keep everything from that index to the end**; drop everything before it.\n",
    "\n",
    "That preserves each complete turn boundary: if the earliest kept user message is at index `k`, you also keep all assistant/tool items that came after `k`.\n",
    "\n",
    "**Edge cases**\n",
    "\n",
    "* **Fewer than N user messages**: keep **everything** (no trimming yet).\n",
    "* **No user messages**: keep **everything** (treat as a single in-progress turn).\n",
    "* **`limit` in `get_items(limit=…)`**: applied **after** trimming; returns only the last `limit` items of the already-trimmed slice.\n",
    "\n",
    "**Tiny example**\n",
    "\n",
    "History (old → new):\n",
    "\n",
    "```\n",
    "0: user(\"Hi\")\n",
    "1: assistant(\"Hello!\")\n",
    "2: tool_call(\"lookup\")\n",
    "3: tool_result(\"…\")\n",
    "4: user(\"It didn't work\")\n",
    "5: assistant(\"Try rebooting\")\n",
    "6: user(\"Rebooted, now error 42\")\n",
    "7: assistant(\"On it\")\n",
    "```\n",
    "\n",
    "With `max_turns = 2`, the last two user messages are at indices **4** and **6**.\n",
    "Earliest of those is **4** → keep items **4..7**, drop **0..3**.\n",
    "\n",
    "**Why this works well**\n",
    "\n",
    "* You always keep **complete** turns, so the assistant retains the immediate context it needs (both the user’s last asks and the assistant/tool steps in between).\n",
    "* It prevents context bloat by discarding older turns wholesale, not just messages.\n",
    "\n",
    "**Customization knobs**\n",
    "\n",
    "* Change `max_turns` at init or via `set_max_turns(...)`.\n",
    "* Adjust `_is_user_msg(...)` if your item schema differs.\n",
    "* If you’d rather cap by **message count** or **tokens**, replace `_trim_to_last_turns(...)` or add a second pass that measures tokens.\n",
    "\n",
    "**Complexity**\n",
    "\n",
    "* Each trim scans the list once ⇒ **O(n)** per trim (n = items in memory). In practice this is cheap because n is bounded by your recent turns.\n",
    "\n",
    "That’s the whole flow: **append → find last N user boundaries → keep from the earliest boundary onward**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa349f",
   "metadata": {},
   "source": [
    "## 2. Context Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380492d",
   "metadata": {},
   "source": [
    "A well-crafted summarization prompt is essential for preserving the context of a conversation, and it should always be tailored to the specific use case. Think of it like being a customer support agent handing off a case to the next agent—what concise yet critical details would they need to continue smoothly? The prompt should strike the right balance: not overloaded with unnecessary information, but not so sparse that key context is lost. Achieving this balance requires careful design and ongoing experimentation to fine-tune the level of detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = \"\"\"\n",
    "You are a senior customer-support assistant for tech devices, setup, and software issues.\n",
    "Compress the earlier conversation into a precise, reusable snapshot for future turns.\n",
    "\n",
    "Before you write (do this silently):\n",
    "- Contradiction check: compare user claims with system instructions and tool definitions/logs; note any conflicts or reversals.\n",
    "- Temporal ordering: sort key events by time; the most recent update wins. If timestamps exist, keep them.\n",
    "- Hallucination control: if any fact is uncertain/not stated, mark it as UNVERIFIED rather than guessing.\n",
    "\n",
    "Write a structured, factual summary ≤ 200 words using the sections below (use the exact headings):\n",
    "\n",
    "• Product & Environment:\n",
    "  - Device/model, OS/app versions, network/context if mentioned.\n",
    "\n",
    "• Reported Issue:\n",
    "  - Single-sentence problem statement (latest state).\n",
    "\n",
    "• Steps Tried & Results:\n",
    "  - Chronological bullets (include tool calls + outcomes, errors, codes).\n",
    "\n",
    "• Identifiers:\n",
    "  - Ticket #, device serial/model, account/email (only if provided).\n",
    "\n",
    "• Timeline Milestones:\n",
    "  - Key events with timestamps or relative order (e.g., 10:32 install → 10:41 error).\n",
    "\n",
    "• Tool Performance Insights:\n",
    "  - What tool calls worked/failed and why (if evident).\n",
    "\n",
    "• Current Status & Blockers:\n",
    "  - What’s resolved vs pending; explicit blockers preventing progress.\n",
    "\n",
    "• Next Recommended Step:\n",
    "  - One concrete action (or two alternatives) aligned with policies/tools.\n",
    "\n",
    "• Escalation:\n",
    "  - “Suggested” or “Not needed”; include reason and target team if applicable.\n",
    "\n",
    "Rules:\n",
    "- Be concise, no fluff; use short bullets, verbs first.\n",
    "- Do not invent new facts; quote error strings/codes exactly when available.\n",
    "- If previous info was superseded, note “Superseded:” and omit details unless critical.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f7b15",
   "metadata": {},
   "source": [
    "**Key Principles for Designing Summarization Prompts**\n",
    "\n",
    "* **Milestones:** Highlight important events in the conversation—for example, when an issue is resolved, valuable information is uncovered, or all necessary details have been collected.\n",
    "\n",
    "* **Contradiction Check:** Ensure the summary does not conflict with system instructions or tool definitions. This is especially critical for reasoning models, which are more prone to conflicts in the context.\n",
    "\n",
    "* **Timestamps & Temporal Flow:** Incorporate timing of events in the summary. This helps the model reason about updates in sequence and reduces confusion when forgetting or remembering the latest memory over a timeline.\n",
    "\n",
    "* **Chunking:** Organize details into categories or sections rather than long paragraphs. Structured grouping improves an LLM’s ability to understand relationships between pieces of information.\n",
    "\n",
    "* **Tool Performance Insights:** Capture lessons learned from multi-turn, tool-enabled interactions—for example, noting which tools worked effectively for specific queries and why. These insights are valuable for guiding future steps.\n",
    "\n",
    "* **Guidance & Examples:** Steer the summary with clear guidance. Where possible, extract concrete examples from the conversation history to make future turns more grounded and context-rich.\n",
    "\n",
    "* **Hallucination Control:** Be precise in what you include. Even minor hallucinations in a summary can propagate forward, contaminating future context with inaccuracies.\n",
    "\n",
    "* **Use Case Specificity:** Tailor the compression prompt to the specific use case. Think about how a human would track and recall information in working memory while solving the same task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8f3f811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMSummarizer:\n",
    "    def __init__(self, client, model=\"gpt-4o\", max_tokens=400, tool_trim_limit=600):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        self.tool_trim_limit = tool_trim_limit\n",
    "\n",
    "    async def summarize(self, messages: List[Item]) -> Tuple[str, str]:\n",
    "        user_shadow = \"Summarize the conversation we had so far.\"\n",
    "        # Map history into a compact prompt\n",
    "        history_snippets = []\n",
    "        for m in messages:\n",
    "            role = m.get(\"role\", \"assistant\")\n",
    "            content = (m.get(\"content\") or \"\").strip()\n",
    "            if not content:\n",
    "                continue\n",
    "            # trim very long tool blobs\n",
    "            if role in (\"tool\", \"tool_result\") and len(content) > self.tool_trim_limit:\n",
    "                content = content[:self.tool_trim_limit] + \" …\"\n",
    "            history_snippets.append(f\"{role.upper()}: {content}\")\n",
    "            #print(history_snippets)\n",
    "        # Example using Responses; adapt if you use SDK Agents runs instead\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": SUMMARY_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\".join(history_snippets)}\n",
    "        ]\n",
    "        print(len(prompt_messages))\n",
    "        resp = await asyncio.to_thread(\n",
    "                    self.client.responses.create,\n",
    "                    model=self.model,\n",
    "                    input=prompt_messages,\n",
    "                    max_output_tokens=self.max_tokens\n",
    "                )      \n",
    "        \n",
    "        summary = resp.output_text\n",
    "\n",
    "        await asyncio.sleep(0)  # yield control\n",
    "        return user_shadow, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a3e7cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import deque\n",
    "from typing import Optional\n",
    "import itertools\n",
    "\n",
    "class SummarizingSession:\n",
    "    \"\"\"\n",
    "    Keeps the last N user turns verbatim.\n",
    "    Summarizes everything before that into a synthetic user→assistant pair.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_turns: int = 3,\n",
    "        summarizer: Optional[Summarizer] = None,\n",
    "        session_id: Optional[str] = None,\n",
    "    ):\n",
    "        assert max_turns >= 1\n",
    "        self.max_turns = max_turns\n",
    "        self._items: deque[Item] = deque()\n",
    "        self._lock = asyncio.Lock()\n",
    "        self.session_id = session_id or \"default\"\n",
    "        self.summarizer = summarizer or HeuristicSummarizer()\n",
    "\n",
    "    # ----- public API that mirrors common Session interfaces -----\n",
    "\n",
    "    async def get_items(self, limit: Optional[int] = None) -> list[Item]:\n",
    "        async with self._lock:\n",
    "            data = list(self._items)\n",
    "        return data[-limit:] if limit else data\n",
    "\n",
    "    async def add_items(self, items: list[Item]) -> None:\n",
    "        # Append first\n",
    "        async with self._lock:\n",
    "            self._items.extend(items)\n",
    "            need_summary, boundary_idx = self._should_summarize_locked()\n",
    "\n",
    "        # If we need a summary, **do it without the lock** to avoid blocking others\n",
    "        if need_summary:\n",
    "            # Take a snapshot of the prefix to summarize\n",
    "            async with self._lock:\n",
    "                prefix = list(itertools.islice(self._items, 0, boundary_idx))\n",
    "            # Produce the summary outside the lock\n",
    "            user_shadow, assistant_summary = await self.summarizer.summarize(prefix)\n",
    "\n",
    "            # Re-acquire and re-check (in case of concurrent updates)\n",
    "            async with self._lock:\n",
    "                need_summary_now, boundary_idx_now = self._should_summarize_locked()\n",
    "                if need_summary_now:\n",
    "                    suffix = list(itertools.islice(self._items, boundary_idx_now, None))            \n",
    "                    self._items.clear()\n",
    "                    self._items.extend([\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": user_shadow,\n",
    "                            \"metadata\": {\n",
    "                                \"synthetic\": True,\n",
    "                                \"kind\": \"history_summary_prompt\",\n",
    "                                \"summary_for_turns\": f\"< all before idx {boundary_idx_now} >\",\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": assistant_summary,\n",
    "                            \"metadata\": {\n",
    "                                \"synthetic\": True,\n",
    "                                \"kind\": \"history_summary\",\n",
    "                                \"summary_for_turns\": f\"< all before idx {boundary_idx_now} >\",\n",
    "                            },\n",
    "                        },\n",
    "                    ])\n",
    "                    self._items.extend(suffix)\n",
    "                # else: another concurrent writer already summarized; do nothing.\n",
    "\n",
    "    async def pop_item(self) -> Optional[Item]:\n",
    "        async with self._lock:\n",
    "            return self._items.pop() if self._items else None\n",
    "\n",
    "    async def clear_session(self) -> None:\n",
    "        async with self._lock:\n",
    "            self._items.clear()\n",
    "\n",
    "    def set_max_turns(self, n: int) -> None:\n",
    "        assert n >= 1\n",
    "        self.max_turns = n\n",
    "\n",
    "    # ----- helpers -----\n",
    "\n",
    "    def _is_user(self, it: Item) -> bool:\n",
    "        return it.get(\"role\") == \"user\"\n",
    "\n",
    "    def _should_summarize_locked(self) -> tuple[bool, int]:\n",
    "        \"\"\"\n",
    "        Returns (need_summary, boundary_idx).\n",
    "        boundary_idx = earliest index to keep (start of last N user turns).\n",
    "        If False, boundary_idx is undefined.\n",
    "        \"\"\"\n",
    "        idxs = []\n",
    "        for i in range(len(self._items) - 1, -1, -1):\n",
    "            if self._is_user(self._items[i]):\n",
    "                idxs.append(i)\n",
    "                if len(idxs) == self.max_turns:\n",
    "                    break\n",
    "        if len(idxs) < self.max_turns:\n",
    "            return False, -1  # not enough user turns yet\n",
    "\n",
    "        boundary = min(idxs)  # earliest of the last N user turns\n",
    "        if boundary <= 0:\n",
    "            return False, -1  # nothing to summarize before boundary\n",
    "        return True, boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a8d22531",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SummarizingSession(\n",
    "    max_turns=4,\n",
    "    summarizer=LLMSummarizer(client),  # or LLMSummarizer(client)\n",
    ")\n",
    "\n",
    "# Example flow\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Hi, my router won't connect. by the way, I am using Windows 10. I tried troubleshooting via your FAQs but I didn't get anywhere. This is my third tiem calling you.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Let's check your firmware version.\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Firmware v1.0.3; still failing.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Try a factory reset.\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"Reset done; error 42 now.\"}])\n",
    "await session.add_items([{\"role\": \"assistant\", \"content\": \"Try to install a new firmware.\"}])\n",
    "await session.add_items([{\"role\": \"user\", \"content\": \"I tried but I got another error now.\"}])\n",
    "# At this point, with max_turns=3, everything *before* the earliest of the last 3 user\n",
    "# messages is summarized into a synthetic pair, and the last 3 turns remain verbatim.\n",
    "\n",
    "history = await session.get_items()\n",
    "# Pass `history` into your agent runner / responses call as the conversation context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9f229de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"Hi, my router won't connect. by the way, I am using Windows 10. I tried troubleshooting via your FAQs but I didn't get anywhere. This is my third tiem calling you.\"},\n",
       " {'role': 'assistant', 'content': \"Let's check your firmware version.\"},\n",
       " {'role': 'user', 'content': 'Firmware v1.0.3; still failing.'},\n",
       " {'role': 'assistant', 'content': 'Try a factory reset.'},\n",
       " {'role': 'user', 'content': 'Reset done; error 42 now.'},\n",
       " {'role': 'assistant', 'content': 'Try to install a new firmware.'},\n",
       " {'role': 'user', 'content': 'I tried but I got another error now.'}]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1e26d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"I still have a problem with my router.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "950d663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    support_agent,\n",
    "    message,\n",
    "    session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6af52337",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = await session.get_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5c071451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synthetic': True,\n",
       " 'kind': 'history_summary',\n",
       " 'summary_for_turns': '< all before idx 2 >'}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[1]['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "64d555ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **User Goal:** Connect router to Windows 10.\n",
      "- **Constraints:** Previous attempts using FAQs were unsuccessful. Third contact for support.\n",
      "- **Current Environment:** Windows 10.\n",
      "- **Tried:** FAQ troubleshooting.\n",
      "- **Current Status:** Unable to connect router.\n",
      "- **Next Step:** Check router firmware version.\n"
     ]
    }
   ],
   "source": [
    "print(history[1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a935659",
   "metadata": {},
   "source": [
    "Once the history exceeds `max_turns`. It keeps the most recent N user turns intact, **summarizes everything older into two synthetic messages**:\n",
    "\n",
    "* `user`: *\"Summarize the conversation we had so far.\"*\n",
    "* `assistant`: *generated summary*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744c6d8",
   "metadata": {},
   "source": [
    "### Notes & design choices\n",
    "\n",
    "* **Turn boundary preserved at the “fresh” side**: the **last `max_turns` user turns** remain verbatim; everything older is compressed.\n",
    "* **Two-message summary block**: easy for downstream tooling to detect or display (`metadata.synthetic == True`).\n",
    "* **Async + lock discipline**: we **release the lock** while the (potentially slow) summarization runs; then re-check the condition before applying the summary to avoid racey merges.\n",
    "* **Idempotent behavior**: if more messages arrive during summarization, the post-await recheck prevents stale rewrites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730afac9",
   "metadata": {},
   "source": [
    "## Evals\n",
    "\n",
    "At the end of the day, **evals is all you need**—even for context engineering. The key question to ask is: *how do we know the model isn’t “losing context” or \"confusing context\"?*\n",
    "\n",
    "While a full cookbook aorund memory could stand on its own in the future, here are some lightweight evaluation harness ideas to start with:\n",
    "\n",
    "* **Baseline & Deltas:** Continue running your core eval sets and compare before/after experiments to measure memory improvements.\n",
    "* **LLM-as-Judge:** Use a model with a carefully designed grader prompt to evaluate summarization quality. Focus on whether it captures the most important details in the correct format.\n",
    "* **Transcript Replay:** Re-run long conversations and measure next-turn accuracy with and without context trimming. Metrics could include exact match on entities/IDs and rubric-based scoring on reasoning quality.\n",
    "* **Error Regression Tracking:** Watch for common failure modes—unanswered questions, dropped constraints, or unnecessary/repeated tool calls.\n",
    "* **Token Pressure Checks:** Flag cases where token limits force dropping protected context. Log before/after token counts to detect when critical details are being pruned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2843701",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
