{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate a book written in LaTeX from Slovenian into English\n",
    "\n",
    "With permission of the author, we will demonstrate how to translate the book [Euclidean Plane Geometry](https://sites.google.com/site/projektivna/), written by Milan MitroviÄ‡ from Slovenian into English, without modifying any of the LaTeX commands.\n",
    "\n",
    "To achieve this, we will first split the book into chunks, each roughly a page long, then translate each chunk into English, and finally stitch them back together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# OpenAI tiktoken tokenizer: https://github.com/openai/tiktoken\n",
    "# we use it to count the number of tokens in the text\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "with open(\"data/geometry_slovenian.tex\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Count the tokens in each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the largest chunk:  1211\n",
      "Number of chunks:  5877\n"
     ]
    }
   ],
   "source": [
    "chunks = text.split('\\n\\n')\n",
    "ntokens = []\n",
    "for chunk in chunks:\n",
    "    ntokens.append(len(tokenizer.encode(chunk)))\n",
    "print(\"Size of the largest chunk: \", max(ntokens))\n",
    "print(\"Number of chunks: \", len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that a double newline is a good separator in this case, in order not to break the flow of the text. Also no individual chunk is larger than 1211 tokens. The model we will use is gpt-4o, which has a limit of 16,384 tokens, so we don't need to worry about breaking the chunks down further.\n",
    "\n",
    "We will group the shorter chunks into chunks of around 15000 tokens, to increase the coherence of the text, and decrease the frequency of breaks within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_chunks(chunks, ntokens, max_len=15000, hard_max_len=16000):\n",
    "    \"\"\"\n",
    "    Group very short chunks, to form approximately page long chunks.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    cur_batch = \"\"\n",
    "    cur_tokens = 0\n",
    "    \n",
    "    # iterate over chunks, and group the short ones together\n",
    "    for chunk, ntoken in zip(chunks, ntokens):\n",
    "        # discard chunks that exceed hard max length\n",
    "        if ntoken > hard_max_len:\n",
    "            print(f\"Warning: Chunk discarded for being too long ({ntoken} tokens > {hard_max_len} token limit). Preview: '{chunk[:50]}...'\")\n",
    "            continue\n",
    "\n",
    "        # if room in current batch, add new chunk\n",
    "        if cur_tokens + 1 + ntoken <= max_len:\n",
    "            cur_batch += \"\\n\\n\" + chunk\n",
    "            cur_tokens += 1 + ntoken  # adds 1 token for the two newlines\n",
    "        # otherwise, record the batch and start a new one\n",
    "        else:\n",
    "            batches.append(cur_batch)\n",
    "            cur_batch = chunk\n",
    "            cur_tokens = ntoken\n",
    "            \n",
    "    if cur_batch:  # add the last batch if it's not empty\n",
    "        batches.append(cur_batch)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "\n",
    "chunks = group_chunks(chunks, ntokens)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that adding a sample untranslated and translated first command, where only the content of the chapter name needs to be translated, helps to get more consistent results.\n",
    "\n",
    "The format of the prompt sent to the model consists of:\n",
    "1. A high level instruction to translate only the text, but not commands into the desired language\n",
    "2. A sample untranslated command, where only the content of the chapter name needs to be translated\n",
    "3. The chunk of text to be translated\n",
    "4. The translated sample command from 2, which shows the model the beginning of the translation process\n",
    "\n",
    "The expected output is the translated chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_chunk(chunk, model='gpt-4o',\n",
    "                    dest_language='English',\n",
    "                    sample_translation=(\"\\poglavje{Osnove Geometrije} \\label{osn9Geom}\", \"\\poglavje{The basics of Geometry} \\label{osn9Geom}\")\n",
    "                    ):\n",
    "    prompt = f'''Translate only the text from the following LaTeX document into {dest_language}. Leave all LaTeX commands unchanged\n",
    "    \n",
    "\"\"\"\n",
    "{sample_translation[0]}\n",
    "{chunk}\"\"\"\n",
    "\n",
    "{sample_translation[1]}\n",
    "'''\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\":prompt}],\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        max_tokens=15000,\n",
    "    )\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    result = result.replace('\"\"\"', '') # remove the double quotes, as we used them to surround the text\n",
    "    return result\n",
    "print(translate_chunk(chunks[0], model='gpt-4o', dest_language='English'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that this one chunk in particular translates only the text, but leaves LaTeX commands intact.\n",
    "\n",
    "Let's now translate all the chunks in the book - this will take 2-3 hours, as we're processing requests sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_language = \"English\"\n",
    "\n",
    "translated_chunks = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(str(i+1) + \" / \" + str(len(chunks)))\n",
    "    # translate each chunk\n",
    "    translated_chunks.append(translate_chunk(chunk, model='gpt-4o', dest_language=dest_language))\n",
    "\n",
    "# join the chunks together\n",
    "result = '\\n\\n'.join(translated_chunks)\n",
    "\n",
    "# save the final result\n",
    "with open(f\"data/geometry_{dest_language}.tex\", \"w\") as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 / 39 translated.\n",
      "Chunk 3 / 39 translated.\n",
      "Chunk 5 / 39 translated.\n",
      "Chunk 2 / 39 translated.\n",
      "Chunk 6 / 39 translated.\n",
      "Chunk 4 / 39 translated.\n",
      "Chunk 8 / 39 translated.\n",
      "Chunk 7 / 39 translated.\n",
      "Chunk 9 / 39 translated.\n",
      "Chunk 14 / 39 translated.\n",
      "Chunk 10 / 39 translated.\n",
      "Chunk 11 / 39 translated.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Function to translate a single chunk\n",
    "def translate_chunk_wrapper(chunk, model='gpt-4o', dest_language='English'):\n",
    "    return translate_chunk(chunk, model=model, dest_language=dest_language)\n",
    "\n",
    "# Set the destination language\n",
    "dest_language = \"English\"\n",
    "\n",
    "# List to store translated chunks\n",
    "translated_chunks = []\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize the translation\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Submit all translation tasks\n",
    "    futures = {executor.submit(translate_chunk_wrapper, chunk, 'gpt-4o', dest_language): i for i, chunk in enumerate(chunks)}\n",
    "    \n",
    "    # Process completed tasks as they finish\n",
    "    for future in as_completed(futures):\n",
    "        i = futures[future]\n",
    "        try:\n",
    "            translated_chunk = future.result()\n",
    "            translated_chunks.append(translated_chunk)\n",
    "            print(f\"Chunk {i+1} / {len(chunks)} translated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Chunk {i+1} failed with exception: {e}\")\n",
    "\n",
    "# Join the translated chunks together\n",
    "result = '\\n\\n'.join(translated_chunks)\n",
    "\n",
    "# Save the final result\n",
    "with open(f\"data/geometry_{dest_language}.tex\", \"w\") as f:\n",
    "    f.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
