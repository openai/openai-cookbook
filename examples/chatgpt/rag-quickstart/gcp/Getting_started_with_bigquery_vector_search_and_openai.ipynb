{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Bigquery with GCP Functions and GPT actions in ChatGPT\n",
    "\n",
    "This notebook provides step-by-step instructions on using Google Cloud BigQuery as a database with vector search capabilities, with OpenAI embeddings, then creating a Google Cloud Function on top to plug into a Custom GPT in ChatGPT.\n",
    "\n",
    "This can be a solution for customers looking to set up RAG infrastructure contained within Google Cloud Platform (GCP), and exposing it as an endpoint to integrate that with other platforms such as ChatGPT.\n",
    "\n",
    "Google Cloud BigQuery is a fully-managed, serverless data warehouse that enables super-fast SQL queries using the processing power of Google's infrastructure. It allows developers to store and analyze massive datasets with ease.\n",
    "\n",
    "Google Cloud Functions is a lightweight, event-based, asynchronous compute solution that allows you to create small, single-purpose functions that respond to cloud events without managing servers or runtime environments.\n",
    "\n",
    "## Pre-requisites:\n",
    "\n",
    "To run this cookbook, you must have:\n",
    "- A GCP project you have access to\n",
    "- GCP user with permission to create a BigQuery dataset and Google Cloud Function\n",
    "- [GCP CLI](https://cloud.google.com/sdk/docs/downloads-interactive) installed and connected\n",
    "- OpenAI API key\n",
    "- ChatGPT Plus, Teams or Enterprise subscription\n",
    "\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Below is a diagram of the architecture of this solution, which we'll walk through step-by-step:\n",
    "\n",
    "![bigquery-rag-architecture.png](../../../../images/bigquery_rag_architecture.png)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **[Setup of Environment](#set-up-environment)** Setup environment by installing and importing the required libraries and configuring our GCP settings. Includes:\n",
    "    - [Install and Import Required Libraries](#install-and-import-required-libraries)\n",
    "    - [Configure GCP project](#configure-gcp-project)\n",
    "    - [Configure OpenAI Settings](#configure-openai-settings)\n",
    "\n",
    "\n",
    "2. **[Prepare Data](#prepare-data)** Prepare the data for uploading by embedding the documents, as well as capturing additional metadata. We will use a subset of OpenAI's docs as example data for this.\n",
    "\n",
    "3. **[Create BigQuery Table with Vector search](#create-bigquery-table-with-vector-search)**  \n",
    "Create a BigQuery table and upload the data we've prepared. Includes:\n",
    "\n",
    "    - [Create Dataset](#create-bigquery-dataset): Steps to create a dataset in BigQuery.\n",
    "    - [Create Table and upload data](#creating-table-and-upload-data): Instructions to create a table in BigQuery.\n",
    "\n",
    "4. **[Create GCP Function](#create-gcp-function)** using gcloud CLI and environment variables computed previously\n",
    "\n",
    "5. **[Input in a Custom GPT in ChatGPT](#input-in-a-custom-gpt-in-chatgpt)** Perform searches on the embedded data in BigQuery:\n",
    "\n",
    "    - [Vector Search](#test-search): Steps to perform vector-based search queries.\n",
    "    - [Metadata filtering Search](#perform-search-with-metadata-filtering): Instructions for performing metadata filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import required libraries\n",
    "The below libraries can be categorized as standard Python libraries, third-party libraries, and GCP-related libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q google-auth\n",
    "! pip install -q openai\n",
    "! pip install -q pandas\n",
    "! pip install -q google-cloud-functions\n",
    "! pip install -q python-dotenv\n",
    "! pip install -q pyperclip\n",
    "! pip install -q PyPDF2\n",
    "! pip install -q tiktoken\n",
    "! pip install -q google-cloud-bigquery\n",
    "! pip install -q pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import json  \n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "from itertools import islice\n",
    "import concurrent.futures\n",
    "import yaml\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import pyperclip\n",
    "\n",
    "# OpenAI Libraries\n",
    "from openai import OpenAI\n",
    "\n",
    "# Google Cloud Identity and Credentials\n",
    "from google.auth import default\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import functions_v1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure GCP project\n",
    "\n",
    "If not already set-up, we'll install GCP CLI's, authenticate to GCP and set your default project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gcloud to PATH\n",
    "os.environ['PATH'] += os.pathsep + os.path.expanduser('~/google-cloud-sdk/bin')\n",
    "\n",
    "# Verify gcloud is in PATH\n",
    "! gcloud --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"<insert_project_id>\"  # Replace with your actual project ID\n",
    "! gcloud config set project {project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud services enable cloudfunctions.googleapis.com\n",
    "! gcloud services enable cloudbuild.googleapis.com\n",
    "! gcloud services enable bigquery.googleapis.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure OpenAI settings\n",
    "\n",
    "This section guides you through setting up authentication for  OpenAI. Before going through this section, make sure you have your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as an env var>\") # Saving this as a variable to reference in function app in later step\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "embeddings_model = \"text-embedding-3-small\" # We'll use this by default, but you can change to your text-embedding-3-large if desired"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure GCP BigQuery with Vector Search capabilities\n",
    "\n",
    "This section explains how to create a dataset in BigQuery and store vectors of float, used for embeddings & vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth import default\n",
    "\n",
    "# Use default credentials\n",
    "credentials, project_id = default()\n",
    "region = \"us-central1\" # e.g: \"us-central1\"\n",
    "print(\"Default Project ID:\", project_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "We're going to embed and store a few pages of the OpenAI docs in the oai_docs folder. We'll first embed each, add it to a CSV, and then use that CSV to upload to the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use some techniques highlighted in [this cookbook](khttps://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb). This is a quick way to embed text, without taking into account variables like sections, using our vision model to describe images/graphs/diagrams, overlapping text between chunks for longer documents, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to handle longer text files beyond the context of 8191 tokens, we can either use the chunk embeddings separately, or combine them in some way, such as averaging (weighted by the size of each chunk).\n",
    "\n",
    "We will take a function from Python's own cookbook that breaks up a sequence into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n",
    "    # batched('ABCDEFG', 3) --> ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while (batch := tuple(islice(it, n))):\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function that encodes a string into tokens and then breaks it up into chunks. We'll use tiktoken, a fast open-source tokenizer by OpenAI.\n",
    "\n",
    "To read more about counting tokens with Tiktoken, check out [this cookbook](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_tokens(text, chunk_length, encoding_name='cl100k_base'):\n",
    "    # Get the encoding object for the specified encoding name. OpenAI's tiktoken library, which is used in this notebook, currently supports two encodings: 'bpe' and 'cl100k_base'. The 'bpe' encoding is used for GPT-3 and earlier models, while 'cl100k_base' is used for newer models like GPT-4.\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # Encode the input text into tokens\n",
    "    tokens = encoding.encode(text)\n",
    "    # Create an iterator that yields chunks of tokens of the specified length\n",
    "    chunks_iterator = batched(tokens, chunk_length)\n",
    "    # Yield each chunk from the iterator\n",
    "    yield from chunks_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write a function that safely handles embedding requests, even when the input text is longer than the maximum context length, by chunking the input tokens and embedding each chunk individually. The average flag can be set to True to return the weighted average of the chunk embeddings, or False to simply return the unmodified list of chunk embeddings.\n",
    "\n",
    "> Note: there are other techniques you can take here, including:\n",
    "> - using GPT-4o to capture images/chart descriptions for embedding\n",
    "> - chunking based on paragraphs or sections\n",
    "> - adding more descriptive metadata about each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING='cl100k_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, model):\n",
    "    # Generate embeddings for the provided text using the specified model\n",
    "    embeddings_response = openai_client.embeddings.create(model=model, input=text)\n",
    "    # Extract the embedding data from the response\n",
    "    embedding = embeddings_response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "def len_safe_get_embedding(text, model=embeddings_model, max_tokens=EMBEDDING_CTX_LENGTH, encoding_name=EMBEDDING_ENCODING):\n",
    "    # Initialize lists to store embeddings and corresponding text chunks\n",
    "    chunk_embeddings = []\n",
    "    chunk_texts = []\n",
    "    # Iterate over chunks of tokens from the input text\n",
    "    for chunk in chunked_tokens(text, chunk_length=max_tokens, encoding_name=encoding_name):\n",
    "        # Generate embeddings for each chunk and append to the list\n",
    "        chunk_embeddings.append(generate_embeddings(chunk, model=model))\n",
    "        # Decode the chunk back to text and append to the list\n",
    "        chunk_texts.append(tiktoken.get_encoding(encoding_name).decode(chunk))\n",
    "    # Return the list of chunk embeddings and the corresponding text chunks\n",
    "    return chunk_embeddings, chunk_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define a helper function that will capture additional metadata about the documents. In this example, I'll choose from a list of categories to use later on in a metadata filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['authentication','models','techniques','tools','setup','billing_limits','other']\n",
    "\n",
    "def categorize_text(text, categories):\n",
    "\n",
    "    # Create a prompt for categorization\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"You are an expert in LLMs, and you will be given text that corresponds to an article in OpenAI's documentation.\n",
    "         Categorize the document into one of these categories: {', '.join(categories)}. Only respond with the category name and nothing else.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    try:\n",
    "        # Call the OpenAI API to categorize the text\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        # Extract the category from the response\n",
    "        category = response.choices[0].message.content\n",
    "        return category\n",
    "    except Exception as e:\n",
    "        print(f\"Error categorizing text: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define some helper functions to process the .txt files in the oai_docs folder. Feel free to use this on your own data, this supports both .txt and .pdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Initialize the PDF reader\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    # Iterate through each page in the PDF and extract text\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def process_file(file_path, idx, categories, embeddings_model):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Processing file {idx + 1}: {file_name}\")\n",
    "    \n",
    "    # Read text content from .txt files\n",
    "    if file_name.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    # Extract text content from .pdf files\n",
    "    elif file_name.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    \n",
    "    title = file_name\n",
    "    # Generate embeddings for the title\n",
    "    title_vectors, title_text = len_safe_get_embedding(title, embeddings_model)\n",
    "    print(f\"Generated title embeddings for {file_name}\")\n",
    "    \n",
    "    # Generate embeddings for the content\n",
    "    content_vectors, content_text = len_safe_get_embedding(text, embeddings_model)\n",
    "    print(f\"Generated content embeddings for {file_name}\")\n",
    "    \n",
    "    category = categorize_text(' '.join(content_text), categories)\n",
    "    print(f\"Categorized {file_name} as {category}\")\n",
    "    \n",
    "    # Prepare the data to be appended\n",
    "    data = []\n",
    "    for i, content_vector in enumerate(content_vectors):\n",
    "        data.append({\n",
    "            \"id\": f\"{idx}_{i}\",\n",
    "            \"vector_id\": f\"{idx}_{i}\",\n",
    "            \"title\": title_text[0],\n",
    "            \"text\": content_text[i],\n",
    "            \"title_vector\": json.dumps(title_vectors[0]),  # Assuming title is short and has only one chunk\n",
    "            \"content_vector\": json.dumps(content_vector),\n",
    "            \"category\": category\n",
    "        })\n",
    "        print(f\"Appended data for chunk {i + 1}/{len(content_vectors)} of {file_name}\")\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use this helper function to process our OpenAI documentation. Feel free to update this to use your own data by changing the folder in process_files below.\n",
    "\n",
    "Note that this will process the documents in chosen folder concurrently, so this should take <30 seconds if using txt files, and slightly longer if using PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customize the location below if you are using different data besides the OpenAI documentation. Note that if you are using a different dataset, you will need to update the categories list as well.\n",
    "folder_name = \"../../../data/oai_docs\"\n",
    "\n",
    "files = [os.path.join(folder_name, f) for f in os.listdir(folder_name) if f.endswith('.txt') or f.endswith('.pdf')]\n",
    "data = []\n",
    "\n",
    "# Process each file concurrently\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_file, file_path, idx, categories, embeddings_model): idx for idx, file_path in enumerate(files)}\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            data.extend(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "# Write the data to a CSV file\n",
    "csv_file = os.path.join(\"..\", \"embedded_data.csv\")\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = [\"id\", \"vector_id\", \"title\", \"text\", \"title_vector\", \"content_vector\",\"category\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "        print(f\"Wrote row with id {row['id']} to CSV\")\n",
    "\n",
    "# Convert the CSV file to a Dataframe\n",
    "article_df = pd.read_csv(\"../embedded_data.csv\")\n",
    "# Read vectors from strings back into a list using json.loads\n",
    "article_df[\"title_vector\"] = article_df.title_vector.apply(json.loads)\n",
    "article_df[\"content_vector\"] = article_df.content_vector.apply(json.loads)\n",
    "article_df[\"vector_id\"] = article_df[\"vector_id\"].apply(str)\n",
    "article_df[\"category\"] = article_df[\"category\"].apply(str)\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an `embedded_data.csv` file with six columns that we can upload to our vector database! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create BigQuery table with Vector Search\n",
    "\n",
    "## Create BigQuery dataset\n",
    "\n",
    "We'll leverage Google SDK and create a dataset named \"oai_docs\" with a table name of \"embedded_data\", but feel free to change those variables (you can also change regions).\n",
    "\n",
    "*PS: We won't create a BigQuery index, that could improve the performance of the vector search, because such index requires more than 1k rows in our dataset which we don't have in our example, but feel free to leverage that for your own use-case.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigquery table\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import Conflict\n",
    "\n",
    "# Define the dataset ID (project_id.dataset_id)\n",
    "raw_dataset_id = 'oai_docs'\n",
    "dataset_id = project_id + '.' + raw_dataset_id\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Construct a full Dataset object to send to the API\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "# Specify the geographic location where the dataset should reside\n",
    "dataset.location = \"US\"\n",
    "\n",
    "# Send the dataset to the API for creation\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print(f\"Created dataset {client.project}.{dataset.dataset_id}\")\n",
    "except Conflict:\n",
    "    print(f\"dataset {dataset.dataset_id } already exists\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file, properly handling multiline fields\n",
    "csv_file_path = \"../embedded_data.csv\"\n",
    "df = pd.read_csv(csv_file_path, engine='python', quotechar='\"', quoting=1)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating table and upload data\n",
    "\n",
    "We'll create the table with the attribute name and types. Note the 'content_vector' attribute that allows to store a vector of float for a single row, which we'll use for our vector search.\n",
    "\n",
    "This code will then loop on our CSVs previously created to insert the rows into Bigquery. \n",
    "If you run this code multiple time, multiple identical rows will be inserted which will give less accurate results when doing search (you could put uniqueness on IDs or clean the DB each time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file, properly handling multiline fields\n",
    "dataset_id = project_id + '.' + raw_dataset_id\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "csv_file_path = \"../embedded_data.csv\"\n",
    "df = pd.read_csv(csv_file_path, engine='python', quotechar='\"', quoting=1)\n",
    "\n",
    "# Preprocess the data to ensure content_vector is correctly formatted\n",
    "# removing last and first character which are brackets [], comma splitting and converting to float\n",
    "def preprocess_content_vector(row):\n",
    "    row['content_vector'] = [float(x) for x in row['content_vector'][1:-1].split(',')]\n",
    "    return row\n",
    "\n",
    "# Apply preprocessing to the dataframe\n",
    "df = df.apply(preprocess_content_vector, axis=1)\n",
    "\n",
    "# Define the schema of the final table\n",
    "final_schema = [\n",
    "    bigquery.SchemaField(\"id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"vector_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"title\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"text\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"title_vector\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"content_vector\", \"FLOAT64\", mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"category\", \"STRING\"),\n",
    "]\n",
    "\n",
    "# Define the final table ID\n",
    "raw_table_id = 'embedded_data'\n",
    "final_table_id = f'{dataset_id}.' + raw_table_id\n",
    "\n",
    "# Create the final table object\n",
    "final_table = bigquery.Table(final_table_id, schema=final_schema)\n",
    "\n",
    "# Send the table to the API for creation\n",
    "final_table = client.create_table(final_table, exists_ok=True)  # API request\n",
    "print(f\"Created final table {project_id}.{final_table.dataset_id}.{final_table.table_id}\")\n",
    "\n",
    "# Convert DataFrame to list of dictionaries for BigQuery insertion\n",
    "rows_to_insert = df.to_dict(orient='records')\n",
    "\n",
    "# Upload data to the final table\n",
    "errors = client.insert_rows_json(f\"{final_table.dataset_id}.{final_table.table_id}\", rows_to_insert)  # API request\n",
    "\n",
    "if errors:\n",
    "    print(f\"Encountered errors while inserting rows: {errors}\")\n",
    "else:\n",
    "    print(f\"Successfully loaded data into {dataset_id}:{final_table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test search\n",
    "Now that the data is uploaded, we'll test both pure vector similarity search and with metadata filtering locally below to make sure it is working as expected.\n",
    "\n",
    "You can test both a pure vector search and metadata filtering.\n",
    "\n",
    "The query below is pure vector search, where we don't filter out on category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What model should I use to embed?\"\n",
    "category = \"models\"\n",
    "\n",
    "embedding_query = generate_embeddings(query, embeddings_model)\n",
    "embedding_query_list = ', '.join(map(str, embedding_query))\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH search_results AS (\n",
    "  SELECT query.id AS query_id, base.id AS base_id, distance\n",
    "  FROM VECTOR_SEARCH(\n",
    "    TABLE oai_docs.embedded_data, 'content_vector',\n",
    "    (SELECT ARRAY[{embedding_query_list}] AS content_vector, 'query_vector' AS id),\n",
    "    top_k => 2, distance_type => 'COSINE', options => '{{\"use_brute_force\": true}}')\n",
    ")\n",
    "SELECT sr.query_id, sr.base_id, sr.distance, ed.text, ed.title\n",
    "FROM search_results sr\n",
    "JOIN oai_docs.embedded_data ed ON sr.base_id = ed.id\n",
    "ORDER BY sr.distance ASC\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()  # Wait for the job to complete\n",
    "\n",
    "for row in results:\n",
    "    print(f\"query_id: {row['query_id']}, base_id: {row['base_id']}, distance: {row['distance']}, text_truncated: {row['text'][0:100]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform search with metadata filtering\n",
    "Metadata filtering allows to restrict findings that have certain attributes on top of having the closest semantic findings of vector search.\n",
    "\n",
    "The provided code snippet demonstrates how to execute a query with metadata filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What model should I use to embed?\"\n",
    "category = \"models\"\n",
    "\n",
    "embedding_query = generate_embeddings(query, embeddings_model)\n",
    "embedding_query_list = ', '.join(map(str, embedding_query))\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH search_results AS (\n",
    "  SELECT query.id AS query_id, base.id AS base_id, distance\n",
    "  FROM VECTOR_SEARCH(\n",
    "    (SELECT * FROM oai_docs.embedded_data WHERE category = '{category}'), \n",
    "    'content_vector',\n",
    "    (SELECT ARRAY[{embedding_query_list}] AS content_vector, 'query_vector' AS id),\n",
    "    top_k => 4, distance_type => 'COSINE', options => '{{\"use_brute_force\": true}}')\n",
    ")\n",
    "SELECT sr.query_id, sr.base_id, sr.distance, ed.text, ed.title, ed.category\n",
    "FROM search_results sr\n",
    "JOIN oai_docs.embedded_data ed ON sr.base_id = ed.id\n",
    "ORDER BY sr.distance ASC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()  # Wait for the job to complete\n",
    "\n",
    "for row in results:\n",
    "    print(f\"category: {row['category']}, title: {row['title']}, base_id: {row['base_id']}, distance: {row['distance']}, text_truncated: {row['text'][0:100]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GCP function\n",
    "\n",
    "## Exporting variables\n",
    "\n",
    "We'll deploy the function in `main.py` in this folder (also available [here](https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/main.py)).\n",
    "\n",
    "In a first step, we'll export the variables to target our table/dataset as well as to generate Embeddings using OpenAI's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the environment variables (they were used previously and are just retrieved)\n",
    "env_variables = {\n",
    "    'OPENAI_API_KEY': openai_api_key,\n",
    "    'EMBEDDINGS_MODEL': embeddings_model,\n",
    "    'PROJECT_ID': project_id,\n",
    "    'DATASET_ID': raw_dataset_id,\n",
    "    'TABLE_ID': raw_table_id\n",
    "}\n",
    "\n",
    "# Write the environment variables to a YAML file\n",
    "with open('env.yml', 'w') as yaml_file:\n",
    "    yaml.dump(env_variables, yaml_file, default_flow_style=False)\n",
    "\n",
    "print(\"env.yml file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the function\n",
    "\n",
    "We will now create a google function called \"openai_docs_search\" for our current project, for that we'll launch the CLI command below, leveraging the previously created environment variables. Note that this function can be called from everywhere without authentication, do not use that for production or add additional authentication mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud functions deploy openai_docs_search \\\n",
    "  --runtime python39 \\\n",
    "  --trigger-http \\\n",
    "  --allow-unauthenticated \\\n",
    "  --env-vars-file env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input in a Custom GPT in ChatGPT\n",
    "\n",
    "Now that we have a GCP Function that queries this Vector Search Index, let's put it as a GPT Action!\n",
    "\n",
    "See documentation [here](https://openai.com/index/introducing-gpts/) on GPTs and [here](https://platform.openai.com/docs/actions) on GPT Actions. Use the below as the instructions for the GPT and as the OpenAPI spec for the GPT Action.\n",
    "\n",
    "## Create OpenAPI Spec\n",
    "\n",
    "Below is a sample OpenAPI spec. When we run the block below, a functional spec should be copied to the clipboard to paste in the GPT Action.\n",
    "\n",
    "Note that this does not have any authentication by default, but you can set up GCP Functions with Auth by following GCP's docs [here](https://cloud.google.com/functions/docs/securing/authenticating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = f\"\"\"\n",
    "openapi: 3.1.0\n",
    "info:\n",
    "  title: OpenAI API documentation search\n",
    "  description: API to perform a semantic search over OpenAI APIs\n",
    "  version: 1.0.0\n",
    "servers:\n",
    "  - url: https://{region}-{project_id}.cloudfunctions.net\n",
    "    description: Main (production) server\n",
    "paths:\n",
    "  /openai_docs_search:\n",
    "    post:\n",
    "      operationId: openai_docs_search\n",
    "      summary: Perform a search\n",
    "      description: Returns search results for the given query parameters.\n",
    "      requestBody:\n",
    "        required: true\n",
    "        content:\n",
    "          application/json:\n",
    "            schema:\n",
    "              type: object\n",
    "              properties:\n",
    "                query:\n",
    "                  type: string\n",
    "                  description: The search query string\n",
    "                top_k:\n",
    "                  type: integer\n",
    "                  description: Number of top results to return. Maximum is 3.\n",
    "                category:\n",
    "                  type: string\n",
    "                  description: The category to filter on, on top of similarity search (used for metadata filtering). Possible values are {categories}.\n",
    "      responses:\n",
    "        '200':\n",
    "          description: A JSON response with the search results\n",
    "          content:\n",
    "            application/json:\n",
    "              schema:\n",
    "                type: object\n",
    "                properties:\n",
    "                  items:\n",
    "                    type: array\n",
    "                    items:\n",
    "                      type: object\n",
    "                      properties:\n",
    "                        text:\n",
    "                          type: string\n",
    "                          example: \"Learn how to turn text into numbers, unlocking use cases like search...\"\n",
    "                        title:\n",
    "                          type: string\n",
    "                          example: \"embeddings.txt\"\n",
    "                        distance:\n",
    "                          type: number\n",
    "                          format: float\n",
    "                          example: 0.484939891778730\n",
    "                        category:\n",
    "                          type: string\n",
    "                          example: \"models\"\n",
    "\"\"\"\n",
    "print(spec)\n",
    "pyperclip.copy(spec)\n",
    "print(\"OpenAPI spec copied to clipboard\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GPT Instructions\n",
    "\n",
    "Feel free to modify instructions as you see fit. Check out our docs [here](https://platform.openai.com/docs/guides/prompt-engineering) for some tips on prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = f'''\n",
    "You are an OpenAI docs assistant. You have an action in your knowledge base where you can make a POST request to search for information. The POST request should always include: {{\n",
    "    \"query\": \"<user_query>\",\n",
    "    \"k_\": <integer>,\n",
    "    \"category\": <string, but optional>\n",
    "}}. Your goal is to assist users by performing searches using this POST request and providing them with relevant information based on the query.\n",
    "\n",
    "You must only include knowledge you get from your action in your response.\n",
    "The category must be from the following list: {categories}, which you should determine based on the user's query. If you cannot determine, then do not include the category in the POST request.\n",
    "'''\n",
    "pyperclip.copy(instructions)\n",
    "print(\"GPT Instructions copied to clipboard\")\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "We've now succesfully integrated GCP BigQuery Vector Search with GPT Actions in ChatGPT by doing the following:\n",
    "1. Embedded docs using OpenAI's embeddings, while adding some additional metadata using gpt-4o.\n",
    "2. Uploaded that data to GCP BigQuery (raw data and vectors of embeddings)\n",
    "3. Created an endpoint on GCP Functions to retrieve those\n",
    "4. Incorporated it into a custom GPT.\n",
    "\n",
    "Our GPT can now retrieve informaiton to help answer user queries, making it much more accurate and customized to our data. Here's the GPT in action:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gcp-rag-quickstart-gpt.png](../../../../images/gcp_rag_quickstart_gpt.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
