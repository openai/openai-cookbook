{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bb8b92",
   "metadata": {},
   "source": [
    "# Building a Codex Evaluation Pipeline before Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3bdb2b",
   "metadata": {},
   "source": [
    "When you want to roll Codex out into large-scale engineering workflows, the fastest way to build confidence is to **evaluate it on work you already know how to do well**. Large mechanical refactoring tasks, such as API or interface migrations are a common example. They are time-consuming, repetitive, and well-understood, which makes them strong candidates for offloading to a coding agent like Codex.\n",
    "\n",
    "That said, success here depends on getting several moving parts right. You need to validate that your prompts, documentation, and context are sufficient for the agent to operate effectively, while also understanding which Codex model is best suited for the task and how much reasoning depth it actually requires. Rigorous evaluation is what turns these questions from guesswork into confidence.\n",
    "\n",
    "The goal here, is to measure how well Codex matches your real-world standards **before** handing it more autonomy.\n",
    "\n",
    "In this cookbook, we walk through a simple Codex evaluation pipeline leveraging the Codex SDK. We’ll dive into code examples, highlight the strategy, and show how to turn historical human patches into a repeatable Codex benchmark. \n",
    "\n",
    "The pipeline mirrors how you might evaluate a model as a business: define tasks, run Codex on the same codebase humans solved, and compare outputs with a grader. The overall process, can be broken down to the following steps:\n",
    "\n",
    "1. Define evaluation tasks from real historical refactors.\n",
    "2. Recreate the exact code context with git worktrees.\n",
    "3. Run Codex with a focused task prompt.\n",
    "4. Grade Codex output against the original human patch.\n",
    "5. Summarize and visualize results in a dashboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02690cbe",
   "metadata": {},
   "source": [
    "## High-level Architecture at a Glance\n",
    "\n",
    "The pipeline mirrors the below reference diagram. You can think of it as four stages:\n",
    "\n",
    "1. **Task ingestion**: Parse a YAML task file describing refactor work.\n",
    "2. **Orchestration**: Check out the right base commit into a Git worktree.\n",
    "3. **Codex solve**: Run the prompt against the workspace and capture a diff.\n",
    "4. **Grading & scoring**: Compare generated vs. human diffs and store results.\n",
    "5. **Data Analysis**: Aggregate results across tasks and runs to analyze model performance, failure modes, and trade-offs (e.g. accuracy vs. reasoning depth), enabling informed decisions about prompts, models, and rollout readiness.\n",
    "\n",
    "\n",
    "<img src=\"./images/eval_pipeline.jpeg\" alt=\"Evaluation pipeline\" width=800/>\n",
    "\n",
    "In the next section, we'll be going through the implementation details step by step.\n",
    "\n",
    "You can find the full working example on the Github repository [here](https://github.com/openai/openai-cookbook/tree/main/examples/codex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f221365",
   "metadata": {},
   "source": [
    "## Step 1: Define evaluation tasks (YAML)\n",
    "\n",
    "Tasks are stored as structured YAML in `evals/tasks.yaml`. Each task includes:\n",
    "\n",
    "- The **Gerrit Change-Id** of a historical patch\n",
    "- A prompt describing the refactoring task\n",
    "- Metadata like patch id, path, and summary\n",
    "\n",
    "Example:\n",
    "\n",
    "```yaml\n",
    "tasks:\n",
    "- name: \"192755_embed-fonts-decomposed-pdf\"\n",
    "  patch_id: 192755\n",
    "  change_id: Ib6e354d5a4a9076b81e6a26fe78bdd4994024ec1\n",
    "  author: \"John Doe\"\n",
    "  patch_summary: >\n",
    "    Add scrollbar width to desired size if enabled\n",
    "  files_changed: 1\n",
    "  code_scope_path: source/tbxctrls/linectrl.cxx\n",
    "  task_prompt: >\n",
    "    Add scrollbar width to desired size if enabled in tbxctrls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c5842",
   "metadata": {},
   "source": [
    "## Step 2 — Retrieve the ground‑truth diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4825445",
   "metadata": {},
   "source": [
    "The pipeline resolves the commit hash into:\n",
    "\n",
    "- **Merged commit** (the human patch you want to match)\n",
    "- **Base commit** (its parent; the repo state before the change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746026b0",
   "metadata": {},
   "source": [
    "That logic lives in [`blog/code_example/src/git.ts`](https://github.com/openai/openai-cookbook/tree/main/examples/codex/codex_evaluation/eval_pipeline/blog/code_example/src/git.ts) and [`blog/code_example/src/grader.ts`](https://github.com/openai/openai-cookbook/tree/main/examples/codex/codex_evaluation/eval_pipeline/blog/code_example/src/grader.ts). \n",
    "\n",
    "*The *Key idea:** this lets you compare the model against the *actual* human patch for the same task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4201f7",
   "metadata": {},
   "source": [
    "```ts\n",
    "export async function resolveBaseCommitFromCommitHash(repoDir: string, commitHash: string) {\n",
    "  const mergedCommit = await runGitCapture(['rev-parse', commitHash], repoDir);\n",
    "  const baseCommit = await runGitCapture(['rev-parse', `${mergedCommit}^`], repoDir);\n",
    "  return { mergedCommit, baseCommit };\n",
    "}\n",
    "\n",
    "export async function getCommitDiff(commitHash: string, repoDir: string): Promise<string> {\n",
    "  const { mergedCommit, baseCommit } = await resolveBaseCommitFromCommitHash(repoDir, commitHash);\n",
    "  return runGitCapture(['diff', '--no-color', `${baseCommit}..${mergedCommit}`], repoDir);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedaf647",
   "metadata": {},
   "source": [
    "In [`blog/code_example/src/main.ts`](https://github.com/openai/openai-cookbook/tree/main/examples/codex/codex_evaluation/eval_pipeline/blog/code_example/src/main.ts), that diff is written to `evals_output/<run>/original_diffs/<task>.diff`, giving you a canonical “human patch” to grade against."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea5002",
   "metadata": {},
   "source": [
    "## Step 3 — Run Codex in a clean worktree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825261b0",
   "metadata": {},
   "source": [
    "This is the heart of the evaluation. In [`blog/code_example/src/solver.ts`](https://github.com/openai/openai-cookbook/tree/main/examples/codex/codex_evaluation/eval_pipeline/blog/code_example/src/solver.ts), each task is executed like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc223da4",
   "metadata": {},
   "source": [
    "1. Create a git worktree at the base commit.\n",
    "2. Start a Codex thread that points at that worktree.\n",
    "3. Run the prompt.\n",
    "4. Capture the resulting diff.\n",
    "\n",
    "**Core snippet (simplified):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00430e5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01032d11",
   "metadata": {},
   "source": [
    "```ts\n",
    "const workTreeName = `wk-eval-${runId}-${row.commit_hash}`;\n",
    "const { baseCommit } = await resolveBaseCommitFromCommitHash(workingDirectory, row.commit_hash);\n",
    "await addWorktree(workTreeName, workingDirectory, baseCommit);\n",
    "\n",
    "const thread = codex.startThread({\n",
    "  skipGitRepoCheck: true,\n",
    "  workingDirectory: `${workingDirectory}/${workTreeName}`,\n",
    "  sandboxMode: 'workspace-write',\n",
    "});\n",
    "\n",
    "const prompt = row.additional_context\n",
    "  ? `${row.task}\\n\\nAdditional context:\\n${row.additional_context}`\n",
    "  : row.task;\n",
    "\n",
    "await thread.run(prompt);\n",
    "const diff = await diffWorktreeStream(fullWorkingDirectory, ['--no-color']);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd593ff",
   "metadata": {},
   "source": [
    "**What you get:**\n",
    "\n",
    "- A generated patch (`diffs/<task>.diff`)\n",
    "- A Codex execution log (`codex_logs/<task>.codex.log`)\n",
    "- Timing and metadata for benchmarking (`results.json`)\n",
    "\n",
    "This is exactly the “model solver” box in the reference architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c04fd",
   "metadata": {},
   "source": [
    "## Step 4 — Grade model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d064cf",
   "metadata": {},
   "source": [
    "We want to score *semantic similarity*, not formatting. The grader in [`blog/code_example/src/grader.ts`](https://github.com/openai/openai-cookbook/tree/main/examples/codex/codex_evaluation/eval_pipeline/blog/code_example/src/grader.ts) uses GPT‑5 to compare the generated diff and the original diff. It scores each task from 1–5 and returns a short rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b5213",
   "metadata": {},
   "source": [
    "**Grading prompt (condensed from the actual code):**\n",
    "\n",
    "```text\n",
    "You are an expert evaluator of code changes.\n",
    "You are given two git diffs and a task description.\n",
    "\n",
    "Evaluate the semantic similarity of GeneratedDiff vs OriginalDiff.\n",
    "Ignore formatting and style.\n",
    "Return a score 1–5 with a short rationale.\n",
    "```\n",
    "\n",
    "The actual implementation uses OpenAI’s structured response parsing with a Zod schema:\n",
    "\n",
    "```ts\n",
    "export const GradeResult = z.object({\n",
    "  score: z.number().min(0).max(5),\n",
    "  explanation: z.string().min(1),\n",
    "});\n",
    "  model: 'gpt-5',\n",
    "  instructions,\n",
    "  input: originalDiff,\n",
    "  text: { format: zodTextFormat(GradeResult, 'gradeResult') },\n",
    "});\n",
    "```\n",
    "**Why this matters:** you can compare *what matters* (task intent), not superficial patch shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f799a0bc",
   "metadata": {},
   "source": [
    "## Step 5 — Record metrics and analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deadc75",
   "metadata": {},
   "source": [
    "The evaluation run is logged in `evals_output/<experiment>-<run_id>/results.json` via [`blog/code_example/src/reporting.ts`](https://github.com/openai/openai-cookbook/tree/main/examples/codex/codex_evaluation/eval_pipeline/blog/code_example/src/reporting.ts). This file includes:\n",
    "\n",
    "- Task‑level scores, errors, and durations\n",
    "- Run‑level success rate and average score\n",
    "- Paths to diffs and Codex logs\n",
    "\n",
    "The `analytics/` folder contains a lightweight Flask dashboard that reads these results and renders:\n",
    "\n",
    "- Experiment summaries\n",
    "- Per‑task averages\n",
    "- Score distributions\n",
    "\n",
    "To launch it:\n",
    "\n",
    "```bash\n",
    "FLASK_APP=analytics.app flask run\n",
    "# or\n",
    "python analytics/app.py\n",
    "```\n",
    "\n",
    "Set `EVALS_OUTPUT_DIR` if you store outputs elsewhere.\n",
    "The [`blog/code_example/analysis/analyze.py`](https://github.com/openai/openai-cookbook/tree/main/examples/codex/codex_evaluation/eval_pipeline/blog/code_example/analysis/analyze.py) script summarizes a run and prints quick metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d6d2f",
   "metadata": {},
   "source": [
    "## Putting it all together (CLI example)\n",
    "\n",
    "The CLI wraps the full pipeline in one command (`src/main.ts`). Example usage:\n",
    "The CLI wraps the full pipeline in one command ([`blog/code_example/src/main.ts`](https://github.com/openai/openai-cookbook/tree/main/examples/codex/codex_evaluation/eval_pipeline/blog/code_example/src/main.ts)). Example usage:\n",
    "\n",
    "```bash\n",
    "npm run solve -- --tasks evals/tasks.yaml \\\n",
    "npm run solve -- --tasks data/tasks.yaml \\\n",
    "  -w /path/to/your/repo \\\n",
    "  -o evals_output \\\n",
    "  -e refactor-eval \\\n",
    "  -m gpt-5-codex \\\n",
    "  -r medium\n",
    "```\n",
    "\n",
    "This will:\n",
    "\n",
    "1. Load tasks from YAML.\n",
    "2. Resolve Gerrit diffs.\n",
    "3. Ask Codex to solve each task.\n",
    "2. Resolve **git commit hashes** into base + merged commits.\n",
    "3. Ask Codex to solve each task in a worktree.\n",
    "4. Grade each solution.\n",
    "5. Save results for dashboarding.\n",
    "5. Save results for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56668dd3",
   "metadata": {},
   "source": [
    "## How to adapt this to your business use case\n",
    "\n",
    "A few pragmatic tips when applying this to your own refactoring or bug‑fix workloads:\n",
    "\n",
    "- **Start with high‑signal tasks.** Pull 20–50 historical patches that represent your “core” work.\n",
    "- **Keep prompts stable.** Variation in prompt templates makes evals hard to compare.\n",
    "- **Track costs.** `src/reporting.ts` extracts usage from Codex logs; use this to estimate ROI.\n",
    "- **Track costs.** [`blog/code_example/src/reporting.ts`](https://github.com/openai/openai-cookbook/tree/main/examples/codex/codex_evaluation/eval_pipeline/blog/code_example/src/reporting.ts) extracts usage from Codex logs; use this to estimate ROI.\n",
    "- **Repeat runs.** Use the `--repeat` flag to evaluate variance across runs.\n",
    "- **Instrument failures.** Log both “no diff” and grading errors to identify failure modes quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookbook-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
