{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv8-mnmnj0Wp"
      },
      "source": [
        "# Deep Research Agents Cookbook\n",
        "\n",
        "This cookbook demonstrates how to build Agentic research workflows using the OpenAI Deep Research API and the OpenAI [Agents SDK](https://openai.github.io/openai-agents-python/). It is a continuation of [a fundamentals cookbook](https://cookbook.openai.com/examples/deep_research_api/introduction_to_deep_research_api), if you have not already familiarized yourself with that content, please consider doing so.\n",
        "\n",
        "You’ll learn how to orchestrate single and multi-agent pipelines, enrich user queries to maximize output quality, stream research progress, integrate web search and [MCP for internal file search](https://cookbook.openai.com/examples/deep_research_api/how_to_build_a_deep_research_mcp_server/readme), and architect a robust research application.\n",
        "\n",
        "Consider using Deep Research Agents for tasks that require planning, synthesis, tool use, or multi-step reasoning. Do not use Deep Research for trivial fact lookups, simple Q&A, or short-form chat, a vanilla openai.responsesAPI would be faster and cheaper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6gLS5aVj0Wr"
      },
      "source": [
        "### Prerequisites\n",
        "* OpenAI API key (set as OPENAI_API_KEY in your environment)\n",
        "* Agents SDK and OpenAI Python SDK\n",
        "\n",
        "### Setup\n",
        "*Install dependencies*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWE9uQq4j0Ws",
        "outputId": "99c15803-0506-4464-d624-a31b5bc809a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai>=1.88\n",
            "  Downloading openai-1.97.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting openai-agents>=0.0.19\n",
            "  Downloading openai_agents-0.2.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting anyio<5,>=3.5.0 (from openai>=1.88)\n",
            "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai>=1.88)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.88)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.88)\n",
            "  Downloading jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai>=1.88)\n",
            "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m626.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m936.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting sniffio (from openai>=1.88)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting tqdm>4 (from openai>=1.88)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions<5,>=4.11 (from openai>=1.88)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting griffe<2,>=1.5.6 (from openai-agents>=0.0.19)\n",
            "  Downloading griffe-1.9.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting mcp<2,>=1.11.0 (from openai-agents>=0.0.19)\n",
            "  Downloading mcp-1.12.2-py3-none-any.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests<3,>=2.0 (from openai-agents>=0.0.19)\n",
            "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting types-requests<3,>=2.0 (from openai-agents>=0.0.19)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai>=1.88)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting colorama>=0.4 (from griffe<2,>=1.5.6->openai-agents>=0.0.19)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting certifi (from httpx<1,>=0.23.0->openai>=1.88)\n",
            "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.88)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.88)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting httpx-sse>=0.4 (from mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting jsonschema>=4.20.0 (from mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading jsonschema-4.25.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting pydantic-settings>=2.5.2 (from mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting sse-starlette>=1.6.1 (from mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting starlette>=0.27 (from mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting uvicorn>=0.23.1 (from mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai>=1.88)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai>=1.88)\n",
            "  Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai>=1.88)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.0->openai-agents>=0.0.19)\n",
            "  Downloading charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.0->openai-agents>=0.0.19)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting attrs>=22.2.0 (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading rpds_py-0.26.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.5.2->mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting click>=7.0 (from uvicorn>=0.23.1->mcp<2,>=1.11.0->openai-agents>=0.0.19)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading openai-1.97.1-py3-none-any.whl (764 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.4/764.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading openai_agents-0.2.3-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.4/161.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading griffe-1.9.0-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl (320 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading mcp-1.12.2-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl (199 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.9/199.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema-4.25.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
            "Downloading starlette-0.47.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
            "Downloading rpds_py-0.26.0-cp312-cp312-macosx_11_0_arm64.whl (350 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m350.4/350.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, typing-extensions, tqdm, sniffio, rpds-py, python-multipart, python-dotenv, jiter, idna, httpx-sse, h11, distro, colorama, click, charset_normalizer, certifi, attrs, annotated-types, uvicorn, typing-inspection, types-requests, requests, referencing, pydantic-core, httpcore, griffe, anyio, starlette, sse-starlette, pydantic, jsonschema-specifications, httpx, pydantic-settings, openai, jsonschema, mcp, openai-agents\n",
            "Successfully installed annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 certifi-2025.7.14 charset_normalizer-3.4.2 click-8.2.1 colorama-0.4.6 distro-1.9.0 griffe-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 idna-3.10 jiter-0.10.0 jsonschema-4.25.0 jsonschema-specifications-2025.4.1 mcp-1.12.2 openai-1.97.1 openai-agents-0.2.3 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 python-multipart-0.0.20 referencing-0.36.2 requests-2.32.4 rpds-py-0.26.0 sniffio-1.3.1 sse-starlette-3.0.2 starlette-0.47.2 tqdm-4.67.1 types-requests-2.32.4.20250611 typing-extensions-4.14.1 typing-inspection-0.4.1 urllib3-2.5.0 uvicorn-0.35.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade \"openai>=1.88\" \"openai-agents>=0.0.19\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9lWqn_Wj0Wt"
      },
      "source": [
        "### Import libraries and configure client\n",
        "\n",
        "**Zero Data Retention**\n",
        "\n",
        "We disable Data Retention through the os.environ setting below. This allows Enterprises to operate in a Zero Data Retention environment with Deep Research. If Data Retention is _not_ an active constraint for you, then consider keeping it enabled so you can have automated tracability for your agent workflows and deep integration with other platform tools like evaluations and fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OWnnTNZJj0Wt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from agents import Agent, Runner, WebSearchTool, RunConfig, set_default_openai_client, HostedMCPTool\n",
        "from typing import List, Dict, Optional\n",
        "from pydantic import BaseModel\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Use env var for API key and set a long timeout\n",
        "client = AsyncOpenAI(api_key=\"\", timeout=600.0)\n",
        "set_default_openai_client(client)\n",
        "os.environ[\"OPENAI_AGENTS_DISABLE_TRACING\"] = \"1\" # Disable tracing for Zero Data Retention (ZDR) Organizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Omyb04nj0Wy"
      },
      "source": [
        "### Basic Deep Research Agent\n",
        "\n",
        "The Basic Research Agent performs Deep Research using the o4-mini-deep-research-alpha model. It has native WebSearch access to the public internet and streams its findings directly back into the notebook. In this case we are using the `o4-mini-deep-research-alpha` model, because it is faster than the full o3 deep research model, with acceptable intelligence.\n",
        "\n",
        "**Learning objective:**\n",
        "\n",
        "After this, you can run a single-agent research task and stream its progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c91rFNYWj0Wy",
        "outputId": "6f2e3bbe-f321-4a8e-b7df-6b6c5bade65a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Researching: Research the economic impact of semaglutide on global healthcare systems.\n",
            "\n",
            "--- switched to agent: Research Agent ---\n",
            "\n",
            "--- RESEARCHING ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error streaming response: Error code: 401 - {'error': {'message': 'Missing bearer or basic authentication in header', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
          ]
        },
        {
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Missing bearer or basic authentication in header', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result_stream.final_output\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Run the research and print the result\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m basic_research(\u001b[33m\"\u001b[39m\u001b[33mResearch the economic impact of semaglutide on global healthcare systems.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mbasic_research\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResearching: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m result_stream = Runner.run_streamed(\n\u001b[32m     13\u001b[39m     research_agent,\n\u001b[32m     14\u001b[39m     query\n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ev \u001b[38;5;129;01min\u001b[39;00m result_stream.stream_events():\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ev.type == \u001b[33m\"\u001b[39m\u001b[33magent_updated_stream_event\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- switched to agent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mev.new_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/result.py:215\u001b[39m, in \u001b[36mRunResultStreaming.stream_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m._cleanup_tasks()\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/run.py:723\u001b[39m, in \u001b[36mAgentRunner._start_streaming\u001b[39m\u001b[34m(cls, starting_input, streamed_result, starting_agent, max_turns, hooks, context_wrapper, run_config, previous_response_id, session)\u001b[39m\n\u001b[32m    712\u001b[39m     streamed_result._input_guardrails_task = asyncio.create_task(\n\u001b[32m    713\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails_with_queue(\n\u001b[32m    714\u001b[39m             starting_agent,\n\u001b[32m   (...)\u001b[39m\u001b[32m    720\u001b[39m         )\n\u001b[32m    721\u001b[39m     )\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn_streamed(\n\u001b[32m    724\u001b[39m         streamed_result,\n\u001b[32m    725\u001b[39m         current_agent,\n\u001b[32m    726\u001b[39m         hooks,\n\u001b[32m    727\u001b[39m         context_wrapper,\n\u001b[32m    728\u001b[39m         run_config,\n\u001b[32m    729\u001b[39m         should_run_agent_start_hooks,\n\u001b[32m    730\u001b[39m         tool_use_tracker,\n\u001b[32m    731\u001b[39m         all_tools,\n\u001b[32m    732\u001b[39m         previous_response_id,\n\u001b[32m    733\u001b[39m     )\n\u001b[32m    734\u001b[39m     should_run_agent_start_hooks = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    736\u001b[39m     streamed_result.raw_responses = streamed_result.raw_responses + [\n\u001b[32m    737\u001b[39m         turn_result.model_response\n\u001b[32m    738\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/run.py:867\u001b[39m, in \u001b[36mAgentRunner._run_single_turn_streamed\u001b[39m\u001b[34m(cls, streamed_result, agent, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, all_tools, previous_response_id)\u001b[39m\n\u001b[32m    864\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m streamed_result.new_items])\n\u001b[32m    866\u001b[39m \u001b[38;5;66;03m# 1. Stream the output events\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m model.stream_response(\n\u001b[32m    868\u001b[39m     system_prompt,\n\u001b[32m    869\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    870\u001b[39m     model_settings,\n\u001b[32m    871\u001b[39m     all_tools,\n\u001b[32m    872\u001b[39m     output_schema,\n\u001b[32m    873\u001b[39m     handoffs,\n\u001b[32m    874\u001b[39m     get_model_tracing_impl(\n\u001b[32m    875\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    876\u001b[39m     ),\n\u001b[32m    877\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    878\u001b[39m     prompt=prompt_config,\n\u001b[32m    879\u001b[39m ):\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, ResponseCompletedEvent):\n\u001b[32m    881\u001b[39m         usage = (\n\u001b[32m    882\u001b[39m             Usage(\n\u001b[32m    883\u001b[39m                 requests=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    891\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Usage()\n\u001b[32m    892\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/models/openai_responses.py:161\u001b[39m, in \u001b[36mOpenAIResponsesModel.stream_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, prompt)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m         stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m    162\u001b[39m             system_instructions,\n\u001b[32m    163\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    164\u001b[39m             model_settings,\n\u001b[32m    165\u001b[39m             tools,\n\u001b[32m    166\u001b[39m             output_schema,\n\u001b[32m    167\u001b[39m             handoffs,\n\u001b[32m    168\u001b[39m             previous_response_id,\n\u001b[32m    169\u001b[39m             stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    170\u001b[39m             prompt=prompt,\n\u001b[32m    171\u001b[39m         )\n\u001b[32m    173\u001b[39m         final_response: Response | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    175\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/models/openai_responses.py:267\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, stream, prompt)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     logger.debug(\n\u001b[32m    258\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling LLM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with input:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    259\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson.dumps(list_input,\u001b[38;5;250m \u001b[39mindent=\u001b[32m2\u001b[39m,\u001b[38;5;250m \u001b[39mensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrevious response id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_response_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    265\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    268\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(previous_response_id),\n\u001b[32m    269\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(system_instructions),\n\u001b[32m    270\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    271\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    272\u001b[39m     include=include,\n\u001b[32m    273\u001b[39m     tools=converted_tools.tools,\n\u001b[32m    274\u001b[39m     prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(prompt),\n\u001b[32m    275\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    276\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    277\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.truncation),\n\u001b[32m    278\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    279\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    280\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    281\u001b[39m     stream=stream,\n\u001b[32m    282\u001b[39m     extra_headers={**_HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    283\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    284\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    285\u001b[39m     text=response_format,\n\u001b[32m    286\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.store),\n\u001b[32m    287\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.reasoning),\n\u001b[32m    288\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    289\u001b[39m     **(model_settings.extra_args \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    290\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/resources/responses/responses.py:2000\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1967\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1968\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1969\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1998\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1999\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2000\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2001\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2002\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2003\u001b[39m             {\n\u001b[32m   2004\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2005\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2006\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2007\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2008\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2009\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2010\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2011\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2012\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2013\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2014\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2015\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2016\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2017\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2018\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2019\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2020\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2021\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2022\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2023\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2024\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2025\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2026\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2027\u001b[39m             },\n\u001b[32m   2028\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2029\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2030\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2031\u001b[39m         ),\n\u001b[32m   2032\u001b[39m         options=make_request_options(\n\u001b[32m   2033\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2034\u001b[39m         ),\n\u001b[32m   2035\u001b[39m         cast_to=Response,\n\u001b[32m   2036\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2037\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2038\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1791\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1779\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1786\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1787\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1788\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1789\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1790\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1591\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1588\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1590\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1591\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1593\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Missing bearer or basic authentication in header', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        }
      ],
      "source": [
        "# Define the research agent\n",
        "research_agent = Agent(\n",
        "    name=\"Research Agent\",\n",
        "    model=\"o4-mini-deep-research-2025-06-26\",\n",
        "    tools=[WebSearchTool()],\n",
        "    instructions=\"You perform deep empirical research based on the user's question.\"\n",
        ")\n",
        "\n",
        "# Async function to run the research and print streaming progress\n",
        "async def basic_research(query):\n",
        "    print(f\"Researching: {query}\")\n",
        "    result_stream = Runner.run_streamed(\n",
        "        research_agent,\n",
        "        query\n",
        "    )\n",
        "\n",
        "    async for ev in result_stream.stream_events():\n",
        "        if ev.type == \"agent_updated_stream_event\":\n",
        "            print(f\"\\n--- switched to agent: {ev.new_agent.name} ---\")\n",
        "            print(f\"\\n--- RESEARCHING ---\")\n",
        "        elif (\n",
        "            ev.type == \"raw_response_event\"\n",
        "            and hasattr(ev.data, \"item\")\n",
        "            and hasattr(ev.data.item, \"action\")\n",
        "        ):\n",
        "            action = ev.data.item.action or {}\n",
        "            if action.get(\"type\") == \"search\":\n",
        "                print(f\"[Web search] query={action.get('query')!r}\")\n",
        "\n",
        "    # streaming is complete → final_output is now populated\n",
        "    return result_stream.final_output\n",
        "\n",
        "# Run the research and print the result\n",
        "result = await basic_research(\"Research the economic impact of semaglutide on global healthcare systems.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4wk2sulj0Wy"
      },
      "source": [
        "### Multi-Agent Research with Clarification\n",
        "\n",
        "Multi-Agent Deep Research\n",
        "\n",
        "Consider how you might further improve the Research quality \"Deep Research\" produces. In this case, we are leveraging a multi-agent architecture to enrich the prompt with _more information_ about the users query and what we expect to see in the final research report, before submitting it to a deep research agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlZ6rxFn7C0d"
      },
      "source": [
        "\n",
        "## Sub-Agent Prompt enrichment\n",
        "\n",
        "The supporting Agent prompts are specifically designed to improve the quality of the final research output by providing structure and rigor to the users intial query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glkoOX6q6Ph9"
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────\n",
        "#  Prompts\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "CLARIFYING_AGENT_PROMPT =  \"\"\"\n",
        "    If the user hasn't specifically asked for research (unlikely), ask them what research they would like you to do.\n",
        "\n",
        "        GUIDELINES:\n",
        "        1. **Be concise while gathering all necessary information** Ask 2–3 clarifying questions to gather more context for research.\n",
        "        - Make sure to gather all the information needed to carry out the research task in a concise, well-structured manner. Use bullet points or numbered lists if appropriate for clarity. Don't ask for unnecessary information, or information that the user has already provided.\n",
        "        2. **Maintain a Friendly and Non-Condescending Tone**\n",
        "        - For example, instead of saying “I need a bit more detail on Y,” say, “Could you share more detail on Y?”\n",
        "        3. **Adhere to Safety Guidelines**\n",
        "        \"\"\"\n",
        "\n",
        "RESEARCH_INSTRUCTION_AGENT_PROMPT = \"\"\"\n",
        "\n",
        "        Based on the following guidelines, take the users query, and rewrite it into detailed research instructions. OUTPUT ONLY THE RESEARCH INSTRUCTIONS, NOTHING ELSE. Transfer to the research agent.\n",
        "\n",
        "        GUIDELINES:\n",
        "        1. **Maximize Specificity and Detail**\n",
        "        - Include all known user preferences and explicitly list key attributes or dimensions to consider.\n",
        "        - It is of utmost importance that all details from the user are included in the expanded prompt.\n",
        "\n",
        "        2. **Fill in Unstated But Necessary Dimensions as Open-Ended**\n",
        "        - If certain attributes are essential for a meaningful output but the user has not provided them, explicitly state that they are open-ended or default to “no specific constraint.”\n",
        "\n",
        "        3. **Avoid Unwarranted Assumptions**\n",
        "        - If the user has not provided a particular detail, do not invent one.\n",
        "        - Instead, state the lack of specification and guide the deep research model to treat it as flexible or accept all possible options.\n",
        "\n",
        "        4. **Use the First Person**\n",
        "        - Phrase the request from the perspective of the user.\n",
        "\n",
        "        5. **Tables**\n",
        "        - If you determine that including a table will help illustrate, organize, or enhance the information in your deep research output, you must explicitly request that the deep research model provide them.\n",
        "        Examples:\n",
        "        - Product Comparison (Consumer): When comparing different smartphone models, request a table listing each model’s features, price, and consumer ratings side-by-side.\n",
        "        - Project Tracking (Work): When outlining project deliverables, create a table showing tasks, deadlines, responsible team members, and status updates.\n",
        "        - Budget Planning (Consumer): When creating a personal or household budget, request a table detailing income sources, monthly expenses, and savings goals.\n",
        "        Competitor Analysis (Work): When evaluating competitor products, request a table with key metrics—such as market share, pricing, and main differentiators.\n",
        "\n",
        "        6. **Headers and Formatting**\n",
        "        - You should include the expected output format in the prompt.\n",
        "        - If the user is asking for content that would be best returned in a structured format (e.g. a report, plan, etc.), ask the Deep Research model to “Format as a report with the appropriate headers and formatting that ensures clarity and structure.”\n",
        "\n",
        "        7. **Language**\n",
        "        - If the user input is in a language other than English, tell the model to respond in this language, unless the user query explicitly asks for the response in a different language.\n",
        "\n",
        "        8. **Sources**\n",
        "        - If specific sources should be prioritized, specify them in the prompt.\n",
        "        - Prioritize Internal Knowledge. Only retrieve a single file once.\n",
        "        - For product and travel research, prefer linking directly to official or primary websites (e.g., official brand sites, manufacturer pages, or reputable e-commerce platforms like Amazon for user reviews) rather than aggregator sites or SEO-heavy blogs.\n",
        "        - For academic or scientific queries, prefer linking directly to the original paper or official journal publication rather than survey papers or secondary summaries.\n",
        "        - If the query is in a specific language, prioritize sources published in that language.\n",
        "\n",
        "        IMPORTANT: Ensure that the complete payload to this function is valid JSON\n",
        "        IMPORTANT: SPECIFY REQUIRED OUTPUT LANGUAGE IN THE PROMPT\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1dXCfxa6sf1"
      },
      "source": [
        "# Four-Agent Deep Research Pipeline\n",
        "\n",
        "1. **Triage Agent**  \n",
        "   - Inspects the user’s query  \n",
        "   - If context is missing, routes to the Clarifier Agent; otherwise routes to the Instruction Agent  \n",
        "\n",
        "2. **Clarifier Agent**  \n",
        "   - Asks follow-up questions  \n",
        "   - Waits for user (or mock) answers  \n",
        "\n",
        "3. **Instruction Builder Agent**  \n",
        "   - Converts the enriched input into a precise research brief  \n",
        "\n",
        "4. **Research Agent** (`o3-deep-research`)  \n",
        "   - Performs web-scale empirical research with `WebSearchTool`\n",
        "   - Performs a search against internal knowledge store using MCP, if there are relevant documents, the agent incorporates those relevant snippets in its reference material.   \n",
        "   - Streams intermediate events for transparency\n",
        "   - Outputs final Research Artifact (which we later parse)\n",
        "\n",
        "![Four‑Agent Diagram](https://raw.githubusercontent.com/openai/openai-cookbook/main/images/agent_dr.png)\n",
        "\n",
        "For more insight into _how_ the MCP server is build. [See this resource.](https://cookbook.openai.com/examples/deep_research_api/how_to_build_a_deep_research_mcp_server/readme )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-8WVGMBj0Wz"
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────\n",
        "# Structured outputs (needed only for Clarifying agent)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "class Clarifications(BaseModel):\n",
        "    questions: List[str]\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Agents\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "research_agent = Agent(\n",
        "    name=\"Research Agent\",\n",
        "    model=\"o3-deep-research-2025-06-26\",\n",
        "    instructions=\"Perform deep empirical research based on the user's instructions.\",\n",
        "    tools=[WebSearchTool(),\n",
        "           HostedMCPTool(\n",
        "            tool_config={\n",
        "                \"type\": \"mcp\",\n",
        "                \"server_label\": \"file_search\",\n",
        "                \"server_url\": \"https://<url>/sse\",\n",
        "                \"require_approval\": \"never\",\n",
        "            }\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "instruction_agent = Agent(\n",
        "    name=\"Research Instruction Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=RESEARCH_INSTRUCTION_AGENT_PROMPT,\n",
        "    handoffs=[research_agent],\n",
        ")\n",
        "\n",
        "clarifying_agent = Agent(\n",
        "    name=\"Clarifying Questions Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=CLARIFYING_AGENT_PROMPT,\n",
        "    output_type=Clarifications,\n",
        "    handoffs=[instruction_agent],\n",
        ")\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name=\"Triage Agent\",\n",
        "    instructions=(\n",
        "        \"Decide whether clarifications are required.\\n\"\n",
        "        \"• If yes → call transfer_to_clarifying_questions_agent\\n\"\n",
        "        \"• If no  → call transfer_to_research_instruction_agent\\n\"\n",
        "        \"Return exactly ONE function-call.\"\n",
        "    ),\n",
        "    handoffs=[clarifying_agent, instruction_agent],\n",
        ")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "#  Auto-clarify helper\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "async def basic_research(\n",
        "    query: str,\n",
        "    mock_answers: Optional[Dict[str, str]] = None,\n",
        "    verbose: bool = False,\n",
        "):\n",
        "    stream = Runner.run_streamed(\n",
        "        triage_agent,\n",
        "        query,\n",
        "        run_config=RunConfig(tracing_disabled=True),\n",
        "    )\n",
        "\n",
        "    async for ev in stream.stream_events():\n",
        "        if isinstance(getattr(ev, \"item\", None), Clarifications):\n",
        "            reply = []\n",
        "            for q in ev.item.questions:\n",
        "                ans = (mock_answers or {}).get(q, \"No preference.\")\n",
        "                reply.append(f\"**{q}**\\n{ans}\")\n",
        "            stream.send_user_message(\"\\n\\n\".join(reply))\n",
        "            continue\n",
        "        if verbose:\n",
        "            print(ev)\n",
        "\n",
        "    #return stream.final_output\n",
        "    return stream\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "#  Example run\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "result = await basic_research(\n",
        "    \"Research the economic impact of semaglutide on global healthcare systems.\",\n",
        "    mock_answers={},   # or provide canned answers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEVDHxRzjvJM"
      },
      "source": [
        "## Agent Interaction Flow\n",
        "\n",
        "Although provided natively through Agent SDK traces you may want to print human-readable high-level agent interaction flow with tool calls. Run print_agent_interaction to get a simplified readable sequence of agent steps, including: Agent name, Type of event (handoff, tool call, message output), Brief tool call info (tool name and arguments).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YZ_ibZIic_u",
        "outputId": "23c97975-94f2-47e0-ea0f-7475b46c4f6c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def parse_agent_interaction_flow(stream):\n",
        "    print(\"=== Agent Interaction Flow ===\")\n",
        "    count = 1\n",
        "\n",
        "    for item in stream.new_items:\n",
        "        # Agent name, fallback if missing\n",
        "        agent_name = getattr(item.agent, \"name\", \"Unknown Agent\") if hasattr(item, \"agent\") else \"Unknown Agent\"\n",
        "\n",
        "        if item.type == \"handoff_call_item\":\n",
        "            func_name = getattr(item.raw_item, \"name\", \"Unknown Function\")\n",
        "            print(f\"{count}. [{agent_name}] → Handoff Call: {func_name}\")\n",
        "            count += 1\n",
        "\n",
        "        elif item.type == \"handoff_output_item\":\n",
        "            print(f\"{count}. [{agent_name}] → Handoff Output\")\n",
        "            count += 1\n",
        "\n",
        "        elif item.type == \"mcp_list_tools_item\":\n",
        "            print(f\"{count}. [{agent_name}] → mcp_list_tools_item\")\n",
        "            count += 1\n",
        "\n",
        "        elif item.type == \"reasoning_item\":\n",
        "            print(f\"{count}. [{agent_name}] → Reasoning step\")\n",
        "            count += 1\n",
        "\n",
        "        elif item.type == \"tool_call_item\":\n",
        "            tool_name = getattr(item.raw_item, \"name\", None)\n",
        "\n",
        "            # Skip tool call if tool_name is missing or empty\n",
        "            if not isinstance(tool_name, str) or not tool_name.strip():\n",
        "                continue  # skip silently\n",
        "\n",
        "            tool_name = tool_name.strip()\n",
        "\n",
        "            args = getattr(item.raw_item, \"arguments\", None)\n",
        "            args_str = \"\"\n",
        "\n",
        "            if args:\n",
        "                try:\n",
        "                    parsed_args = json.loads(args)\n",
        "                    if parsed_args:\n",
        "                        \n",
        "                        args_str = json.dumps(parsed_args)\n",
        "                except Exception:\n",
        "                    if args.strip() and args.strip() != \"{}\":\n",
        "                        args_str = args.strip()\n",
        "\n",
        "            args_display = f\" with args {args_str}\" if args_str else \"\"\n",
        "\n",
        "            print(f\"{count}. [{agent_name}] → Tool Call: {tool_name}{args_display}\")\n",
        "            count += 1\n",
        "\n",
        "        elif item.type == \"message_output_item\":\n",
        "            print(f\"{count}. [{agent_name}] → Message Output\")\n",
        "            count += 1\n",
        "\n",
        "        else:\n",
        "            print(f\"{count}. [{agent_name}] → {item.type}\")\n",
        "            count += 1\n",
        "\n",
        "# Example usage:\n",
        "parse_agent_interaction_flow(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mZW3GT5kCOw"
      },
      "source": [
        "## Citations\n",
        "\n",
        "Below is a Python snippet to extract and print the URL citations related to the final output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCyfxC7siant",
        "outputId": "28539b23-00db-4dfa-902b-cd80c67ba765"
      },
      "outputs": [],
      "source": [
        "def print_final_output_citations(stream, preceding_chars=50):\n",
        "    # Iterate over new_items in reverse to find the last message_output_item(s)\n",
        "    for item in reversed(stream.new_items):\n",
        "        if item.type == \"message_output_item\":\n",
        "            for content in getattr(item.raw_item, 'content', []):\n",
        "                if not hasattr(content, 'annotations') or not hasattr(content, 'text'):\n",
        "                    continue\n",
        "                text = content.text\n",
        "                for ann in content.annotations:\n",
        "                    if getattr(ann, 'type', None) == 'url_citation':\n",
        "                        title = getattr(ann, 'title', '<no title>')\n",
        "                        url = getattr(ann, 'url', '<no url>')\n",
        "                        start = getattr(ann, 'start_index', None)\n",
        "                        end = getattr(ann, 'end_index', None)\n",
        "\n",
        "                        if start is not None and end is not None and isinstance(text, str):\n",
        "                            # Calculate preceding snippet start index safely\n",
        "                            pre_start = max(0, start - preceding_chars)\n",
        "                            preceding_text = text[pre_start:start].replace('\\n', ' ').strip()\n",
        "                            excerpt = text[start:end].replace('\\n', ' ').strip()\n",
        "                            print(\"# --------\")\n",
        "                            print(\"# MCP CITATION SAMPLE:\")\n",
        "                            print(f\"#   Title:       {title}\")\n",
        "                            print(f\"#   URL:         {url}\")\n",
        "                            print(f\"#   Location:    chars {start}–{end}\")\n",
        "                            print(f\"#   Preceding:   '{preceding_text}'\")\n",
        "                            print(f\"#   Excerpt:     '{excerpt}'\\n\")\n",
        "                        else:\n",
        "                            # fallback if no indices available\n",
        "                            print(f\"- {title}: {url}\")\n",
        "            break\n",
        "\n",
        "# Usage\n",
        "print_final_output_citations(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTeTcni5L-1s",
        "outputId": "eb442687-0530-4198-d778-b7d0dcf07df0"
      },
      "outputs": [],
      "source": [
        "## Deep Research Research Report\n",
        "\n",
        "print(result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UJcBbp9j0Wz"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "With the patterns in this notebook, you now have a foundation for building scalable, production-ready research workflows using OpenAI Deep Research Agents. The examples demonstrate not only how to orchestrate multi-agent pipelines and stream research progress, but also how to integrate web search and MCP for external knowledge access.\n",
        "\n",
        "By leveraging agentic workflows, you can move beyond simple Q&A to tackle complex, multi-step research tasks that require planning, synthesis, and tool use. The modular multi-agent design: triage, clarification, instruction, and research agents enables you to adapt these pipelines to a wide range of domains and use cases, from healthcare and finance to technical due diligence and market analysis.\n",
        "\n",
        "As the Deep Research API and Agents SDK continue to evolve, these patterns will help you stay at the forefront of automated, data-backed research. Whether you’re building internal knowledge tools, automating competitive intelligence, or supporting expert analysts, these workflows provide a strong, extensible starting point.\n",
        "\n",
        "**Happy researching!**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
