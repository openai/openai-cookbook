{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv8-mnmnj0Wp"
      },
      "source": [
        "# Deep Research Agents Cookbook\n",
        "\n",
        "This cookbook demonstrates how to build Agentic research workflows using the OpenAI Deep Research API and the OpenAI [Agents SDK](https://openai.github.io/openai-agents-python/). It is a continuation of [a fundamentals cookbook](https://cookbook.openai.com/examples/deep_research_api/introduction_to_deep_research_api), if you have not already familiarized yourself with that content, please consider doing so.\n",
        "\n",
        "You’ll learn how to orchestrate single and multi-agent pipelines, enrich user queries to maximize output quality, stream research progress, integrate web search and [MCP for internal file search](https://cookbook.openai.com/examples/deep_research_api/how_to_build_a_deep_research_mcp_server/readme), and architect a robust research application.\n",
        "\n",
        "Consider using Deep Research Agents for tasks that require planning, synthesis, tool use, or multi-step reasoning. Do not use Deep Research for trivial fact lookups, simple Q&A, or short-form chat, a vanilla openai.responsesAPI would be faster and cheaper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6gLS5aVj0Wr"
      },
      "source": [
        "### Prerequisites\n",
        "* OpenAI API key (set as OPENAI_API_KEY in your environment)\n",
        "* Agents SDK and OpenAI Python SDK\n",
        "\n",
        "### Setup\n",
        "*Install dependencies*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWE9uQq4j0Ws",
        "outputId": "99c15803-0506-4464-d624-a31b5bc809a4"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade \"openai>=1.88\" \"openai-agents>=0.0.19\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9lWqn_Wj0Wt"
      },
      "source": [
        "### Import libraries and configure client\n",
        "\n",
        "**Zero Data Retention**\n",
        "\n",
        "We disable Data Retention through the os.environ setting below. This allows Enterprises to operate in a Zero Data Retention environment with Deep Research. If Data Retention is _not_ an active constraint for you, then consider keeping it enabled so you can have automated tracability for your agent workflows and deep integration with other platform tools like evaluations and fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OWnnTNZJj0Wt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from agents import Agent, Runner, WebSearchTool, RunConfig, set_default_openai_client, HostedMCPTool\n",
        "from typing import List, Dict, Optional\n",
        "from pydantic import BaseModel\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Use env var for API key and set a long timeout\n",
        "client = AsyncOpenAI(api_key=\"\", timeout=600.0)\n",
        "set_default_openai_client(client)\n",
        "os.environ[\"OPENAI_AGENTS_DISABLE_TRACING\"] = \"1\" # Disable tracing for Zero Data Retention (ZDR) Organizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Omyb04nj0Wy"
      },
      "source": [
        "### Basic Deep Research Agent\n",
        "\n",
        "The Basic Research Agent performs Deep Research using the o4-mini-deep-research-alpha model. It has native WebSearch access to the public internet and streams its findings directly back into the notebook. In this case we are using the `o4-mini-deep-research-alpha` model, because it is faster than the full o3 deep research model, with acceptable intelligence.\n",
        "\n",
        "**Learning objective:**\n",
        "\n",
        "After this, you can run a single-agent research task and stream its progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c91rFNYWj0Wy",
        "outputId": "6f2e3bbe-f321-4a8e-b7df-6b6c5bade65a"
      },
      "outputs": [],
      "source": [
        "# Define the research agent\n",
        "research_agent = Agent(\n",
        "    name=\"Research Agent\",\n",
        "    model=\"o4-mini-deep-research-2025-06-26\",\n",
        "    tools=[WebSearchTool()],\n",
        "    instructions=\"You perform deep empirical research based on the user's question.\"\n",
        ")\n",
        "\n",
        "# Async function to run the research and print streaming progress\n",
        "async def basic_research(query):\n",
        "    print(f\"Researching: {query}\")\n",
        "    result_stream = Runner.run_streamed(\n",
        "        research_agent,\n",
        "        query\n",
        "    )\n",
        "\n",
        "    async for ev in result_stream.stream_events():\n",
        "        if ev.type == \"agent_updated_stream_event\":\n",
        "            print(f\"\\n--- switched to agent: {ev.new_agent.name} ---\")\n",
        "            print(f\"\\n--- RESEARCHING ---\")\n",
        "        elif (\n",
        "            ev.type == \"raw_response_event\"\n",
        "            and hasattr(ev.data, \"item\")\n",
        "            and hasattr(ev.data.item, \"action\")\n",
        "        ):\n",
        "            action = ev.data.item.action or {}\n",
        "            if action.get(\"type\") == \"search\":\n",
        "                print(f\"[Web search] query={action.get('query')!r}\")\n",
        "\n",
        "    # streaming is complete → final_output is now populated\n",
        "    return result_stream.final_output\n",
        "\n",
        "# Run the research and print the result\n",
        "result = await basic_research(\"Research the economic impact of semaglutide on global healthcare systems.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4wk2sulj0Wy"
      },
      "source": [
        "### Multi-Agent Research with Clarification\n",
        "\n",
        "Multi-Agent Deep Research\n",
        "\n",
        "Consider how you might further improve the Research quality \"Deep Research\" produces. In this case, we are leveraging a multi-agent architecture to enrich the prompt with _more information_ about the users query and what we expect to see in the final research report, before submitting it to a deep research agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlZ6rxFn7C0d"
      },
      "source": [
        "\n",
        "## Sub-Agent Prompt enrichment\n",
        "\n",
        "The supporting Agent prompts are specifically designed to improve the quality of the final research output by providing structure and rigor to the users intial query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "glkoOX6q6Ph9"
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────\n",
        "#  Prompts\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "CLARIFYING_AGENT_PROMPT =  \"\"\"\n",
        "    If the user hasn't specifically asked for research (unlikely), ask them what research they would like you to do.\n",
        "\n",
        "        GUIDELINES:\n",
        "        1. **Be concise while gathering all necessary information** Ask 2–3 clarifying questions to gather more context for research.\n",
        "        - Make sure to gather all the information needed to carry out the research task in a concise, well-structured manner. Use bullet points or numbered lists if appropriate for clarity. Don't ask for unnecessary information, or information that the user has already provided.\n",
        "        2. **Maintain a Friendly and Non-Condescending Tone**\n",
        "        - For example, instead of saying “I need a bit more detail on Y,” say, “Could you share more detail on Y?”\n",
        "        3. **Adhere to Safety Guidelines**\n",
        "        \"\"\"\n",
        "\n",
        "RESEARCH_INSTRUCTION_AGENT_PROMPT = \"\"\"\n",
        "\n",
        "        Based on the following guidelines, take the users query, and rewrite it into detailed research instructions. OUTPUT ONLY THE RESEARCH INSTRUCTIONS, NOTHING ELSE. Transfer to the research agent.\n",
        "\n",
        "        GUIDELINES:\n",
        "        1. **Maximize Specificity and Detail**\n",
        "        - Include all known user preferences and explicitly list key attributes or dimensions to consider.\n",
        "        - It is of utmost importance that all details from the user are included in the expanded prompt.\n",
        "\n",
        "        2. **Fill in Unstated But Necessary Dimensions as Open-Ended**\n",
        "        - If certain attributes are essential for a meaningful output but the user has not provided them, explicitly state that they are open-ended or default to “no specific constraint.”\n",
        "\n",
        "        3. **Avoid Unwarranted Assumptions**\n",
        "        - If the user has not provided a particular detail, do not invent one.\n",
        "        - Instead, state the lack of specification and guide the deep research model to treat it as flexible or accept all possible options.\n",
        "\n",
        "        4. **Use the First Person**\n",
        "        - Phrase the request from the perspective of the user.\n",
        "\n",
        "        5. **Tables**\n",
        "        - If you determine that including a table will help illustrate, organize, or enhance the information in your deep research output, you must explicitly request that the deep research model provide them.\n",
        "        Examples:\n",
        "        - Product Comparison (Consumer): When comparing different smartphone models, request a table listing each model’s features, price, and consumer ratings side-by-side.\n",
        "        - Project Tracking (Work): When outlining project deliverables, create a table showing tasks, deadlines, responsible team members, and status updates.\n",
        "        - Budget Planning (Consumer): When creating a personal or household budget, request a table detailing income sources, monthly expenses, and savings goals.\n",
        "        Competitor Analysis (Work): When evaluating competitor products, request a table with key metrics—such as market share, pricing, and main differentiators.\n",
        "\n",
        "        6. **Headers and Formatting**\n",
        "        - You should include the expected output format in the prompt.\n",
        "        - If the user is asking for content that would be best returned in a structured format (e.g. a report, plan, etc.), ask the Deep Research model to “Format as a report with the appropriate headers and formatting that ensures clarity and structure.”\n",
        "\n",
        "        7. **Language**\n",
        "        - If the user input is in a language other than English, tell the model to respond in this language, unless the user query explicitly asks for the response in a different language.\n",
        "\n",
        "        8. **Sources**\n",
        "        - If specific sources should be prioritized, specify them in the prompt.\n",
        "        - Prioritize Internal Knowledge. Only retrieve a single file once.\n",
        "        - For product and travel research, prefer linking directly to official or primary websites (e.g., official brand sites, manufacturer pages, or reputable e-commerce platforms like Amazon for user reviews) rather than aggregator sites or SEO-heavy blogs.\n",
        "        - For academic or scientific queries, prefer linking directly to the original paper or official journal publication rather than survey papers or secondary summaries.\n",
        "        - If the query is in a specific language, prioritize sources published in that language.\n",
        "\n",
        "        IMPORTANT: Ensure that the complete payload to this function is valid JSON\n",
        "        IMPORTANT: SPECIFY REQUIRED OUTPUT LANGUAGE IN THE PROMPT\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1dXCfxa6sf1"
      },
      "source": [
        "# Four-Agent Deep Research Pipeline\n",
        "\n",
        "1. **Triage Agent**  \n",
        "   - Inspects the user’s query  \n",
        "   - If context is missing, routes to the Clarifier Agent; otherwise routes to the Instruction Agent  \n",
        "\n",
        "2. **Clarifier Agent**  \n",
        "   - Asks follow-up questions  \n",
        "   - Waits for user (or mock) answers  \n",
        "\n",
        "3. **Instruction Builder Agent**  \n",
        "   - Converts the enriched input into a precise research brief  \n",
        "\n",
        "4. **Research Agent** (`o3-deep-research`)  \n",
        "   - Performs web-scale empirical research with `WebSearchTool`\n",
        "   - Performs a search against internal knowledge store using MCP, if there are relevant documents, the agent incorporates those relevant snippets in its reference material.   \n",
        "   - Streams intermediate events for transparency\n",
        "   - Outputs final Research Artifact (which we later parse)\n",
        "\n",
        "![../../images/agents_dr.png](../../../images/agent_dr.png)\n",
        "\n",
        "For more insight into _how_ the MCP server is build. [See this resource.](https://cookbook.openai.com/examples/deep_research_api/how_to_build_a_deep_research_mcp_server/readme )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y-8WVGMBj0Wz"
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────\n",
        "# Structured outputs (needed only for Clarifying agent)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "class Clarifications(BaseModel):\n",
        "    questions: List[str]\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# Agents\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "research_agent = Agent(\n",
        "    name=\"Research Agent\",\n",
        "    model=\"o3-deep-research-2025-06-26\",\n",
        "    instructions=\"Perform deep empirical research based on the user's instructions.\",\n",
        "    tools=[WebSearchTool(),\n",
        "           HostedMCPTool(\n",
        "            tool_config={\n",
        "                \"type\": \"mcp\",\n",
        "                \"server_label\": \"file_search\",\n",
        "                \"server_url\": \"https://<url>/sse\",\n",
        "                \"require_approval\": \"never\",\n",
        "            }\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "instruction_agent = Agent(\n",
        "    name=\"Research Instruction Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=RESEARCH_INSTRUCTION_AGENT_PROMPT,\n",
        "    handoffs=[research_agent],\n",
        ")\n",
        "\n",
        "clarifying_agent = Agent(\n",
        "    name=\"Clarifying Questions Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=CLARIFYING_AGENT_PROMPT,\n",
        "    output_type=Clarifications,\n",
        "    handoffs=[instruction_agent],\n",
        ")\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name=\"Triage Agent\",\n",
        "    instructions=(\n",
        "        \"Decide whether clarifications are required.\\n\"\n",
        "        \"• If yes → call transfer_to_clarifying_questions_agent\\n\"\n",
        "        \"• If no  → call transfer_to_research_instruction_agent\\n\"\n",
        "        \"Return exactly ONE function-call.\"\n",
        "    ),\n",
        "    handoffs=[clarifying_agent, instruction_agent],\n",
        ")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "#  Auto-clarify helper\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "async def basic_research(\n",
        "    query: str,\n",
        "    mock_answers: Optional[Dict[str, str]] = None,\n",
        "    verbose: bool = False,\n",
        "):\n",
        "    stream = Runner.run_streamed(\n",
        "        triage_agent,\n",
        "        query,\n",
        "        run_config=RunConfig(tracing_disabled=True),\n",
        "    )\n",
        "\n",
        "    async for ev in stream.stream_events():\n",
        "        if isinstance(getattr(ev, \"item\", None), Clarifications):\n",
        "            reply = []\n",
        "            for q in ev.item.questions:\n",
        "                ans = (mock_answers or {}).get(q, \"No preference.\")\n",
        "                reply.append(f\"**{q}**\\n{ans}\")\n",
        "            stream.send_user_message(\"\\n\\n\".join(reply))\n",
        "            continue\n",
        "        if verbose:\n",
        "            print(ev)\n",
        "\n",
        "    #return stream.final_output\n",
        "    return stream\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "#  Example run\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "result = await basic_research(\n",
        "    \"Research the economic impact of semaglutide on global healthcare systems.\",\n",
        "    mock_answers={},   # or provide canned answers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEVDHxRzjvJM"
      },
      "source": [
        "## Agent Interaction Flow\n",
        "\n",
        "Although provided natively through Agent SDK traces you may want to print human-readable high-level agent interaction flow with tool calls. Run print_agent_interaction to get a simplified readable sequence of agent steps, including: Agent name, Type of event (handoff, tool call, message output), Brief tool call info (tool name and arguments).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YZ_ibZIic_u",
        "outputId": "23c97975-94f2-47e0-ea0f-7475b46c4f6c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def parse_agent_interaction_flow(stream):\n",
        "    print(\"=== Agent Interaction Flow ===\")\n",
        "    count = 1\n",
        "\n",
        "    for item in stream.new_items:\n",
        "        # Agent name, fallback if missing\n",
        "        agent_name = getattr(item.agent, \"name\", \"Unknown Agent\") if hasattr(item, \"agent\") else \"Unknown Agent\"\n",
        "\n",
        "        if item.type == \"handoff_call_item\":\n",
        "            func_name = getattr(item.raw_item, \"name\", \"Unknown Function\")\n",
        "            print(f\"{count}. [{agent_name}] → Handoff Call: {func_name}\")\n",
        "            count += 1\n",
        "\n",
        "        elif item.type == \"handoff_output_item\":\n",
        "            print(f\"{count}. [{agent_name}] → Handoff Output\")\n",
        "            count += 1\n",
        "\n",
        "        elif item.type == \"mcp_list_tools_item\":\n",
        "            print(f\"{count}. [{agent_name}] → mcp_list_tools_item\")\n",
        "            count += 1\n",
        "\n",
        "        elif item.type == \"reasoning_item\":\n",
        "            print(f\"{count}. [{agent_name}] → Reasoning step\")\n",
        "            count += 1\n",
        "\n",
        "        elif item.type == \"tool_call_item\":\n",
        "            tool_name = getattr(item.raw_item, \"name\", None)\n",
        "\n",
        "            # Skip tool call if tool_name is missing or empty\n",
        "            if not isinstance(tool_name, str) or not tool_name.strip():\n",
        "                continue  # skip silently\n",
        "\n",
        "            tool_name = tool_name.strip()\n",
        "\n",
        "            args = getattr(item.raw_item, \"arguments\", None)\n",
        "            args_str = \"\"\n",
        "\n",
        "            if args:\n",
        "                try:\n",
        "                    parsed_args = json.loads(args)\n",
        "                    if parsed_args:\n",
        "                        args_str = json.dumps(parsed_args)\n",
        "                except Exception:\n",
        "                    if args.strip() and args.strip() != \"{}\":\n",
        "                        args_str = args.strip()\n",
        "\n",
        "            args_display = f\" with args {args_str}\" if args_str else \"\"\n",
        "\n",
        "            print(f\"{count}. [{agent_name}] → Tool Call: {tool_name}{args_display}\")\n",
        "            count += 1\n",
        "\n",
        "        elif item.type == \"message_output_item\":\n",
        "            print(f\"{count}. [{agent_name}] → Message Output\")\n",
        "            count += 1\n",
        "\n",
        "        else:\n",
        "            print(f\"{count}. [{agent_name}] → {item.type}\")\n",
        "            count += 1\n",
        "\n",
        "# Example usage:\n",
        "parse_agent_interaction_flow(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mZW3GT5kCOw"
      },
      "source": [
        "## Citations\n",
        "\n",
        "Below is a Python snippet to extract and print the URL citations related to the final output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCyfxC7siant",
        "outputId": "28539b23-00db-4dfa-902b-cd80c67ba765"
      },
      "outputs": [],
      "source": [
        "def print_final_output_citations(stream, preceding_chars=50):\n",
        "    # Iterate over new_items in reverse to find the last message_output_item(s)\n",
        "    for item in reversed(stream.new_items):\n",
        "        if item.type == \"message_output_item\":\n",
        "            for content in getattr(item.raw_item, 'content', []):\n",
        "                if not hasattr(content, 'annotations') or not hasattr(content, 'text'):\n",
        "                    continue\n",
        "                text = content.text\n",
        "                for ann in content.annotations:\n",
        "                    if getattr(ann, 'type', None) == 'url_citation':\n",
        "                        title = getattr(ann, 'title', '<no title>')\n",
        "                        url = getattr(ann, 'url', '<no url>')\n",
        "                        start = getattr(ann, 'start_index', None)\n",
        "                        end = getattr(ann, 'end_index', None)\n",
        "\n",
        "                        if start is not None and end is not None and isinstance(text, str):\n",
        "                            # Calculate preceding snippet start index safely\n",
        "                            pre_start = max(0, start - preceding_chars)\n",
        "                            preceding_text = text[pre_start:start].replace('\\n', ' ').strip()\n",
        "                            excerpt = text[start:end].replace('\\n', ' ').strip()\n",
        "                            print(\"# --------\")\n",
        "                            print(\"# MCP CITATION SAMPLE:\")\n",
        "                            print(f\"#   Title:       {title}\")\n",
        "                            print(f\"#   URL:         {url}\")\n",
        "                            print(f\"#   Location:    chars {start}–{end}\")\n",
        "                            print(f\"#   Preceding:   '{preceding_text}'\")\n",
        "                            print(f\"#   Excerpt:     '{excerpt}'\\n\")\n",
        "                        else:\n",
        "                            # fallback if no indices available\n",
        "                            print(f\"- {title}: {url}\")\n",
        "            break\n",
        "\n",
        "# Usage\n",
        "print_final_output_citations(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTeTcni5L-1s",
        "outputId": "eb442687-0530-4198-d778-b7d0dcf07df0"
      },
      "outputs": [],
      "source": [
        "## Deep Research Research Report\n",
        "\n",
        "print(result.final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UJcBbp9j0Wz"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "With the patterns in this notebook, you now have a foundation for building scalable, production-ready research workflows using OpenAI Deep Research Agents. The examples demonstrate not only how to orchestrate multi-agent pipelines and stream research progress, but also how to integrate web search and MCP for external knowledge access.\n",
        "\n",
        "By leveraging agentic workflows, you can move beyond simple Q&A to tackle complex, multi-step research tasks that require planning, synthesis, and tool use. The modular multi-agent design: triage, clarification, instruction, and research agents enables you to adapt these pipelines to a wide range of domains and use cases, from healthcare and finance to technical due diligence and market analysis.\n",
        "\n",
        "As the Deep Research API and Agents SDK continue to evolve, these patterns will help you stay at the forefront of automated, data-backed research. Whether you’re building internal knowledge tools, automating competitive intelligence, or supporting expert analysts, these workflows provide a strong, extensible starting point.\n",
        "\n",
        "**Happy researching!**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
