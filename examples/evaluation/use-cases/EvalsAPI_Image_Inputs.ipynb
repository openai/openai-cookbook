{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals API: Image Inputs\n",
    "\n",
    "OpenAIâ€™s Evals API now supports image inputs, in its step toward multimodal functionality! API users can use OpenAI's Evals API to evaluate their image use cases to see how their LLM integration is performing and improve it.\n",
    "\n",
    "In this cookbook, we'll walk through an image example with the Evals API. More specifically, we will use Evals API to evaluate model-generated responses to an image and its corresponding prompt, using **sampling** to generate model responses and **model grading** (LLM as a Judge) to score those model responses against the image and reference answer.\n",
    "\n",
    "Based on your use case, you might only need the sampling functionality or the model grader, and you can revise what you pass in during the eval and run creation to fit your needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this example, we will use the [VibeEval](https://huggingface.co/datasets/RekaAI/VibeEval) dataset that's hosted on Hugging Face. It contains a collection of image, prompt, and reference answer data. First, we load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/homebrew/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.10/site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/homebrew/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /Users/daisyshe/Library/Python/3.10/lib/python/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/daisyshe/Library/Python/3.10/lib/python/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/daisyshe/Library/Python/3.10/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/daisyshe/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"RekaAI/VibeEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the relevant fields and put it in a json-like format to pass in as a data source in the Evals API. Input image data can be in the form of a web URL or a base64 encoded string. Here, we use the provided web URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_data_source = []\n",
    "\n",
    "# select the first 5 examples in the dataset to use for this cookbook\n",
    "for example in dataset[\"test\"].select(range(5)):\n",
    "    evals_data_source.append({\n",
    "        \"item\": {\n",
    "            \"media_url\": example[\"media_url\"], # image web URL\n",
    "            \"reference\": example[\"reference\"], # reference answer\n",
    "            \"prompt\": example[\"prompt\"] # prompt\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print the data source list, each item should be of a similar form to:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"item\": {\n",
    "    \"media_url\": \"https://storage.googleapis.com/reka-annotate.appspot.com/vibe-eval/difficulty-normal_food1_7e5c2cb9c8200d70.jpg\"\n",
    "    \"reference\": \"This appears to be a classic Margherita pizza, which has the following ingredients...\"\n",
    "    \"prompt\": \"What ingredients do I need to make this?\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evals Structure\n",
    "\n",
    "Now that we have our data source and task, we will create our evals. For the evals API docs, visit [API docs](https://platform.openai.com/docs/evals/overview).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/homebrew/lib/python3.10/site-packages (1.95.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/lib/python3.10/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/daisyshe/Library/Python/3.10/lib/python/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/daisyshe/Library/Python/3.10/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=\"https://api.openai.com/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evals have two parts, the \"Eval\" and the \"Run\". In the \"Eval\", we define the expected structure of the data and the testing criteria (grader). Based on the data that we have compiled, our data source config is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"item_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"media_url\": { \"type\": \"string\" },\n",
    "          \"reference\": { \"type\": \"string\" },\n",
    "          \"prompt\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"media_url\", \"reference\", \"prompt\"]\n",
    "      },\n",
    "    \"include_sample_schema\": True, # enables sampling\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our testing criteria, we set up our grader config. In this example, it is a model grader that takes in an image, reference answer, and sampled model response (in the `sample` namespace), and then outputs a score between 0 and 1 based on how closely the model response matches the reference answer and its general suitability for the conversation. For more info on model graders, visit [API docs](hhttps://platform.openai.com/docs/api-reference/graders). \n",
    "\n",
    "Getting the both the data and the grader right are key for an effective evaluation. So, you will likely want to iteratively refine the prompts for your graders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The image url field / templating need to be placed in an input image object to be interpreted as an image. Otherwise, the image will be interpreted as a text string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_config = {\n",
    "\t    \"type\": \"score_model\",\n",
    "        \"name\": \"Score Model Grader\",\n",
    "        \"input\":[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "\t\t        \"content\": \"You are an expert grader. Judge how well the model response suits the image and prompt as well as matches the meaniing of the reference answer. Output a score of 1 if great. If it's somewhat compatible, output a score around 0.5. Otherwise, give a score of 0.\"\n",
    "\t        },\n",
    "\t        {\n",
    "\t\t        \"role\": \"user\",\n",
    "\t\t        \"content\": [{ \"type\": \"input_text\", \"text\": \"Prompt: {{ item.prompt }}.\"},\n",
    "\t\t\t\t\t\t\t{ \"type\": \"input_image\", \"image_url\": \"{{ item.media_url }}\", \"detail\": \"auto\" },\n",
    "\t\t\t\t\t\t\t{ \"type\": \"input_text\", \"text\": \"Reference answer: {{ item.reference }}. Model response: {{ sample.output_text }}.\"}\n",
    "\t\t\t\t]\n",
    "\t        }\n",
    "\t\t],\n",
    "\t\t\"pass_threshold\": 0.9,\n",
    "\t    \"range\": [0, 1],\n",
    "\t    \"model\": \"o4-mini\" # model for grading; check that the model you use supports image inputs\n",
    "\t}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the eval object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_object = client.evals.create(\n",
    "        name=\"Image Grading\",\n",
    "        data_source_config=data_source_config,\n",
    "        testing_criteria=[grader_config],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the run, we pass in the eval object id and the data source (i.e., the data we compiled earlier) in addition to the chat message trajectory we'd like for sampling to get the model response. While we won't dive into it in this cookbook, EvalsAPI also supports stored completions containing images as a data source. \n",
    "\n",
    "Here's the sampling message trajectory we'll use for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"type\": \"message\",\n",
    "    \"content\": {\n",
    "        \"type\": \"input_text\",\n",
    "        \"text\": \"{{ item.prompt }}\"\n",
    "      }\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"type\": \"message\",\n",
    "    \"content\": {\n",
    "        \"type\": \"input_image\",\n",
    "        \"image_url\": \"{{ item.media_url }}\",\n",
    "        \"detail\": \"auto\"\n",
    "    }\n",
    "  }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_run = client.evals.runs.create(\n",
    "        name=\"Image Input Eval Run\",\n",
    "        eval_id=eval_object.id,\n",
    "        data_source={\n",
    "            \"type\": \"responses\", # sample using responses API\n",
    "            \"source\": {\n",
    "                \"type\": \"file_content\",\n",
    "                \"content\": evals_data_source\n",
    "            },\n",
    "            \"model\": \"gpt-4o-mini\", # model used to generate the response; check that the model you use supports image inputs\n",
    "            \"input_messages\": {\n",
    "                \"type\": \"template\", \n",
    "                \"template\": sampling_messages}\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the run finishes, we can take a look at the result. You can also check in your org's OpenAI evals dashboard to see the progress and results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>reference</th>\n",
       "      <th>model_response</th>\n",
       "      <th>grading_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please provide latex code to replicate this table</td>\n",
       "      <td>Below is the latex code for your table:\\n```te...</td>\n",
       "      <td>Here is the LaTeX code to replicate the table ...</td>\n",
       "      <td>{\"steps\":[{\"description\":\"Check if the modelâ€™s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What ingredients do I need to make this?</td>\n",
       "      <td>This appears to be a classic Margherita pizza,...</td>\n",
       "      <td>To make a classic Margherita pizza like the on...</td>\n",
       "      <td>{\"steps\":[{\"description\":\"Compare the model re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is this safe for a vegan to eat?</td>\n",
       "      <td>Based on the image, this dish appears to be a ...</td>\n",
       "      <td>To determine if this dish is safe for a vegan ...</td>\n",
       "      <td>{\"steps\":[{\"description\":\"The reference answer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where was this taken?</td>\n",
       "      <td>This image is of the seafront in San SebastiÃ¡n...</td>\n",
       "      <td>I can't determine the exact location of the im...</td>\n",
       "      <td>{\"steps\":[{\"description\":\"Compare model respon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the man in the picture doing?</td>\n",
       "      <td>The man on the postcard is playing bagpipes, w...</td>\n",
       "      <td>The man in the picture is playing the bagpipes...</td>\n",
       "      <td>{\"steps\":[{\"description\":\"Compare the model re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Please provide latex code to replicate this table   \n",
       "1           What ingredients do I need to make this?   \n",
       "2                   Is this safe for a vegan to eat?   \n",
       "3                              Where was this taken?   \n",
       "4              What is the man in the picture doing?   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Below is the latex code for your table:\\n```te...   \n",
       "1  This appears to be a classic Margherita pizza,...   \n",
       "2  Based on the image, this dish appears to be a ...   \n",
       "3  This image is of the seafront in San SebastiÃ¡n...   \n",
       "4  The man on the postcard is playing bagpipes, w...   \n",
       "\n",
       "                                      model_response  \\\n",
       "0  Here is the LaTeX code to replicate the table ...   \n",
       "1  To make a classic Margherita pizza like the on...   \n",
       "2  To determine if this dish is safe for a vegan ...   \n",
       "3  I can't determine the exact location of the im...   \n",
       "4  The man in the picture is playing the bagpipes...   \n",
       "\n",
       "                                     grading_results  \n",
       "0  {\"steps\":[{\"description\":\"Check if the modelâ€™s...  \n",
       "1  {\"steps\":[{\"description\":\"Compare the model re...  \n",
       "2  {\"steps\":[{\"description\":\"The reference answer...  \n",
       "3  {\"steps\":[{\"description\":\"Compare model respon...  \n",
       "4  {\"steps\":[{\"description\":\"Compare the model re...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "while True:\n",
    "    run = client.evals.runs.retrieve(run_id=eval_run.id, eval_id=eval_object.id)\n",
    "    if run.status == \"completed\" or run.status == \"failed\": # check if the run is finished\n",
    "        output_items = client.evals.runs.output_items.list(\n",
    "            run_id=run.id, eval_id=eval_object.id\n",
    "        )\n",
    "        df = pd.DataFrame({\n",
    "            \"prompt\": [item.datasource_item[\"prompt\"]for item in output_items],\n",
    "            \"reference\": [item.datasource_item[\"reference\"] for item in output_items],\n",
    "            \"model_response\": [item.sample.output[0].content for item in output_items],\n",
    "            \"grading_results\": [item.results[0][\"sample\"][\"output\"][0][\"content\"]\n",
    "                                for item in output_items]\n",
    "        })\n",
    "        display(df)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the full output item, such as for the pizza ingredients image, we can do the following. The structure of the output item is specified in the API docs [here](https://platform.openai.com/docs/api-reference/evals/run-output-item-object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"outputitem_68768c0f7658819187d4f128c2e0ff8c\",\n",
      "  \"created_at\": 1752599567,\n",
      "  \"datasource_item\": {\n",
      "    \"prompt\": \"What ingredients do I need to make this?\",\n",
      "    \"media_url\": \"https://storage.googleapis.com/reka-annotate.appspot.com/vibe-eval/difficulty-normal_food1_7e5c2cb9c8200d70.jpg\",\n",
      "    \"reference\": \"This appears to be a classic Margherita pizza, which has the following ingredients:\\n\\n- Pizza Dough: You'll need yeast, flour, salt, and water to make the dough. A simple recipe is 500g of flour, 1 tsp of salt, 1 tbsp of sugar, and about 300ml warm water.\\n\\n- Tomatoes: Fresh or canned San Marzano tomatoes are traditionally used for their sweet flavor. If using fresh tomatoes, you can blend them into a sauce.\\n\\n- Mozzarella Cheese: Traditionally mozzarella di bufala campana D.O.P., but Fior di Latte or other fresh mozzarella work well too.\\n\\n- Basil Leaves: Fresh basil leaves add a burst of flavor.\\n\\n- Olive Oil: Extra virgin olive oil is drizzled over the pizza before baking for added flavor.\\n\\n- Salt & Pepper\\n\\nYou would also need a pizza stone or baking sheet preheated in an oven set to around 475\\u00b0F (246\\u00b0C). Once your dough is prepared and shaped into a circle (use parchment paper if it's homemade), spread your tomato sauce on top leaving some space at the edge. Add dollops of cheese on top then gently press them down with your fingers. Drizzle with olive oil and season with salt & pepper. Finally add your basil leaves before placing it in the oven to bake until the crust is golden brown and bubbly - about 10 minutes depending on thickness.\"\n",
      "  },\n",
      "  \"datasource_item_id\": 2,\n",
      "  \"eval_id\": \"eval_687689442f7c8191aa614761671be57c\",\n",
      "  \"object\": \"eval.run.output_item\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"name\": \"Score Model Grader-3510e5e4-b0f8-4cfb-a051-f5440152ae1e\",\n",
      "      \"sample\": {\n",
      "        \"input\": [\n",
      "          {\n",
      "            \"role\": \"system\",\n",
      "            \"content\": \"You are an expert grader. Judge how well the model response suits the image and prompt as well as matches the meaniing of the reference answer. Output a score of 1 if great. If it's somewhat compatible, output a score around 0.5. Otherwise, give a score of 0.\"\n",
      "          },\n",
      "          {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": \"Prompt: What ingredients do I need to make this?. <image>https://storage.googleapis.com/reka-annotate.appspot.com/vibe-eval/difficulty-normal_food1_7e5c2cb9c8200d70.jpg</image> Reference answer: This appears to be a classic Margherita pizza, which has the following ingredients:\\n\\n- Pizza Dough: You'll need yeast, flour, salt, and water to make the dough. A simple recipe is 500g of flour, 1 tsp of salt, 1 tbsp of sugar, and about 300ml warm water.\\n\\n- Tomatoes: Fresh or canned San Marzano tomatoes are traditionally used for their sweet flavor. If using fresh tomatoes, you can blend them into a sauce.\\n\\n- Mozzarella Cheese: Traditionally mozzarella di bufala campana D.O.P., but Fior di Latte or other fresh mozzarella work well too.\\n\\n- Basil Leaves: Fresh basil leaves add a burst of flavor.\\n\\n- Olive Oil: Extra virgin olive oil is drizzled over the pizza before baking for added flavor.\\n\\n- Salt & Pepper\\n\\nYou would also need a pizza stone or baking sheet preheated in an oven set to around 475\\u00b0F (246\\u00b0C). Once your dough is prepared and shaped into a circle (use parchment paper if it's homemade), spread your tomato sauce on top leaving some space at the edge. Add dollops of cheese on top then gently press them down with your fingers. Drizzle with olive oil and season with salt & pepper. Finally add your basil leaves before placing it in the oven to bake until the crust is golden brown and bubbly - about 10 minutes depending on thickness.. Model response: To make a classic Margherita pizza like the one in the image, you'll need the following ingredients:\\n\\n### For the Dough:\\n- **Flour** (preferably Type \\\"00\\\" pizza flour)\\n- **Water**\\n- **Yeast** (active dry or fresh)\\n- **Salt**\\n- **Olive oil** (optional)\\n\\n### For the Topping:\\n- **Tomato Sauce** (preferably San Marzano tomatoes, crushed)\\n- **Fresh Mozzarella Cheese** (preferably buffalo mozzarella)\\n- **Fresh Basil Leaves**\\n- **Olive Oil** (for drizzling)\\n- **Salt** (to taste)\\n\\n### Optional:\\n- **Parmesan Cheese** (for extra flavor)\\n- **Crushed Red Pepper Flakes** (for heat)\\n\\n### Instructions Summary:\\n1. Prepare the dough by mixing flour, water, yeast, and salt, then let it rise.\\n2. Shape the dough into a pizza base.\\n3. Spread tomato sauce over the base.\\n4. Add sliced mozzarella and basil leaves.\\n5. Bake in a hot oven or pizza stone until the crust is golden and the cheese is bubbly.\\n6. Drizzle with olive oil before serving. \\n\\nEnjoy your pizza-making!.\"\n",
      "          }\n",
      "        ],\n",
      "        \"output\": [\n",
      "          {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"{\\\"steps\\\":[{\\\"description\\\":\\\"Compare the model response ingredients list with the reference ingredients list. Both include flour, water, yeast, salt, olive oil, tomato sauce (San Marzano), fresh mozzarella, basil, and salt. The model also adds optional parmesan and red pepper flakes, which do not conflict with the core Margherita recipe.\\\",\\\"conclusion\\\":\\\"Ingredient lists match well, with minor optional additions that don\\u2019t detract.\\\"},{\\\"description\\\":\\\"Assess whether the response answers the prompt: The user asks \\u201cWhat ingredients do I need to make this?\\u201d and the model response provides a clear, structured list of ingredients along with an optional instructions summary.\\\",\\\"conclusion\\\":\\\"The response directly addresses the prompt.\\\"},{\\\"description\\\":\\\"Compare the overall completeness and accuracy against the reference: The model includes all key ingredients (dough components, sauce, cheese, basil, olive oil, salt) and optional extras. Instructions are concise and helpful.\\\",\\\"conclusion\\\":\\\"The model response is comprehensive and accurate.\\\"}],\\\"result\\\":1.0}\"\n",
      "          }\n",
      "        ],\n",
      "        \"finish_reason\": \"stop\",\n",
      "        \"model\": \"o4-mini-2025-04-16\",\n",
      "        \"usage\": {\n",
      "          \"total_tokens\": 2395,\n",
      "          \"completion_tokens\": 420,\n",
      "          \"prompt_tokens\": 1975,\n",
      "          \"cached_tokens\": 0\n",
      "        },\n",
      "        \"error\": null,\n",
      "        \"seed\": null,\n",
      "        \"temperature\": 1.0,\n",
      "        \"top_p\": 1.0,\n",
      "        \"reasoning_effort\": null,\n",
      "        \"max_completions_tokens\": 4096\n",
      "      },\n",
      "      \"passed\": true,\n",
      "      \"score\": 1.0\n",
      "    }\n",
      "  ],\n",
      "  \"run_id\": \"evalrun_68768bfd44fc8191a359121443dab061\",\n",
      "  \"sample\": \"Sample(error=None, finish_reason='stop', input=[SampleInput(content='What ingredients do I need to make this?', role='user'), SampleInput(content='<image>https://storage.googleapis.com/reka-annotate.appspot.com/vibe-eval/difficulty-normal_food1_7e5c2cb9c8200d70.jpg</image>', role='user')], max_completion_tokens=None, model='gpt-4o-mini-2024-07-18', output=[SampleOutput(content='To make a classic Margherita pizza like the one in the image, you\\\\'ll need the following ingredients:\\\\n\\\\n### For the Dough:\\\\n- **Flour** (preferably Type \\\"00\\\" pizza flour)\\\\n- **Water**\\\\n- **Yeast** (active dry or fresh)\\\\n- **Salt**\\\\n- **Olive oil** (optional)\\\\n\\\\n### For the Topping:\\\\n- **Tomato Sauce** (preferably San Marzano tomatoes, crushed)\\\\n- **Fresh Mozzarella Cheese** (preferably buffalo mozzarella)\\\\n- **Fresh Basil Leaves**\\\\n- **Olive Oil** (for drizzling)\\\\n- **Salt** (to taste)\\\\n\\\\n### Optional:\\\\n- **Parmesan Cheese** (for extra flavor)\\\\n- **Crushed Red Pepper Flakes** (for heat)\\\\n\\\\n### Instructions Summary:\\\\n1. Prepare the dough by mixing flour, water, yeast, and salt, then let it rise.\\\\n2. Shape the dough into a pizza base.\\\\n3. Spread tomato sauce over the base.\\\\n4. Add sliced mozzarella and basil leaves.\\\\n5. Bake in a hot oven or pizza stone until the crust is golden and the cheese is bubbly.\\\\n6. Drizzle with olive oil before serving. \\\\n\\\\nEnjoy your pizza-making!', role='assistant')], seed=None, temperature=1.0, top_p=1.0, usage=SampleUsage(cached_tokens=0, completion_tokens=249, prompt_tokens=36856, total_tokens=37105), max_completions_tokens=4096)\",\n",
      "  \"status\": \"pass\",\n",
      "  \"_datasource_item_content_hash\": \"4baa1b4e9daaee8cce285a14b3d7e8155eef3a7770ebbbb50ee16f78f5024768\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "pizza_item = next(\n",
    "    item for item in output_items \n",
    "    if \"What ingredients do I need to make this?\" in item.datasource_item[\"prompt\"]\n",
    ")\n",
    "\n",
    "print(json.dumps(dict(pizza_item), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, feel free to extend this to your own use cases! Some examples include grading image generation results with our EvalAPI model graders, evaluating your OCR use cases using model sampling, and more. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
