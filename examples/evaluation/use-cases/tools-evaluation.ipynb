{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating Code Symbol Extraction Quality with a Custom Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates how to evaluate a model's ability to extract symbols from code files using the OpenAI **Evals** framework with a custom in-memory dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evalrun_68437e5370c481919a6874594ca177d9 queued ResultCounts(errored=0, failed=0, passed=0, total=0)\n",
            "evalrun_68437e544fe881918f76dbd8dce3fd15 queued ResultCounts(errored=0, failed=0, passed=0, total=0)\n",
            "evalrun_68437e5370c481919a6874594ca177d9 in_progress ResultCounts(errored=0, failed=0, passed=0, total=0)\n",
            "evalrun_68437e544fe881918f76dbd8dce3fd15 in_progress ResultCounts(errored=0, failed=0, passed=0, total=0)\n",
            "evalrun_68437e5370c481919a6874594ca177d9 in_progress ResultCounts(errored=0, failed=0, passed=0, total=0)\n",
            "evalrun_68437e544fe881918f76dbd8dce3fd15 in_progress ResultCounts(errored=0, failed=0, passed=0, total=0)\n",
            "evalrun_68437e5370c481919a6874594ca177d9 in_progress ResultCounts(errored=0, failed=0, passed=0, total=0)\n",
            "evalrun_68437e544fe881918f76dbd8dce3fd15 completed ResultCounts(errored=0, failed=0, passed=1, total=1)\n",
            "evalrun_68437e5370c481919a6874594ca177d9 in_progress ResultCounts(errored=0, failed=0, passed=0, total=0)\n",
            "evalrun_68437e544fe881918f76dbd8dce3fd15 completed ResultCounts(errored=0, failed=0, passed=1, total=1)\n",
            "evalrun_68437e5370c481919a6874594ca177d9 completed ResultCounts(errored=0, failed=1, passed=0, total=1)\n",
            "evalrun_68437e544fe881918f76dbd8dce3fd15 completed ResultCounts(errored=0, failed=0, passed=1, total=1)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import openai\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\") or os.getenv(\"_OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "\n",
        "def get_dataset(limit=None):\n",
        "    openai_sdk_file_path = os.path.dirname(openai.__file__)\n",
        "\n",
        "    file_paths = [\n",
        "        os.path.join(openai_sdk_file_path, \"resources\", \"evals\", \"evals.py\"),\n",
        "        os.path.join(openai_sdk_file_path, \"resources\", \"responses\", \"responses.py\"),\n",
        "        os.path.join(openai_sdk_file_path, \"resources\", \"images.py\"),\n",
        "        os.path.join(openai_sdk_file_path, \"resources\", \"embeddings.py\"),\n",
        "        os.path.join(openai_sdk_file_path, \"resources\", \"files.py\"),\n",
        "    ]\n",
        "\n",
        "    items = []\n",
        "    for file_path in file_paths:\n",
        "        items.append({\"input\": open(file_path, \"r\").read()})\n",
        "    if limit:\n",
        "        return items[:limit]\n",
        "    return items\n",
        "\n",
        "\n",
        "structured_output_grader = \"\"\"\n",
        "You are a helpful assistant that grades the quality of extracted information from a code file.\n",
        "You will be given a code file and a list of extracted information.\n",
        "You should grade the quality of the extracted information.\n",
        "\n",
        "You should grade the quality on a scale of 1 to 7.\n",
        "You should apply the following criteria, and calculate your score as follows:\n",
        "You should first check for completeness on a scale of 1 to 7.\n",
        "Then you should apply a quality modifier.\n",
        "\n",
        "The quality modifier is a multiplier from 0 to 1 that you multiply by the completeness score.\n",
        "If there is 100% coverage for completion and it is all high quality, then you would return 7*1.\n",
        "If there is 100% coverage for completion but it is all low quality, then you would return 7*0.5.\n",
        "etc.\n",
        "\"\"\"\n",
        "\n",
        "structured_output_grader_user_prompt = \"\"\"\n",
        "<Code File>\n",
        "{{item.input}}\n",
        "</Code File>\n",
        "\n",
        "<Extracted Information>\n",
        "{{sample.output_tools[0].function.arguments.symbols}}\n",
        "</Extracted Information>\n",
        "\"\"\"\n",
        "\n",
        "logs_eval = client.evals.create(\n",
        "    name=\"Code QA Eval\",\n",
        "    data_source_config={\n",
        "        \"type\": \"custom\",\n",
        "        \"item_schema\": {\"type\": \"object\", \"properties\": {\"input\": {\"type\": \"string\"}}},\n",
        "        \"include_sample_schema\": True,\n",
        "    },\n",
        "    testing_criteria=[\n",
        "        {\n",
        "            \"type\": \"score_model\",\n",
        "            \"name\": \"General Evaluator\",\n",
        "            \"model\": \"o3\",\n",
        "            \"input\": [\n",
        "                {\"role\": \"system\", \"content\": structured_output_grader},\n",
        "                {\"role\": \"user\", \"content\": structured_output_grader_user_prompt},\n",
        "            ],\n",
        "            \"range\": [1, 7],\n",
        "            \"pass_threshold\": 5.5,\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "symbol_tool = {\n",
        "    \"name\": \"extract_symbols\",\n",
        "    \"description\": \"Extract the symbols from the code file\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"symbols\": {\n",
        "                \"type\": \"array\",\n",
        "                \"description\": \"A list of symbols extracted from Python code.\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"name\": {\"type\": \"string\", \"description\": \"The name of the symbol.\"},\n",
        "                        \"symbol_type\": {\"type\": \"string\", \"description\": \"The type of the symbol, e.g., variable, function, class.\"},\n",
        "                    },\n",
        "                    \"required\": [\"name\", \"symbol_type\"],\n",
        "                    \"additionalProperties\": False,\n",
        "                },\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"symbols\"],\n",
        "        \"additionalProperties\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "gpt_4one_completions_run = client.evals.runs.create(\n",
        "    name=\"gpt-4.1\",\n",
        "    eval_id=logs_eval.id,\n",
        "    data_source={\n",
        "        \"type\": \"completions\",\n",
        "        \"source\": {\"type\": \"file_content\", \"content\": [{\"item\": item} for item in get_dataset(limit=1)]},\n",
        "        \"input_messages\": {\n",
        "            \"type\": \"template\",\n",
        "            \"template\": [\n",
        "                {\"type\": \"message\", \"role\": \"system\", \"content\": {\"type\": \"input_text\", \"text\": \"You are a helpful assistant.\"}},\n",
        "                {\"type\": \"message\", \"role\": \"user\", \"content\": {\"type\": \"input_text\", \"text\": \"Extract the symbols from the code file {{item.input}}\"}},\n",
        "            ],\n",
        "        },\n",
        "        \"model\": \"gpt-4.1\",\n",
        "        \"sampling_params\": {\n",
        "            \"seed\": 42,\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_completions_tokens\": 10000,\n",
        "            \"top_p\": 0.9,\n",
        "            \"tools\": [{\"type\": \"function\", \"function\": symbol_tool}],\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "gpt_4one_responses_run = client.evals.runs.create(\n",
        "    name=\"gpt-4.1\",\n",
        "    eval_id=logs_eval.id,\n",
        "    data_source={\n",
        "        \"type\": \"responses\",\n",
        "        \"source\": {\"type\": \"file_content\", \"content\": [{\"item\": item} for item in get_dataset(limit=1)]},\n",
        "        \"input_messages\": {\n",
        "            \"type\": \"template\",\n",
        "            \"template\": [\n",
        "                {\"type\": \"message\", \"role\": \"system\", \"content\": {\"type\": \"input_text\", \"text\": \"You are a helpful assistant.\"}},\n",
        "                {\"type\": \"message\", \"role\": \"user\", \"content\": {\"type\": \"input_text\", \"text\": \"Extract the symbols from the code file {{item.input}}\"}},\n",
        "            ],\n",
        "        },\n",
        "        \"model\": \"gpt-4.1\",\n",
        "        \"sampling_params\": {\n",
        "            \"seed\": 42,\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_completions_tokens\": 10000,\n",
        "            \"top_p\": 0.9,\n",
        "            \"tools\": [{\"type\": \"function\", **symbol_tool}],\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "def poll_runs(eval_id, run_ids):\n",
        "    # poll both runs at the same time, until they are complete or failed\n",
        "    while True:\n",
        "        runs = [client.evals.runs.retrieve(run_id, eval_id=eval_id) for run_id in run_ids]\n",
        "        for run in runs:\n",
        "            print(run.id, run.status, run.result_counts)\n",
        "        if all(run.status in (\"completed\", \"failed\") for run in runs):\n",
        "            break\n",
        "        time.sleep(5)\n",
        "\n",
        "\n",
        "poll_runs(logs_eval.id, [gpt_4one_completions_run.id, gpt_4one_responses_run.id])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "completions_output = client.evals.runs.output_items.list(\n",
        "    run_id=gpt_4one_completions_run.id, eval_id=logs_eval.id\n",
        ")\n",
        "\n",
        "responses_output = client.evals.runs.output_items.list(\n",
        "    run_id=gpt_4one_responses_run.id, eval_id=logs_eval.id\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'name': 'Evals', 'symbol_type': 'class'}, {'name': 'AsyncEvals', 'symbol_type': 'class'}, {'name': 'EvalsWithRawResponse', 'symbol_type': 'class'}, {'name': 'AsyncEvalsWithRawResponse', 'symbol_type': 'class'}, {'name': 'EvalsWithStreamingResponse', 'symbol_type': 'class'}, {'name': 'AsyncEvalsWithStreamingResponse', 'symbol_type': 'class'}, {'name': '__all__', 'symbol_type': 'variable'}, {'name': 'Evals.runs', 'symbol_type': 'function'}, {'name': 'Evals.with_raw_response', 'symbol_type': 'function'}, {'name': 'Evals.with_streaming_response', 'symbol_type': 'function'}, {'name': 'Evals.create', 'symbol_type': 'function'}, {'name': 'Evals.retrieve', 'symbol_type': 'function'}, {'name': 'Evals.update', 'symbol_type': 'function'}, {'name': 'Evals.list', 'symbol_type': 'function'}, {'name': 'Evals.delete', 'symbol_type': 'function'}, {'name': 'AsyncEvals.runs', 'symbol_type': 'function'}, {'name': 'AsyncEvals.with_raw_response', 'symbol_type': 'function'}, {'name': 'AsyncEvals.with_streaming_response', 'symbol_type': 'function'}, {'name': 'AsyncEvals.create', 'symbol_type': 'function'}, {'name': 'AsyncEvals.retrieve', 'symbol_type': 'function'}, {'name': 'AsyncEvals.update', 'symbol_type': 'function'}, {'name': 'AsyncEvals.list', 'symbol_type': 'function'}, {'name': 'AsyncEvals.delete', 'symbol_type': 'function'}, {'name': 'EvalsWithRawResponse.__init__', 'symbol_type': 'function'}, {'name': 'EvalsWithRawResponse.runs', 'symbol_type': 'function'}, {'name': 'AsyncEvalsWithRawResponse.__init__', 'symbol_type': 'function'}, {'name': 'AsyncEvalsWithRawResponse.runs', 'symbol_type': 'function'}, {'name': 'EvalsWithStreamingResponse.__init__', 'symbol_type': 'function'}, {'name': 'EvalsWithStreamingResponse.runs', 'symbol_type': 'function'}, {'name': 'AsyncEvalsWithStreamingResponse.__init__', 'symbol_type': 'function'}, {'name': 'AsyncEvalsWithStreamingResponse.runs', 'symbol_type': 'function'}]\n",
            "[{'name': 'Evals', 'symbol_type': 'class'}, {'name': 'AsyncEvals', 'symbol_type': 'class'}, {'name': 'EvalsWithRawResponse', 'symbol_type': 'class'}, {'name': 'AsyncEvalsWithRawResponse', 'symbol_type': 'class'}, {'name': 'EvalsWithStreamingResponse', 'symbol_type': 'class'}, {'name': 'AsyncEvalsWithStreamingResponse', 'symbol_type': 'class'}, {'name': '__all__', 'symbol_type': 'variable'}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "for item in completions_output:\n",
        "    print(json.loads(item.sample.output[0].tool_calls[0][\"function\"][\"arguments\"])[\"symbols\"])\n",
        "\n",
        "for item in responses_output:\n",
        "    print(json.loads(item.sample.output[0].tool_calls[0][\"function\"][\"arguments\"])[\"symbols\"])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
