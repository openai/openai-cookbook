{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37937b7",
   "metadata": {},
   "source": [
    "# **GPT-5.1-Codex-Max** Prompting Guide\n",
    "\n",
    "GPT-5.1-Codex-Max advances the frontier of intelligence and efficiency and is our best agentic coding model. Follow this guide closely to ensure you’re getting the best performance possible from this model. This guide is for anyone using the model directly via the API for maximum customizability; we also have the [Codex SDK](https://developers.openai.com/codex/sdk/) for simpler integrations.\n",
    "\n",
    "Key improvements\n",
    "\n",
    "* Faster and more token efficient: Matches GPT-5.1-Codex performance on SWE-Bench Verified with \\~30% fewer thinking tokens. We recommend “medium” reasoning effort as a good all-around interactive coding model that balances intelligence and speed.  \n",
    "* Higher intelligence and long-running autonomy: Codex is very capable and will work autonomously for hours to complete your hardest tasks. You can use `high` or `xhigh` reasoning effort for your hardest tasks.  \n",
    "* First-class compaction support: Compaction enables multi-hour reasoning without hitting context limits and longer continuous user conversations without needing to start new chat sessions.  \n",
    "* This model is also much better in PowerShell and Windows environments.\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "If you already have a working Codex implementation, this model should work well with relatively minimal updates, but if you’re starting with a prompt and set of tools that’s optimized for GPT-5-series models, or a third-party model, we recommend making more significant changes. The best reference implementation is our fully open-source codex-cli agent, available on [GitHub](https://github.com/openai/codex). Clone this repo and use Codex (or any coding agent) to ask questions about how things are implemented. From working with customers, we’ve also learned how to customize agent harnesses beyond this particular implementation.\n",
    "\n",
    "Key steps to migrate your harness to codex-cli:\n",
    "\n",
    "1. Update your prompt: If you can, start with our standard Codex-Max prompt as your base and make tactical additions from there.  \n",
    "   a) The most critical snippets are those covering autonomy and persistence, codebase exploration, tool use, and frontend quality.  \n",
    "   b) You should also remove all prompting for the model to communicate an upfront plan, preambles, or other status updates during the rollout, as this can cause the model to stop abruptly before the rollout is complete.  \n",
    "2. Update your tools, including our apply\\_patch implementation and other best practices below. This is a major lever for getting the most performance from GPT-5.1-Codex-Max.\n",
    "\n",
    "# Prompting\n",
    "\n",
    "## Recommended Starter Prompt\n",
    "\n",
    "This prompt began as the default [GPT-5.1-Codex-Max prompt](https://github.com/openai/codex/blob/main/codex-rs/core/gpt-5.1-codex-max_prompt.md) and was further optimized against internal evals for answer correctness, completeness, quality, correct tool usage and parallelism, and bias for action. If you’re running evals with this model, we recommend turning up the autonomy or prompting for a “non-interactive” mode, though in actual usage more clarification may be desirable.\n",
    "\n",
    "```\n",
    "You are Codex, based on GPT-5. You are running as a coding agent in the Codex CLI on a user's computer.\n",
    "\n",
    "\n",
    "# General\n",
    "\n",
    "- Act as a senior engineer who owns outcomes, not a chat assistant. Default to delivering correct, working code end-to-end.\n",
    "- If a tool exists for an action, use it instead of shell commands (e.g. `read_file` over `cat`). Avoid raw `cmd`/terminal unless no tool can perform the action.\n",
    "- Prefer fast, indexed search tools (`rg`, `rg --files`). Fall back only if unavailable.\n",
    "- Optimize for information throughput: batch reads/searches and parallelize tool calls whenever possible instead of making speculative single calls.\n",
    "- Code snippets may include inline line numbers like `L123:`. Treat these as metadata, not part of the code.\n",
    "- If details are missing, make reasonable assumptions and complete a working implementation rather than stopping at a plan.\n",
    "\n",
    "\n",
    "# Autonomy and Ownership\n",
    "\n",
    "- Once given a direction, proactively gather context, decide an approach, implement, verify, and explain—without waiting for follow-up prompts.\n",
    "- Persist until the task is complete within the current turn whenever feasible; avoid stopping at analysis or partial fixes.\n",
    "- Bias to action: proceed with reasonable assumptions and only ask clarifying questions when truly blocked or when changes would be destructive or irreversible.\n",
    "- If progress stalls due to repetition or thrashing, stop, summarize clearly, and ask a targeted question.\n",
    "\n",
    "\n",
    "# Code Implementation\n",
    "\n",
    "- Optimize for correctness, clarity, and long-term reliability over speed or cleverness.\n",
    "- Solve the root problem, not just a visible symptom or narrow slice.\n",
    "- Follow existing codebase conventions for structure, naming, formatting, types, and localization. If you diverge, say why.\n",
    "- Preserve existing behavior and UX by default. Gate or clearly surface intentional behavior changes and add tests when appropriate.\n",
    "- Handle errors explicitly and consistently with repo patterns; no broad catches, silent failures, or success-shaped fallbacks.\n",
    "- Batch logical changes together after reading sufficient context; avoid repeated micro-edits.\n",
    "- Maintain type safety. Avoid `any` or unsafe casts; prefer proper guards and existing helpers.\n",
    "- Search for prior art before adding new helpers or logic; reuse or extract instead of duplicating.\n",
    "\n",
    "\n",
    "# Editing Constraints\n",
    "\n",
    "- Default to ASCII. Introduce Unicode only when justified and consistent with the file.\n",
    "- Add comments sparingly, only where the code would otherwise require significant mental unpacking.\n",
    "- Prefer `apply_patch` for scoped edits; avoid it for auto-generated files or bulk mechanical changes better handled by tooling.\n",
    "- Assume a dirty git worktree and respect user changes you did not make.\n",
    "- Never revert, amend, or discard unrelated changes unless explicitly requested.\n",
    "- If you detect unexpected modifications you did not make, stop immediately and ask how to proceed.\n",
    "- Never use destructive git commands (`reset --hard`, `checkout --`) without explicit approval.\n",
    "\n",
    "\n",
    "# Exploration and Reading Files\n",
    "\n",
    "- Think first: decide all files and resources you will need before making tool calls.\n",
    "- Batch reads and searches across files and directories whenever possible.\n",
    "- Use `multi_tool_use.parallel` for parallelism; avoid serial reads unless the next step is genuinely unknowable.\n",
    "- Preferred workflow: plan reads → one parallel batch → analyze → repeat only if new, unpredictable needs arise.\n",
    "\n",
    "\n",
    "# Planning Tool\n",
    "\n",
    "- Use planning only for multi-step or multi-file work where tracking state matters; skip it for straightforward tasks.\n",
    "- Plans guide execution but are not the deliverable. Do not end an interaction with a plan alone.\n",
    "- Update the plan as work progresses and reconcile all items before finishing.\n",
    "- Mark each item as Done, Blocked (with a one-sentence reason and targeted question), or Cancelled.\n",
    "- Avoid committing to tests or refactors unless you will do them now; otherwise label them as optional next steps.\n",
    "\n",
    "\n",
    "# Special User Requests\n",
    "\n",
    "- If a request can be satisfied directly via a terminal command (e.g. `date`), do so.\n",
    "- If the user asks for a review, switch to review mode: prioritize bugs, risks, regressions, and missing coverage; present findings first, ordered by severity.\n",
    "- If no issues are found, say so explicitly and note any remaining uncertainty or testing gaps.\n",
    "\n",
    "\n",
    "# Frontend Tasks\n",
    "\n",
    "- Avoid generic or interchangeable UI outcomes; aim for intentional, authored designs.\n",
    "- Choose a clear visual direction in typography, color, motion, and layout.\n",
    "- Use motion and background treatments deliberately, not as decoration.\n",
    "- Ensure results are runnable and usable on both desktop and mobile within scope.\n",
    "- When working inside an existing product or design system, preserve its established patterns and language.\n",
    "\n",
    "\n",
    "# Presenting Your Work\n",
    "\n",
    "- Produce plain text suitable for CLI rendering; add structure only where it improves scanability.\n",
    "- Be concise and collaborative in tone.\n",
    "- Explain what changed, where, and why; avoid dumping large files—reference paths instead.\n",
    "- Suggest natural next steps only when they make sense.\n",
    "- When asked for command output, summarize the important details rather than pasting raw output.\n",
    "\n",
    "\n",
    "## Final Answer Structure and Style Guidelines\n",
    "\n",
    "- Use headers sparingly; keep them short and meaningful.\n",
    "- Prefer flat bullet lists with consistent phrasing.\n",
    "- Use backticks for commands, paths, identifiers, and inline code.\n",
    "- Wrap multi-line code in fenced blocks with language info where possible.\n",
    "- Keep wording precise, active, and self-contained.\n",
    "- For file references, use inline code paths and optional 1-based line numbers; avoid URIs and line ranges.\n",
    "\n",
    "```\n",
    "## Mid-Rollout User Updates\n",
    "\n",
    "The Codex model family uses reasoning summaries to communicate user updates as it’s working. This can be in the form of one-liner headings (which updates the ephemeral text in Codex-CLI), or both heading and a short body. This is done by a separate model and therefore is **not promptable**, and we advise against adding any instructions to the prompt related to intermediate plans or messages to the user. We’ve improved these summaries for Codex-Max to be more communicative and provide more critical information about what’s happening and why; some of our users are updating their UX to promote these summaries more prominently in their UI, similar to how intermediate messages are displayed for GPT-5 series models.\n",
    "\n",
    "## Using agents.md\n",
    "\n",
    "Codex-cli automatically enumerates these files and injects them into the conversation; the model has been trained to closely adhere to these instructions.\n",
    "\n",
    "1\\. Files are pulled from \\~/.codex plus each directory from repo root to CWD (with optional fallback names and a size cap).  \n",
    "2\\. They’re merged in order, later directories overriding earlier ones.  \n",
    "3\\. Each merged chunk shows up to the model as its own user-role message like so:\n",
    "\n",
    "```\n",
    "# AGENTS.md instructions for <directory>\n",
    "<INSTRUCTIONS>\n",
    "...file contents...\n",
    "</INSTRUCTIONS>\n",
    "```\n",
    "\n",
    "Additional details\n",
    "\n",
    "* Each discovered file becomes its own user-role message that starts with \\# AGENTS.md instructions for \\<directory\\>, where \\<directory\\> is the path (relative to the repo root) of the folder that provided that file.  \n",
    "* Messages are injected near the top of the conversation history, before the user prompt, in root-to-leaf order: global instructions first, then repo root, then each deeper directory. If an AGENTS.override.md was used, its directory name still appears in the header (e.g., \\# AGENTS.md instructions for backend/api), so the context is obvious in the transcript.\n",
    "\n",
    "# Compaction\n",
    "\n",
    "Compaction unlocks significantly longer effective context windows, where user conversations can persist for many turns without hitting context window limits or long context performance degradation, and agents can perform very long trajectories that exceed a typical context window for long-running, complex tasks. A weaker version of this was previously possible with ad-hoc scaffolding and conversation summarization, but our first-class implementation, available via the Responses API, is integrated with the model and is highly performant.\n",
    "\n",
    "How it works:\n",
    "\n",
    "1. You use the Responses API as today, sending input items that include tool calls, user inputs, and assistant messages.  \n",
    "2. When your context window grows large, you can invoke /compact to generate a new, compacted context window. Two things to note:  \n",
    "   1. The context window that you send to /compact should fit within your model’s context window.  \n",
    "   2. The endpoint is ZDR compatible and will return an “encrypted\\_content” item that you can pass into future requests.  \n",
    "3. For subsequent calls to the /responses endpoint, you can pass your updated, compacted list of conversation items (including the added compaction item). The model retains key prior state with fewer conversation tokens.\n",
    "\n",
    "For endpoint details see our `/responses/compact` [docs](https://platform.openai.com/docs/api-reference/responses/compact).\n",
    "\n",
    "# Tools\n",
    "\n",
    "1. We strongly recommend using our exact `apply_patch` implementation as the model has been trained to excel at this diff format. For terminal commands we recommend our `shell` tool, and for plan/TODO items our `update_plan` tool should be most performant.  \n",
    "2. If you prefer your agent to use more “terminal-like tools” (like `file_read()` instead of calling \\`sed\\` in the terminal), this model can reliably call them instead of terminal (following the instructions below)  \n",
    "3. For other tools, including semantic search, MCPs, or other custom tools, they can work but it requires more tuning and experimentation.\n",
    "\n",
    "### Apply\\_patch\n",
    "\n",
    "The easiest way to implement apply\\_patch is with our first-class implementation in the Responses API, but you can also use our freeform tool implementation with [context-free grammar](https://cookbook.openai.com/examples/gpt-5/gpt-5_new_params_and_tools?utm_source=chatgpt.com#3-contextfree-grammar-cfg). Both are demonstrated below.\n",
    "\n",
    "```py\n",
    "# Sample script to demonstrate the server-defined apply_patch tool\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "from typing import cast\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types.responses import ResponseInputParam, ToolParam\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "## Shared tools and prompt\n",
    "user_request = \"\"\"Add a cancel button that logs when clicked\"\"\"\n",
    "file_excerpt = \"\"\"\\\n",
    "export default function Page() {\n",
    "return (\n",
    "<div>\n",
    "    <p>Page component not implemented</p>\n",
    "    <button onClick={() => console.log(\"clicked\")}>Click me</button>\n",
    "</div>\n",
    ");\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "input_items: ResponseInputParam = [\n",
    "    {\"role\": \"user\", \"content\": user_request},\n",
    "    {\n",
    "        \"type\": \"function_call\",\n",
    "        \"call_id\": \"call_read_file_1\",\n",
    "        \"name\": \"read_file\",\n",
    "        \"arguments\": json.dumps({\"path\": (\"/app/page.tsx\")}),\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": \"call_read_file_1\",\n",
    "        \"output\": file_excerpt,\n",
    "    },\n",
    "]\n",
    "\n",
    "read_file_tool: ToolParam = cast(\n",
    "    ToolParam,\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"read_file\",\n",
    "        \"description\": \"Reads a file from disk\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"path\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"path\"],\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "### Get patch with built-in responses tool\n",
    "tools: list[ToolParam] = [\n",
    "    read_file_tool,\n",
    "    cast(ToolParam, {\"type\": \"apply_patch\"}),\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.1-Codex-Max\",\n",
    "    input=input_items,\n",
    "    tools=tools,\n",
    "    parallel_tool_calls=False,\n",
    ")\n",
    "\n",
    "for item in response.output:\n",
    "    if item.type == \"apply_patch_call\":\n",
    "        print(\"Responses API apply_patch patch:\")\n",
    "        pprint(item.operation)\n",
    "        # output:\n",
    "        # {'diff': '@@\\n'\n",
    "        #          '   return (\\n'\n",
    "        #          '     <div>\\n'\n",
    "        #          '       <p>Page component not implemented</p>\\n'\n",
    "        #          '       <button onClick={() => console.log(\"clicked\")}>Click me</button>\\n'\n",
    "        #          '+      <button onClick={() => console.log(\"cancel clicked\")}>Cancel</button>\\n'\n",
    "        #          '     </div>\\n'\n",
    "        #          '   );\\n'\n",
    "        #          ' }\\n',\n",
    "        #  'path': '/app/page.tsx',\n",
    "        #  'type': 'update_file'}\n",
    "\n",
    "### Get patch with custom tool implementation, including freeform tool definition and context-free grammar\n",
    "apply_patch_grammar = \"\"\"\n",
    "start: begin_patch hunk+ end_patch\n",
    "begin_patch: \"*** Begin Patch\" LF\n",
    "end_patch: \"*** End Patch\" LF?\n",
    "\n",
    "hunk: add_hunk | delete_hunk | update_hunk\n",
    "add_hunk: \"*** Add File: \" filename LF add_line+\n",
    "delete_hunk: \"*** Delete File: \" filename LF\n",
    "update_hunk: \"*** Update File: \" filename LF change_move? change?\n",
    "\n",
    "filename: /(.+)/\n",
    "add_line: \"+\" /(.*)/ LF -> line\n",
    "\n",
    "change_move: \"*** Move to: \" filename LF\n",
    "change: (change_context | change_line)+ eof_line?\n",
    "change_context: (\"@@\" | \"@@ \" /(.+)/) LF\n",
    "change_line: (\"+\" | \"-\" | \" \") /(.*)/ LF\n",
    "eof_line: \"*** End of File\" LF\n",
    "\n",
    "%import common.LF\n",
    "\"\"\"\n",
    "\n",
    "tools_with_cfg: list[ToolParam] = [\n",
    "    read_file_tool,\n",
    "    cast(\n",
    "        ToolParam,\n",
    "        {\n",
    "            \"type\": \"custom\",\n",
    "            \"name\": \"apply_patch_grammar\",\n",
    "            \"description\": \"Use the `apply_patch` tool to edit files. This is a FREEFORM tool, so do not wrap the patch in JSON.\",\n",
    "            \"format\": {\n",
    "                \"type\": \"grammar\",\n",
    "                \"syntax\": \"lark\",\n",
    "                \"definition\": apply_patch_grammar,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "response_cfg = client.responses.create(\n",
    "    model=\"gpt-5.1-Codex-Max\",\n",
    "    input=input_items,\n",
    "    tools=tools_with_cfg,\n",
    "    parallel_tool_calls=False,\n",
    ")\n",
    "\n",
    "for item in response_cfg.output:\n",
    "    if item.type == \"custom_tool_call\":\n",
    "        print(\"\\n\\nContext-free grammar apply_patch patch:\")\n",
    "        print(item.input)\n",
    "        #  Output\n",
    "        # *** Begin Patch\n",
    "        # *** Update File: /app/page.tsx\n",
    "        # @@\n",
    "        #      <div>\n",
    "        #        <p>Page component not implemented</p>\n",
    "        #        <button onClick={() => console.log(\"clicked\")}>Click me</button>\n",
    "        # +      <button onClick={() => console.log(\"cancel clicked\")}>Cancel</button>\n",
    "        #      </div>\n",
    "        #    );\n",
    "        #  }\n",
    "        # *** End Patch\n",
    "```\n",
    "\n",
    "Patches objects the Responses API tool can be implemented by following this [example](https://github.com/openai/openai-agents-python/blob/main/examples/tools/apply_patch.py) and patches from the freeform tool can be applied with the logic in our canonical GPT-5 [apply\\_patch.py](https://github.com/openai/openai-cookbook/blob/main/examples/gpt-5/apply_patch.py%20) implementation.\n",
    "\n",
    "### Shell\\_command\n",
    "\n",
    "This is our default shell tool. Note that we have seen better performance with a command type “string” rather than a list of commands.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"shell_command\",\n",
    "    \"description\": \"Runs a shell command and returns its output.\\n- Always set the `workdir` param when using the shell_command function. Do not use `cd` unless absolutely necessary.\",\n",
    "    \"strict\": false,\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"command\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The shell script to execute in the user's default shell\"\n",
    "        },\n",
    "        \"workdir\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The working directory to execute the command in\"\n",
    "        },\n",
    "        \"timeout_ms\": {\n",
    "          \"type\": \"number\",\n",
    "          \"description\": \"The timeout for the command in milliseconds\"\n",
    "        },\n",
    "        \"with_escalated_permissions\": {\n",
    "          \"type\": \"boolean\",\n",
    "          \"description\": \"Whether to request escalated permissions. Set to true if command needs to be run without sandbox restrictions\"\n",
    "        },\n",
    "        \"justification\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"Only set if with_escalated_permissions is true. 1-sentence explanation of why we want to run this command.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"command\"],\n",
    "      \"additionalProperties\": false\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "If you’re using Windows PowerShell, update to this tool description.\n",
    "\n",
    "```\n",
    "Runs a shell command and returns its output. The arguments you pass will be invoked via PowerShell (e.g., [\"pwsh\", \"-NoLogo\", \"-NoProfile\", \"-Command\", \"<cmd>\"]). Always fill in workdir; avoid using cd in the command string.\n",
    "```\n",
    "\n",
    "You can check out codex-cli for the implementation for `exec_command`, which launches a long-lived PTY when you need streaming output, REPLs, or interactive sessions; and `write_stdin`, to feed extra keystrokes (or just poll output) for an existing exec\\_command session.\n",
    "\n",
    "### Update Plan\n",
    "\n",
    "This is our default TODO tool; feel free to customize as you’d prefer. See the `## Plan tool` section of our starter prompt for additional instructions to maintain hygiene and tweak behavior.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"update_plan\",\n",
    "    \"description\": \"Updates the task plan.\\nProvide an optional explanation and a list of plan items, each with a step and status.\\nAt most one step can be in_progress at a time.\",\n",
    "    \"strict\": false,\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"explanation\": {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"plan\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"step\": {\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"status\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"One of: pending, in_progress, completed\"\n",
    "              }\n",
    "            },\n",
    "            \"additionalProperties\": false,\n",
    "            \"required\": [\n",
    "              \"step\",\n",
    "              \"status\"\n",
    "            ]\n",
    "          },\n",
    "          \"description\": \"The list of steps\"\n",
    "        }\n",
    "      },\n",
    "      \"additionalProperties\": false,\n",
    "      \"required\": [\n",
    "        \"plan\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### View\\_image\n",
    "\n",
    "This is a basic function used in codex-cli for the model to view images.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"view_image\",\n",
    "    \"description\": \"Attach a local image (by filesystem path) to the conversation context for this turn.\",\n",
    "    \"strict\": false,\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"path\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"Local filesystem path to an image file\"\n",
    "        }\n",
    "      },\n",
    "      \"additionalProperties\": false,\n",
    "      \"required\": [\n",
    "        \"path\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## Dedicated terminal-wrapping tools\n",
    "\n",
    "If you would prefer your codex agent to use terminal-wrapping tools (like a dedicated `list_dir(‘.’)` tool instead of `terminal(‘ls .’)`, this generally works well. We see the best results when the name of the tool, the arguments, and the output are as close as possible to those from the underlying command, so it’s as in-distribution as possible for the model (which was primarily trained using a dedicated terminal tool). For example, if you notice the model using git via the terminal and would prefer it to use a dedicated tool, we found that creating a related tool, and adding a directive in the prompt to only use that tool for git commands, fully mitigated the model’s terminal usage for git commands.\n",
    "\n",
    "```\n",
    "GIT_TOOL = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"git\",\n",
    "    \"description\": (\n",
    "        \"Execute a git command in the repository root. Behaves like running git in the\"\n",
    "        \" terminal; supports any subcommand and flags. The command can be provided as a\"\n",
    "        \" full git invocation (e.g., `git status -sb`) or just the arguments after git\"\n",
    "        \" (e.g., `status -sb`).\"\n",
    "    ),\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"command\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": (\n",
    "                    \"The git command to execute. Accepts either a full git invocation or\"\n",
    "                    \" only the subcommand/args.\"\n",
    "                ),\n",
    "            },\n",
    "            \"timeout_sec\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"minimum\": 1,\n",
    "                \"maximum\": 1800,\n",
    "                \"description\": \"Optional timeout in seconds for the git command.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"command\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "PROMPT_TOOL_USE_DIRECTIVE = \"- Strictly avoid raw `cmd`/terminal when a dedicated tool exists. Default to solver tools: `git` (all git), `list_dir`, `apply_patch`. Use `cmd`/`run_terminal_cmd` only when no listed tool can perform the action.\" # update with your desired tools\n",
    "```\n",
    "\n",
    "## Other Custom Tools (web search, semantic search, memory, etc.)\n",
    "\n",
    "The model hasn’t necessarily been post-trained to excel at these tools, but we have seen success here as well. To get the most out of these tools, we recommend:\n",
    "\n",
    "1. Making the tool names and arguments as semantically “correct” as possible, for example “search” is ambiguous but “semantic\\_search” clearly indicates what the tool does, relative to other potential search-related tools you might have. “Query” would be a good param name for this tool.  \n",
    "2. Be explicit in your prompt about when, why, and how to use these tools, including good and bad examples.  \n",
    "3. It could also be helpful to make the results look different from outputs the model is accustomed to seeing from other tools, for example ripgrep results should look different from semantic search results to avoid the model collapsing into old habits.\n",
    "\n",
    "## Parallel Tool Calling\n",
    "\n",
    "In codex-cli, when parallel tool calling is enabled, the responses API request sets `parallel_tool_calls: true` and the following snippet is added to the system instructions:\n",
    "\n",
    "```\n",
    "## Exploration and reading files\n",
    "\n",
    "- **Think first.** Before any tool call, decide ALL files/resources you will need.\n",
    "- **Batch everything.** If you need multiple files (even from different places), read them together.\n",
    "- **multi_tool_use.parallel** Use `multi_tool_use.parallel` to parallelize tool calls and only this.\n",
    "- **Only make sequential calls if you truly cannot know the next file without seeing a result first.**\n",
    "- **Workflow:** (a) plan all needed reads → (b) issue one parallel batch → (c) analyze results → (d) repeat if new, unpredictable reads arise.\n",
    "\n",
    "**Additional notes**:\n",
    "- Always maximize parallelism. Never read files one-by-one unless logically unavoidable.\n",
    "- This concerns every read/list/search operations including, but not only, `cat`, `rg`, `sed`, `ls`, `git show`, `nl`, `wc`, ...\n",
    "- Do not try to parallelize using scripting or anything else than `multi_tool_use.parallel`.\n",
    "```\n",
    "\n",
    "We've found it to be helpful and more in-distribution if parallel tool call items and responses are ordered in the following way:\n",
    "\n",
    "```\n",
    "function_call\n",
    "function_call\n",
    "function_call_output\n",
    "function_call_output\n",
    "```\n",
    "\n",
    "## Tool Response Truncation\n",
    "\n",
    "We recommend doing tool call response truncation as follows to be as in-distribution for the model as possible:\n",
    "\n",
    "* Limit to 10k tokens. You can cheaply approximate this by computing `num_bytes/4`.  \n",
    "* If you hit the truncation limit, you should use half of the budget for the beginning, half for the end, and truncate in the middle with `…3 tokens truncated…`\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
