{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa8ba3e",
   "metadata": {},
   "source": [
    "# Image Evals for Image Generation and Editing Use Cases\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Image models are increasingly used in real product workflows—design\n",
    "mockups, marketing assets, virtual try-on, and high-precision edits to\n",
    "existing brand materials. To trust these systems in production, you need\n",
    "more than “does it look good?” You need **repeatable, workflow-specific\n",
    "evaluation** that measures whether outputs satisfy requirements, fail\n",
    "safely, and improve predictably over time.\n",
    "\n",
    "Vision evaluations(or vision evals) are harder than text evals because the “answer” is an image\n",
    "that mixes:\n",
    "\n",
    "- **Hard constraints**: exact text, counts, attributes, locality\n",
    "  (“change only this region”).  \n",
    "- **Perceptual quality**: sharpness, coherence, realism, aesthetic\n",
    "  match.  \n",
    "- **Hidden failure modes**: subtle distortions, unintended edits, or\n",
    "  small text errors that look fine at a glance but break product\n",
    "  requirements—especially in editing.\n",
    "\n",
    "A good vision eval does **not** score “a pretty picture.” It scores\n",
    "whether the model is **reliable for a specific workflow**. Many images\n",
    "that look visually strong still fail because text is wrong, style is\n",
    "off-brand, or edits spill beyond the intended area. Image evals measure\n",
    "**quality, controllability, and usability** for real prompts—not just\n",
    "visual appeal.\n",
    "\n",
    "#### What this guide covers\n",
    "\n",
    "This cookbook focuses on building a practical image-eval system for four\n",
    "major categories:\n",
    "\n",
    "**1) Image generation evals**\n",
    "\n",
    "- Instruction following (constraints satisfied)  \n",
    "- Text rendering (accuracy, legibility, placement)  \n",
    "- Style control (aesthetic match, brand/character consistency)  \n",
    "- Preference alignment (rubric labels + pairwise comparisons)\n",
    "\n",
    "**2) Image editing evals**\n",
    "\n",
    "- Transformation correctness (the requested change is done exactly)  \n",
    "- Locality (edits happen only where intended)  \n",
    "- Preservation (unrequested regions remain unchanged)  \n",
    "- Spatial control (edits applied to the correct instance / region)\n",
    "\n",
    "**3) Human feedback alignment**\n",
    "\n",
    "- Rubric-based labels and pairwise preferences to capture subjective\n",
    "  quality and “vibe”  \n",
    "- Calibration techniques to keep human judgments consistent over time\n",
    "\n",
    "**4) Strategy for building evals**\n",
    "\n",
    "- Start with non-negotiable correctness gates  \n",
    "- Add graded quality metrics once failures are controlled  \n",
    "- Tag failure modes to drive targeted iteration\n",
    "\n",
    "## Building a Vision Eval Harness\n",
    "\n",
    "A vision eval harness is a small, repeatable system that turns “did this\n",
    "image work?” into **structured, comparable results** across prompts,\n",
    "models, and settings.\n",
    "\n",
    "At a high level, vision evals follow the same loop as any LLM eval\n",
    "system:\n",
    "\n",
    "**Inputs → Model → Outputs → Graders → Scores → Feedback → Improvement**\n",
    "\n",
    "To make this reusable across the rest of the cookbook (generation +\n",
    "editing), build the harness around **three plug-ins**:\n",
    "\n",
    "1.  **Test cases:** what to run (prompt + criteria + optional input\n",
    "    images/mask)  \n",
    "2.  **Runners:** how to call a model and save output images  \n",
    "3.  **Graders:** how to score an output (rubrics, LLM-as-judge, human\n",
    "    labels later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6277f",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "Run this once. It:\n",
    "- creates the API client\n",
    "- creates `images/` in the folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b168bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "os.makedirs(\"../../images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6df41",
   "metadata": {},
   "source": [
    "Below is a minimal `vision_harness/` package you can drop into your repo.\n",
    "\n",
    "For a full, runnable harness you can extend (edit, reproduce, and add your own cases), see [examples/evals/imagegen_evals](https://github.com/openai/openai-cookbook/tree/main/examples/evals/imagegen_evals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d4d28",
   "metadata": {},
   "source": [
    "\n",
    "### vision_harness/types.py\n",
    "\n",
    "Keep the core types generic so you can reuse them for *both* image\n",
    "generation and image editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d89fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal, Optional\n",
    "\n",
    "TaskType = Literal[\"image_generation\", \"image_editing\"]\n",
    "ScoreValue = bool | int | float | str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ImageInputs:\n",
    "    \"\"\"Editing inputs: one or more reference images + optional mask.\"\"\"\n",
    "    image_paths: list[Path]\n",
    "    mask_path: Optional[Path] = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TestCase:\n",
    "    \"\"\"A single evaluable example.\"\"\"\n",
    "    id: str\n",
    "    task_type: TaskType\n",
    "    prompt: str\n",
    "    criteria: str\n",
    "    image_inputs: Optional[ImageInputs] = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelRun:\n",
    "    \"\"\"One model configuration to evaluate (useful for sweeps).\"\"\"\n",
    "    label: str\n",
    "    task_type: TaskType\n",
    "    params: dict[str, Any]  # e.g. {\"model\": \"...\", \"quality\": \"...\", ...}\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Artifact:\n",
    "    \"\"\"A saved artifact from a run (usually an image).\"\"\"\n",
    "    kind: Literal[\"image\"]\n",
    "    path: Path\n",
    "    mime: str = \"image/png\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelResponse:\n",
    "    \"\"\"Normalized output from any runner.\"\"\"\n",
    "    artifacts: list[Artifact] = field(default_factory=list)\n",
    "    raw: dict[str, Any] = field(default_factory=dict)  # optional debug payload\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Score:\n",
    "    key: str\n",
    "    value: ScoreValue\n",
    "    reason: str = \"\"\n",
    "    tags: Optional[list[str]] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a36d9",
   "metadata": {},
   "source": [
    "### vision_harness/io.py\n",
    "\n",
    "You’ll use this in graders (LLM-as-judge) and sometimes in model calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9a6cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "_MIME_BY_SUFFIX = {\n",
    "    \".jpg\": \"image/jpeg\",\n",
    "    \".jpeg\": \"image/jpeg\",\n",
    "    \".png\": \"image/png\",\n",
    "    \".webp\": \"image/webp\",\n",
    "}\n",
    "\n",
    "def image_to_data_url(path: Path) -> str:\n",
    "    mime = _MIME_BY_SUFFIX.get(path.suffix.lower(), \"image/png\")\n",
    "    b64 = base64.b64encode(path.read_bytes()).decode(\"utf-8\")\n",
    "    return f\"data:{mime};base64,{b64}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee65cce3",
   "metadata": {},
   "source": [
    "### vision_harness/storage.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ff02bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class OutputStore:\n",
    "    \"\"\"\n",
    "    Simple artifact store that writes directly to the root folder.\n",
    "    Note: This implementation writes directly to `root` (no per-test subfolders),\n",
    "    which keeps site-relative image paths stable for cookbook rendering.\n",
    "    \"\"\"\n",
    "    root: Path\n",
    "\n",
    "    def run_dir(self, test_id: str, model_label: str) -> Path:\n",
    "        # Ignore test/model subfolders; write everything to the root.\n",
    "        self.root.mkdir(parents=True, exist_ok=True)\n",
    "        return self.root\n",
    "\n",
    "    def new_basename(self, prefix: str) -> str:\n",
    "        created_ms = int(time.time() * 1000)\n",
    "        return f\"{prefix}_{created_ms}\"\n",
    "\n",
    "    def save_png(self, run_dir: Path, basename: str, idx: int, png_bytes: bytes) -> Path:\n",
    "        out = run_dir / f\"{basename}_{idx}.png\"\n",
    "        out.write_bytes(png_bytes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b6c49",
   "metadata": {},
   "source": [
    "### vision_harness/sweeps.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6d06d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from itertools import product\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def grid_sweep(\n",
    "    *,\n",
    "    base_label: str,\n",
    "    task_type: TaskType,\n",
    "    fixed: dict[str, Any],\n",
    "    grid: dict[str, list[Any]],\n",
    ") -> list[ModelRun]:\n",
    "    keys = list(grid.keys())\n",
    "    runs: list[ModelRun] = []\n",
    "\n",
    "    for values in product(*[grid[k] for k in keys]):\n",
    "        params = dict(fixed)\n",
    "        label_parts = [base_label]\n",
    "        for k, v in zip(keys, values):\n",
    "            params[k] = v\n",
    "            label_parts.append(f\"{k}={v}\")\n",
    "        runs.append(ModelRun(label=\",\".join(label_parts), task_type=task_type, params=params))\n",
    "\n",
    "    return runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a4099",
   "metadata": {},
   "source": [
    "### vision_harness/runners.py\n",
    "\n",
    "Two runners: one for **generation**, one for **editing**. Both return a\n",
    "normalized `ModelResponse` containing saved output images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43214dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import base64\n",
    "from contextlib import ExitStack\n",
    "from typing import Optional\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def _mime_for_path(path) -> str:\n",
    "    suffix = str(path).lower()\n",
    "    if suffix.endswith(\".png\"):\n",
    "        return \"image/png\"\n",
    "    if suffix.endswith(\".jpg\") or suffix.endswith(\".jpeg\"):\n",
    "        return \"image/jpeg\"\n",
    "    if suffix.endswith(\".webp\"):\n",
    "        return \"image/webp\"\n",
    "    return \"application/octet-stream\"\n",
    "\n",
    "\n",
    "def _extract_b64_items(images_response) -> list[str]:\n",
    "    b64_items: list[str] = []\n",
    "    for item in getattr(images_response, \"data\", []) or []:\n",
    "        b64 = getattr(item, \"b64_json\", None)\n",
    "        if b64:\n",
    "            b64_items.append(b64)\n",
    "    return b64_items\n",
    "\n",
    "\n",
    "class ImageGenerationRunner:\n",
    "    \"\"\"Text-to-image runner.\"\"\"\n",
    "\n",
    "    def __init__(self, client: Optional[OpenAI] = None):\n",
    "        self.client = client or OpenAI()\n",
    "\n",
    "    def run(self, case: TestCase, run_cfg: ModelRun, store: OutputStore) -> ModelResponse:\n",
    "        assert case.task_type == \"image_generation\"\n",
    "        assert run_cfg.task_type == \"image_generation\"\n",
    "\n",
    "        params = dict(run_cfg.params)\n",
    "        model = params.pop(\"model\")\n",
    "        n = int(params.pop(\"n\", 1))\n",
    "\n",
    "        run_dir = store.run_dir(case.id, run_cfg.label)\n",
    "        basename = store.new_basename(f\"gen_{case.id}_{run_cfg.label}\")\n",
    "\n",
    "        images_response = self.client.images.generate(\n",
    "            model=model,\n",
    "            prompt=case.prompt,\n",
    "            n=n,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        artifacts: list[Artifact] = []\n",
    "        for idx, b64_json in enumerate(_extract_b64_items(images_response)):\n",
    "            png_bytes = base64.b64decode(b64_json)\n",
    "            out_path = store.save_png(run_dir, basename, idx, png_bytes)\n",
    "            artifacts.append(Artifact(kind=\"image\", path=out_path))\n",
    "\n",
    "        return ModelResponse(artifacts=artifacts, raw={\"model\": model, \"params\": run_cfg.params})\n",
    "\n",
    "\n",
    "class ImageEditRunner:\n",
    "    \"\"\"Image editing runner (reference image(s) + optional mask).\"\"\"\n",
    "\n",
    "    def __init__(self, client: Optional[OpenAI] = None):\n",
    "        self.client = client or OpenAI()\n",
    "\n",
    "    def run(self, case: TestCase, run_cfg: ModelRun, store: OutputStore) -> ModelResponse:\n",
    "        assert case.task_type == \"image_editing\"\n",
    "        assert run_cfg.task_type == \"image_editing\"\n",
    "        assert case.image_inputs is not None and case.image_inputs.image_paths\n",
    "\n",
    "        params = dict(run_cfg.params)\n",
    "        model = params.pop(\"model\")\n",
    "        n = int(params.pop(\"n\", 1))\n",
    "\n",
    "        run_dir = store.run_dir(case.id, run_cfg.label)\n",
    "        basename = store.new_basename(f\"edit_{case.id}_{run_cfg.label}\")\n",
    "\n",
    "        with ExitStack() as stack:\n",
    "            image_files = []\n",
    "            for p in case.image_inputs.image_paths:\n",
    "                f = stack.enter_context(p.open(\"rb\"))\n",
    "                image_files.append((p.name, f, _mime_for_path(p)))\n",
    "\n",
    "            mask_file = None\n",
    "            if case.image_inputs.mask_path:\n",
    "                mf = stack.enter_context(case.image_inputs.mask_path.open(\"rb\"))\n",
    "                mask_file = (case.image_inputs.mask_path.name, mf, _mime_for_path(case.image_inputs.mask_path))\n",
    "\n",
    "            edit_kwargs = dict(\n",
    "                model=model,\n",
    "                prompt=case.prompt,\n",
    "                image=image_files,\n",
    "                n=n,\n",
    "                **params,\n",
    "            )\n",
    "            if mask_file is not None:\n",
    "                edit_kwargs[\"mask\"] = mask_file\n",
    "\n",
    "            images_response = self.client.images.edit(**edit_kwargs)\n",
    "\n",
    "        artifacts: list[Artifact] = []\n",
    "        for idx, b64_json in enumerate(_extract_b64_items(images_response)):\n",
    "            png_bytes = base64.b64decode(b64_json)\n",
    "            out_path = store.save_png(run_dir, basename, idx, png_bytes)\n",
    "            artifacts.append(Artifact(kind=\"image\", path=out_path))\n",
    "\n",
    "        return ModelResponse(artifacts=artifacts, raw={\"model\": model, \"params\": run_cfg.params})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84cbfa",
   "metadata": {},
   "source": [
    "### vision_harness/graders.py\n",
    "\n",
    "A clean grader interface + a reusable **LLM-as-judge** grader that can\n",
    "be used for both generation and editing by changing how you build the\n",
    "judge inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "279428a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional, Protocol\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class Grader(Protocol):\n",
    "    key: str\n",
    "\n",
    "    def grade(self, response: ModelResponse, case: TestCase) -> Score | list[Score]: ...\n",
    "\n",
    "\n",
    "def pick_first_image(response: ModelResponse) -> Optional[Path]:\n",
    "    for artifact in response.artifacts:\n",
    "        if artifact.kind == \"image\":\n",
    "            return artifact.path\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_generation_judge_content(case: TestCase, output_image: Path) -> list[dict]:\n",
    "    return [\n",
    "        {\n",
    "            \"type\": \"input_text\",\n",
    "            \"text\": f\"Prompt:\\n{case.prompt}\\n\\nCriteria:\\n{case.criteria}\",\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"input_image\",\n",
    "            \"image_url\": image_to_data_url(output_image),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_editing_judge_content(case: TestCase, output_image: Path) -> list[dict]:\n",
    "    assert case.image_inputs is not None and case.image_inputs.image_paths\n",
    "    content: list[dict] = [\n",
    "        {\n",
    "            \"type\": \"input_text\",\n",
    "            \"text\": f\"Edit instruction:\\n{case.prompt}\\n\\nCriteria:\\n{case.criteria}\",\n",
    "        }\n",
    "    ]\n",
    "    for image_path in case.image_inputs.image_paths:\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"input_image\",\n",
    "                \"image_url\": image_to_data_url(image_path),\n",
    "            }\n",
    "        )\n",
    "    if case.image_inputs.mask_path:\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"input_image\",\n",
    "                \"image_url\": image_to_data_url(case.image_inputs.mask_path),\n",
    "            }\n",
    "        )\n",
    "    content.append(\n",
    "        {\n",
    "            \"type\": \"input_image\",\n",
    "            \"image_url\": image_to_data_url(output_image),\n",
    "        }\n",
    "    )\n",
    "    return content\n",
    "\n",
    "\n",
    "def default_schema() -> dict:\n",
    "    return {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"pass\": {\"type\": \"boolean\"},\n",
    "            \"reason\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"pass\", \"reason\"],\n",
    "        \"additionalProperties\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMajRubricGrader:\n",
    "    \"\"\"\n",
    "    Reusable vision LLM-as-judge grader.\n",
    "    - Provide a system prompt + a content_builder for generation/editing.\n",
    "    - Optionally provide a custom JSON schema and parser.\n",
    "    \"\"\"\n",
    "\n",
    "    key: str\n",
    "    system_prompt: str\n",
    "    content_builder: Callable[[TestCase, Path], list[dict]]\n",
    "    judge_model: str = \"gpt-5.2\"\n",
    "    client: Optional[OpenAI] = None\n",
    "\n",
    "    json_schema_name: str = \"vision_eval_result\"\n",
    "    json_schema: dict = field(default_factory=default_schema)\n",
    "    result_parser: Optional[Callable[[dict, str], Score | list[Score]]] = None\n",
    "\n",
    "    def _parse_result(self, data: dict) -> Score | list[Score]:\n",
    "        if self.result_parser:\n",
    "            return self.result_parser(data, self.key)\n",
    "        return Score(\n",
    "            key=self.key,\n",
    "            value=bool(data.get(\"pass\", False)),\n",
    "            reason=(data.get(\"reason\") or \"\").strip(),\n",
    "            tags=data.get(\"tags\") or None,\n",
    "        )\n",
    "\n",
    "    def grade(self, response: ModelResponse, case: TestCase) -> Score | list[Score]:\n",
    "        output_image = pick_first_image(response)\n",
    "        if not output_image:\n",
    "            return Score(key=self.key, value=False, reason=\"No output image artifact found\")\n",
    "\n",
    "        client = self.client or OpenAI()\n",
    "        content = self.content_builder(case, output_image)\n",
    "\n",
    "        completion = client.responses.create(\n",
    "            model=self.judge_model,\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": content},\n",
    "            ],\n",
    "            text={\n",
    "                \"format\": {\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"name\": self.json_schema_name,\n",
    "                    \"schema\": self.json_schema,\n",
    "                    \"strict\": True,\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "\n",
    "        data = json.loads(completion.output_text)\n",
    "        return self._parse_result(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc922df2",
   "metadata": {},
   "source": [
    "### vision_harness/evaluate.py\n",
    "\n",
    "A simple evaluation loop that returns plain Python data (no “eval row”\n",
    "class). Later sections can write their own reporting/CSV utilities on\n",
    "top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75d024c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def _as_score_list(score_or_scores: Score | list[Score]) -> list[Score]:\n",
    "    if isinstance(score_or_scores, list):\n",
    "        return score_or_scores\n",
    "    return [score_or_scores]\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    *,\n",
    "    cases: list[TestCase],\n",
    "    model_runs: list[ModelRun],\n",
    "    graders: list[Grader],\n",
    "    output_store: OutputStore,\n",
    ") -> list[dict[str, Any]]:\n",
    "    gen_runner = ImageGenerationRunner()\n",
    "    edit_runner = ImageEditRunner()\n",
    "\n",
    "    results: list[dict[str, Any]] = []\n",
    "\n",
    "    for case in cases:\n",
    "        for run_cfg in model_runs:\n",
    "            if run_cfg.task_type != case.task_type:\n",
    "                continue\n",
    "\n",
    "            if case.task_type == \"image_generation\":\n",
    "                response = gen_runner.run(case, run_cfg, output_store)\n",
    "            elif case.task_type == \"image_editing\":\n",
    "                response = edit_runner.run(case, run_cfg, output_store)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown task_type: {case.task_type}\")\n",
    "\n",
    "            score_map: dict[str, Any] = {}\n",
    "            reason_map: dict[str, str] = {}\n",
    "\n",
    "            for grader in graders:\n",
    "                scored = grader.grade(response, case)\n",
    "                for score in _as_score_list(scored):\n",
    "                    score_map[score.key] = score.value\n",
    "                    reason_map[score.key] = score.reason\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"test_id\": case.id,\n",
    "                    \"model_label\": run_cfg.label,\n",
    "                    \"task_type\": case.task_type,\n",
    "                    \"artifact_paths\": [str(a.path) for a in response.artifacts],\n",
    "                    \"scores\": score_map,\n",
    "                    \"reasons\": reason_map,\n",
    "                    \"run_params\": run_cfg.params,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8540a",
   "metadata": {},
   "source": [
    "### Result Table Rendering Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f795655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from html import escape\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def summarize_scores(scores: dict[str, object]) -> str:\n",
    "    return os.linesep.join(f\"{k}: {scores[k]}\" for k in sorted(scores.keys()))\n",
    "\n",
    "\n",
    "def summarize_reasons(reasons: dict[str, str]) -> str:\n",
    "    verdict_reason = (reasons.get(\"verdict\") or \"\").strip()\n",
    "    if verdict_reason:\n",
    "        return verdict_reason\n",
    "    return os.linesep.join(f\"{k}: {reasons[k]}\" for k in sorted(reasons.keys()) if reasons[k])\n",
    "\n",
    "\n",
    "def _pre(text: str) -> str:\n",
    "    return (\n",
    "        \"<pre style='white-space:pre-wrap; word-break:break-word; margin:0'>\"\n",
    "        f\"{escape(text)}\"\n",
    "        \"</pre>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def render_result_table(\n",
    "    *,\n",
    "    case: TestCase,\n",
    "    result: dict[str, object],\n",
    "    title: str,\n",
    ") -> None:\n",
    "    sep = os.linesep\n",
    "    prompt_text = f\"{case.prompt}{sep}{sep}Criteria:{sep}{case.criteria}\"\n",
    "    scores = result[\"scores\"]\n",
    "    reasons = result[\"reasons\"]\n",
    "\n",
    "    prompt_html = _pre(prompt_text)\n",
    "    scores_html = _pre(summarize_scores(scores))\n",
    "    reasons_html = _pre(summarize_reasons(reasons))\n",
    "\n",
    "    table_html = (\n",
    "        \"<table style='width:100%; table-layout:fixed; border-collapse:collapse;'>\"\n",
    "        \"<colgroup>\"\n",
    "        \"<col style='width:33%'>\"\n",
    "        \"<col style='width:33%'>\"\n",
    "        \"<col style='width:33%'>\"\n",
    "        \"</colgroup>\"\n",
    "        \"<thead><tr>\"\n",
    "        \"<th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Input Prompt</th>\"\n",
    "        \"<th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Scores</th>\"\n",
    "        \"<th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Reasoning</th>\"\n",
    "        \"</tr></thead>\"\n",
    "        \"<tbody><tr>\"\n",
    "        f\"<td style='text-align:left; padding:8px; vertical-align:top'>{prompt_html}</td>\"\n",
    "        f\"<td style='text-align:left; padding:8px; vertical-align:top'>{scores_html}</td>\"\n",
    "        f\"<td style='text-align:left; padding:8px; vertical-align:top'>{reasons_html}</td>\"\n",
    "        \"</tr></tbody></table>\"\n",
    "    )\n",
    "\n",
    "    display(HTML(f\"<div style='font-weight:600; margin:6px 0'>{escape(title)}</div>\"))\n",
    "    display(HTML(table_html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63d21e",
   "metadata": {},
   "source": [
    "Later cookbook sections will only need to provide:\n",
    "\n",
    "1.  A list of `TestCase` objects for the use case (UI mockups, marketing\n",
    "    graphics, virtual try-on, logo edits, etc.)  \n",
    "2.  A set of `ModelRun`s (single run or a sweep with `grid_sweep`)  \n",
    "3.  A set of graders, typically:\n",
    "    - **Gating graders** (pass/fail): non-negotiables ex: instruction\n",
    "      following, text correctness, locality/preservation  \n",
    "    - **Graded metrics** (0–5): quality dimensions ex:\n",
    "      realism/usability, layout/hierarchy, artifact severity  \n",
    "    - Optional: human rubric labels and pairwise prefs (stored outside\n",
    "      the harness)\n",
    "\n",
    "This keeps the rest of the cookbook focused on **what to measure** per\n",
    "use case—without rewriting harness plumbing each time.\n",
    "\n",
    "## Image Generation Evals\n",
    "\n",
    "**Use Case Ideas: UI mockups, marketing graphics/posters**\n",
    "\n",
    "**Goal: evaluate text-to-image quality, controllability, and usefulness\n",
    "for real prompts**  \n",
    "Covers:\n",
    "\n",
    "- **instruction following** (constraints satisfied),  \n",
    "- **Text rendering** (generated text is accurate, legible, and placed\n",
    "  correctly),  \n",
    "- **Styling** (requested aesthetic + visual quality, may include brand,\n",
    "  character, and product consistency)  \n",
    "- **Human feedback alignment** (rubric-based labels + pairwise prefs).\n",
    "\n",
    "Image generation models are used to create artifacts that influence real\n",
    "work: UI mockups, product designs, marketing graphics, and internal\n",
    "presentations. In these contexts, images are not judged purely on visual\n",
    "appeal. They are judged on whether they communicate intent, follow\n",
    "constraints, and are usable by downstream stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb09acfe",
   "metadata": {},
   "source": [
    "### UI Mockups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53370158",
   "metadata": {},
   "source": [
    "UI mockups generated by image models are increasingly used for early\n",
    "product exploration, design ideation, and internal reviews. In these\n",
    "workflows, mockups are not just visual inspiration. They are\n",
    "communication tools that help designers, engineers, and stakeholders\n",
    "reason about layout, hierarchy, interaction intent, and feature scope\n",
    "before anything is built. As a result, success is defined less by\n",
    "aesthetic taste and more by whether the output can plausibly function as\n",
    "a product artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec14d3",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8f0e6",
   "metadata": {},
   "source": [
    "<img src=\"../../images/mockup_sample.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69bf1e5",
   "metadata": {},
   "source": [
    "This section shows how to evaluate a generated UI mockup using a screen\n",
    "level example. The goal is to evaluate whether the generated UI consists\n",
    "of a coherent layout, uses recognizable components, and represents a\n",
    "plausible product experience.\n",
    "\n",
    "**Why UI mockup evals are different**\n",
    "\n",
    "UI mockups combine several difficult evaluation dimensions:\n",
    "\n",
    "1.  **Component Fidelity (gating)**  \n",
    "    The generated image must clearly depict the requested screen type\n",
    "    and state. Buttons should look clickable, inputs should look\n",
    "    editable, and navigation should look like navigation.  \n",
    "2.  **Layout Realization (graded)**  \n",
    "    A good UI mockup communicates instantly what the user can do and\n",
    "    what matters most. The layout should make primary actions obvious,\n",
    "    secondary actions clearly subordinate, and information grouped in a\n",
    "    way that reflects real interaction flow  \n",
    "3.  **In-Image Text Rendering (gating)**  \n",
    "    UI text encodes functionality. Labels, headings, and calls to action\n",
    "    must be legible and correctly rendered.\n",
    "\n",
    "**Example task: mobile checkout screen generation**\n",
    "\n",
    "- Scenario: a consumer ecommerce app needs a mobile checkout screen to\n",
    "  review an order and complete payment.\n",
    "- Prompt guidance: be explicit about screen type, platform, required UI\n",
    "  elements, exact button/link text, hierarchy constraints, and\n",
    "  disallowed extras.\n",
    "- Implementation: the full prompt and criteria live in the harness\n",
    "  setup code cell below (`ui_prompt` and `ui_criteria`).\n",
    "\n",
    "#### What to evaluate (practical metrics):\n",
    "\n",
    "##### 1) Instruction Following — Pass / Fail (gate)\n",
    "\n",
    "Most UI mockup evals start with instruction fidelity. At a basic level,\n",
    "the question is simple: did the model generate the screen that was asked\n",
    "for? If the prompt specifies a mobile checkout screen, the output should\n",
    "look like a mobile checkout screen, not a generic landing page. Required\n",
    "sections, states, or constraints should be present, and the overall\n",
    "structure should match the intent of the request. This dimension is\n",
    "often the most heavily weighted, because if the model misses the core\n",
    "ask, the rest of the output does not matter. Once a UI mockup satisfies\n",
    "the basic instruction, evaluation shifts to whether the screen is usable\n",
    "and coherent as a product artifact.\n",
    "\n",
    "**PASS if**\n",
    "\n",
    "- The correct screen type and platform context are present.  \n",
    "- All required sections, states, or constraints are included.\n",
    "\n",
    "**FAIL if**\n",
    "\n",
    "- Missing any required components or the output alters the UI’s purpose.\n",
    "\n",
    "##### 2) Layout and Hierarchy — 0–5\n",
    "\n",
    "Measures whether the screen communicates clearly at a glance.\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "- Clear visual hierarchy between primary and secondary actions  \n",
    "- Consistent alignment and spacing  \n",
    "- Logical grouping of related elements\n",
    "\n",
    "##### 3) Text rendering and legibility — Pass / Fail (gate)\n",
    "\n",
    "Labels, headings, and calls to action need to be readable and\n",
    "unambiguous.\n",
    "\n",
    "**PASS if**\n",
    "\n",
    "- Text is readable, correctly spelled, and sensibly labeled.  \n",
    "- Font sizes reflect hierarchy (headings vs labels vs helper text).\n",
    "\n",
    "**FAIL if**\n",
    "\n",
    "- Any critical text is unreadable, cut off, misspelled, or distorted.\n",
    "\n",
    "##### 4) UI Realism and Usability — *0–5*\n",
    "\n",
    "Measures whether the mockup resembles a plausible product interface.\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "- Inputs look editable / Buttons look clickable  \n",
    "- Navigation looks like navigation  \n",
    "- Interactive elements are visually distinct from static content\n",
    "\n",
    "##### 5) Human feedback (quick and high leverage)\n",
    "\n",
    "UI \"usability clarity\" is where humans add the most value. Keep it lightweight and focused on interaction intent.\n",
    "\n",
    "- Rubric labels (fast):\n",
    "  - Would you use this mockup to iterate on a real product screen? (Y/N)\n",
    "  - If N, why? (missing section, action button unclear, text unreadable, layout confusing, etc.)\n",
    "  - Overall usability clarity: 1–5\n",
    "- Common failure tags (for debugging + iteration):\n",
    "  - `wrong_screen_type`\n",
    "  - `missing_required_section`\n",
    "  - `extra_ui_elements`\n",
    "  - `primary_cta_not_clear`\n",
    "  - `text_unreadable_or_garbled`\n",
    "  - `affordances_unclear`\n",
    "  - `layout_confusing`\n",
    "\n",
    "**Verdict rules (how to convert metrics into a single pass/fail)**\n",
    "\n",
    "- **Instruction Following must PASS**  \n",
    "- **Layout and Hierarchy ≥ 3**  \n",
    "- **Text Rendering must PASS**  \n",
    "- **UI Realism ≥ 3**\n",
    "\n",
    "If any rule fails → overall **FAIL**.\n",
    "\n",
    "**Example `TestCase` set (small but high-signal)**\n",
    "\n",
    "Start with a few cases that cover common UI mockup variants and edge\n",
    "cases.\n",
    "\n",
    "1.  **Mobile** **checkout screen** (the “checkout UI mockup” prompt\n",
    "    above)  \n",
    "2.  **Minimal layout** (header + order total + primary CTA only) - tests\n",
    "    layout and hierarchy with sparse elements.  \n",
    "3.  **Dense hierarchy** (order summary + payment method + promo row +\n",
    "    taxes + two secondary actions) - tests hierarchy under information\n",
    "    load.  \n",
    "4.  **Exact CTA text** (primary button text must be exactly “Place\n",
    "    Order”) - tests in-image text rendering fidelity.  \n",
    "5.  **Secondary action presence (i**nclude both “Edit Cart” and “Change\n",
    "    payment method”) - tests secondary actions remain visually\n",
    "    subordinate.  \n",
    "6.  **Placement constraint** (require order total directly above the\n",
    "    primary CTA)  \n",
    "7.  **Platform fidelity** (mobile vs desktop version) - tests screen\n",
    "    framing and platform cues.\n",
    "\n",
    "**LLM-as-judge rubric prompt**\n",
    "\n",
    "Below is a judge prompt aligned to your existing `LLMajRubricGrader`. It\n",
    "returns structured metric scores + an overall verdict.\n",
    "\n",
    "You can use this with your existing grader by changing the JSON schema\n",
    "to include the fields below (or create separate graders per metric if\n",
    "you prefer).\n",
    "\n",
    "#### System Prompt (UI mockup judge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"<core_mission>\n",
    "Evaluate whether a generated UI mockup image represents a usable mobile checkout screen by checking screen type fidelity, layout/hierarchy, in-image text rendering, and UI affordance clarity.\n",
    "</core_mission>\n",
    "\n",
    "<role>\n",
    "You are an expert evaluator of UI mockups used by designers and engineers. You care about structural correctness, readable UI text, clear hierarchy, and realistic rendering of UI elements.\n",
    "</role>\n",
    "\n",
    "<scope_constraints>\n",
    "- Judge only against the provided instructions.\n",
    "- Be strict about required UI elements and exact button/link text.\n",
    "- Do NOT infer intent beyond what is explicitly stated.\n",
    "- Do NOT reward creativity that violates constraints.\n",
    "- Missing or extra required components are serious errors.\n",
    "- If the UI intent or function is unclear, score conservatively.\n",
    "</scope_constraints>\n",
    "\n",
    "<metrics>\n",
    "1) instruction_following: PASS/FAIL\n",
    "2) layout_hierarchy: 0–5\n",
    "3) in_image_text_rendering: PASS/FAIL\n",
    "4) ui_affordance_rendering: 0–5\n",
    "</metrics>\n",
    "\n",
    "Evaluate EACH metric independently using the definitions below.\n",
    "--------------------------------\n",
    "1) Instruction Following (PASS / FAIL)\n",
    "--------------------------------\n",
    "PASS if:\n",
    "- All required components are present.\n",
    "- No unrequested components or features are added.\n",
    "- The screen matches the requested type and product context.\n",
    "\n",
    "FAIL if:\n",
    "- Any required component is missing.\n",
    "- Any unrequested component materially alters the UI.\n",
    "- The screen does not match the requested type.\n",
    "\n",
    "--------------------------------\n",
    "2) Layout and Hierarchy (0–5)\n",
    "--------------------------------\n",
    "5: Layout is clear, coherent, and immediately usable.\n",
    "   Hierarchy, grouping, spacing, and alignment are strong.\n",
    "\n",
    "3: Generally understandable, but one notable hierarchy or layout issue\n",
    "   that would require iteration.\n",
    "\n",
    "0-2: Layout problems materially hinder usability or comprehension.\n",
    "\n",
    "--------------------------------\n",
    "3) In-Image Text Rendering (PASS / FAIL)\n",
    "--------------------------------\n",
    "PASS if:\n",
    "- Text is readable, correctly spelled, and sensibly labeled.\n",
    "- Font sizes reflect hierarchy (headings vs labels vs helper text).\n",
    "\n",
    "FAIL if:\n",
    "- Any critical text is unreadable, cut off, misspelled, or distorted.\n",
    "\n",
    "--------------------------------\n",
    "4) UI Affordance Rendering (0–5)\n",
    "--------------------------------\n",
    "5: Clearly resembles a real product interface that designers could use.\n",
    "\n",
    "3: Marginally plausible; intent is visible but execution is weak.\n",
    "\n",
    "0-2: Poor realism; interface would be difficult to use in practice.\n",
    "\n",
    "<verdict_rules>\n",
    "- Instruction Following must PASS.\n",
    "- In-Image Text Rendering must PASS.\n",
    "- Layout and Hierarchy score must be ≥ 3.\n",
    "- UI Affordance Rendering score must be ≥ 3.\n",
    "\n",
    "If ANY rule fails, the overall verdict is FAIL.\n",
    "Do not average scores to determine the verdict.\n",
    "</verdict_rules>\n",
    "\n",
    "<output_constraints>\n",
    "Return JSON only.\n",
    "No extra text.\n",
    "</output_constraints>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd0f96a",
   "metadata": {},
   "source": [
    "#### Recommended JSON Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "539bd39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verdict': 'PASS',\n",
       " 'instruction_following': True,\n",
       " 'layout_hierarchy': 3,\n",
       " 'in_image_text_rendering': True,\n",
       " 'ui_affordance_rendering': 4,\n",
       " 'reason': '...'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"verdict\": \"PASS\",\n",
    "  \"instruction_following\": True,\n",
    "  \"layout_hierarchy\": 3,\n",
    "  \"in_image_text_rendering\": True,\n",
    "  \"ui_affordance_rendering\": 4,\n",
    "  \"reason\": \"...\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38588944",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### UI Mockup Example: Harness Setup\n",
    "\n",
    "Define a UI mockup test case, a model run, and an output store under `images/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd7e4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the UI judge prompt before PROMPT is overwritten later.\n",
    "ui_judge_prompt = PROMPT\n",
    "\n",
    "ui_prompt = \"\"\"Generate a high-fidelity mobile checkout screen for an ecommerce app.\n",
    "Orientation: portrait.\n",
    "Screen type: checkout / order review.\n",
    "Use the REQUIRED TEXT:\n",
    "- Checkout\n",
    "- Place Order\n",
    "- Edit Cart\n",
    "Constraints:\n",
    "- Order total appears directly above the primary CTA.\n",
    "- Primary CTA is the most visually prominent element.\n",
    "- Do not include popups, ads, marketing copy, or extra screens.\n",
    "- Do not include placeholder or lorem ipsum text.\n",
    "\"\"\"\n",
    "\n",
    "ui_criteria = \"\"\"The image clearly depicts a mobile checkout screen.\n",
    "All required sections are present and visually distinct.\n",
    "UI elements look clickable/editable and follow common conventions.\n",
    "Primary vs secondary actions are unambiguous.\n",
    "No extra UI states, decorative noise, or placeholder text.\"\"\"\n",
    "\n",
    "ui_case = TestCase(\n",
    "    id=\"ui_checkout_mockup\",\n",
    "    task_type=\"image_generation\",\n",
    "    prompt=ui_prompt,\n",
    "    criteria=ui_criteria,\n",
    ")\n",
    "\n",
    "ui_run = ModelRun(\n",
    "    label=\"gpt-image-1.5-ui\",\n",
    "    task_type=\"image_generation\",\n",
    "    params={\n",
    "        \"model\": \"gpt-image-1.5\",\n",
    "        \"n\": 1,\n",
    "        \"size\": \"1024x1024\",\n",
    "    },\n",
    ")\n",
    "\n",
    "ui_store = OutputStore(root=Path(\"../../images\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6acdb77",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### UI Mockup Example: Run And Grade\n",
    "\n",
    "Run the harness and grade the UI mockup using the UI judge rubric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd531cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_id': 'ui_checkout_mockup',\n",
       " 'model_label': 'gpt-image-1.5-ui',\n",
       " 'task_type': 'image_generation',\n",
       " 'artifact_paths': ['../../images/gen_ui_checkout_mockup_gpt-image-1.5-ui_1769656414221_0.png'],\n",
       " 'scores': {'instruction_following': True,\n",
       "  'layout_hierarchy': 5.0,\n",
       "  'in_image_text_rendering': True,\n",
       "  'ui_affordance_rendering': 5.0,\n",
       "  'verdict': 'PASS'},\n",
       " 'reasons': {'instruction_following': '',\n",
       "  'layout_hierarchy': '',\n",
       "  'in_image_text_rendering': '',\n",
       "  'ui_affordance_rendering': '',\n",
       "  'verdict': 'All required text is present (\"Checkout\", \"Edit Cart\", \"Place Order\"). Screen clearly matches a mobile checkout/order review with distinct sections (Shipping Information, Payment Method, Items, Order Summary). Order Total appears directly above the primary CTA, and the \"Place Order\" button is the most visually prominent element. No popups, ads, marketing copy, or lorem/placeholder text. Text is crisp and readable with appropriate hierarchy. UI elements (back arrow, Edit Cart link, Change links, primary CTA) have clear, realistic affordances.'},\n",
       " 'run_params': {'model': 'gpt-image-1.5', 'n': 1, 'size': '1024x1024'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ui_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"verdict\": {\"type\": \"string\"},\n",
    "        \"instruction_following\": {\"type\": \"boolean\"},\n",
    "        \"layout_hierarchy\": {\"type\": \"number\"},\n",
    "        \"in_image_text_rendering\": {\"type\": \"boolean\"},\n",
    "        \"ui_affordance_rendering\": {\"type\": \"number\"},\n",
    "        \"reason\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"verdict\",\n",
    "        \"instruction_following\",\n",
    "        \"layout_hierarchy\",\n",
    "        \"in_image_text_rendering\",\n",
    "        \"ui_affordance_rendering\",\n",
    "        \"reason\",\n",
    "    ],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def parse_ui_result(data: dict, base_key: str) -> list[Score]:\n",
    "    return [\n",
    "        Score(key=\"instruction_following\", value=bool(data[\"instruction_following\"]), reason=\"\"),\n",
    "        Score(key=\"layout_hierarchy\", value=float(data[\"layout_hierarchy\"]), reason=\"\"),\n",
    "        Score(key=\"in_image_text_rendering\", value=bool(data[\"in_image_text_rendering\"]), reason=\"\"),\n",
    "        Score(key=\"ui_affordance_rendering\", value=float(data[\"ui_affordance_rendering\"]), reason=\"\"),\n",
    "        Score(key=\"verdict\", value=str(data[\"verdict\"]), reason=(data.get(\"reason\") or \"\").strip()),\n",
    "    ]\n",
    "\n",
    "ui_grader = LLMajRubricGrader(\n",
    "    key=\"ui_eval\",\n",
    "    system_prompt=ui_judge_prompt,\n",
    "    content_builder=build_generation_judge_content,\n",
    "    judge_model=\"gpt-5.2\",\n",
    "    json_schema_name=\"ui_mockup_eval\",\n",
    "    json_schema=ui_schema,\n",
    "    result_parser=parse_ui_result,\n",
    ")\n",
    "\n",
    "ui_results = evaluate(\n",
    "    cases=[ui_case],\n",
    "    model_runs=[ui_run],\n",
    "    graders=[ui_grader],\n",
    "    output_store=ui_store,\n",
    ")\n",
    "\n",
    "ui_result = ui_results[0]\n",
    "ui_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799bd0d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### UI Mockup Results\n",
    "\n",
    "Show the prompt, generated image, and scores in a single pandas table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf15b4c",
   "metadata": {},
   "source": [
    "<img src=\"../../images/gen_ui_checkout_mockup_gpt-image-1.5-ui_1769656414221_0.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bd44f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='font-weight:600; margin:6px 0'>UI Mockup: Prompt vs. Scores</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table style='width:100%; table-layout:fixed; border-collapse:collapse;'><colgroup><col style='width:33%'><col style='width:33%'><col style='width:33%'></colgroup><thead><tr><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Input Prompt</th><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Scores</th><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Reasoning</th></tr></thead><tbody><tr><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>Generate a high-fidelity mobile checkout screen for an ecommerce app.\n",
       "Orientation: portrait.\n",
       "Screen type: checkout / order review.\n",
       "Use the REQUIRED TEXT:\n",
       "- Checkout\n",
       "- Place Order\n",
       "- Edit Cart\n",
       "Constraints:\n",
       "- Order total appears directly above the primary CTA.\n",
       "- Primary CTA is the most visually prominent element.\n",
       "- Do not include popups, ads, marketing copy, or extra screens.\n",
       "- Do not include placeholder or lorem ipsum text.\n",
       "\n",
       "\n",
       "Criteria:\n",
       "The image clearly depicts a mobile checkout screen.\n",
       "All required sections are present and visually distinct.\n",
       "UI elements look clickable/editable and follow common conventions.\n",
       "Primary vs secondary actions are unambiguous.\n",
       "No extra UI states, decorative noise, or placeholder text.</pre></td><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>in_image_text_rendering: True\n",
       "instruction_following: True\n",
       "layout_hierarchy: 5.0\n",
       "ui_affordance_rendering: 5.0\n",
       "verdict: PASS</pre></td><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>All required text is present (&quot;Checkout&quot;, &quot;Edit Cart&quot;, &quot;Place Order&quot;). Screen clearly matches a mobile checkout/order review with distinct sections (Shipping Information, Payment Method, Items, Order Summary). Order Total appears directly above the primary CTA, and the &quot;Place Order&quot; button is the most visually prominent element. No popups, ads, marketing copy, or lorem/placeholder text. Text is crisp and readable with appropriate hierarchy. UI elements (back arrow, Edit Cart link, Change links, primary CTA) have clear, realistic affordances.</pre></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_result_table(case=ui_case, result=ui_result, title=\"UI Mockup: Prompt vs. Scores\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545eac5",
   "metadata": {},
   "source": [
    "### Marketing Graphics Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aaff99",
   "metadata": {},
   "source": [
    "Marketing graphics are a “high-stakes text-to-image” workflow: the\n",
    "output is meant to ship (or at least be reviewed as if it could ship).\n",
    "A flyer can look “nice” but still fail if the copy is wrong, the offer\n",
    "is unclear, or the layout hides the key message.\n",
    "\n",
    "This section shows how to evaluate **flyer generation** with a coffee\n",
    "shop example. The goal is to make evaluation workflow-relevant: can a\n",
    "marketer or designer use this output with minimal edits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224dabe9",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090300d",
   "metadata": {},
   "source": [
    "<img src=\"../../images/sample_flyer.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fef54",
   "metadata": {},
   "source": [
    "**Why marketing graphics evals are different**\n",
    "\n",
    "Marketing images combine three hard problem types:\n",
    "\n",
    "1.  Exact text requirements (gating)\n",
    "2.  Layout + hierarchy (graded)\n",
    "3.  Style + brand consistency (graded + optional human prefs)\n",
    "\n",
    "**Example task: Coffee shop flyer generation**\n",
    "\n",
    "- Scenario: a local coffee shop needs a promotional flyer for a\n",
    "  limited-time drink and a weekday deal.\n",
    "- Prompt guidance: be explicit about the deliverable, required copy,\n",
    "  hierarchy, and disallowed extras.\n",
    "- Implementation: the full prompt and criteria live in the harness\n",
    "  setup code cell below (`coffee_generation_prompt` and\n",
    "  `coffee_criteria`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01668cba",
   "metadata": {},
   "source": [
    "#### What to evaluate (practical metrics):\n",
    "\n",
    "Use a mix of gates (hard constraints) and graded signals (quality).\n",
    "\n",
    "##### 1) Instruction following — Pass / Fail (gate)\n",
    "\n",
    "Marketing assets fail fast if they don’t match the spec.\n",
    "\n",
    "**PASS if**\n",
    "\n",
    "- Correct deliverable type: clearly a flyer/poster layout (not a UI screen, not a random photo).\n",
    "- Required components exist (headline/subheadline/offer/CTA/footer + hero image).\n",
    "\n",
    "**FAIL if**\n",
    "\n",
    "- Missing any required component or the output is not recognizably a flyer.\n",
    "\n",
    "##### 2) Text rendering accuracy — Pass / Fail (gate)\n",
    "\n",
    "This is usually the #1 production failure mode.\n",
    "\n",
    "**PASS if**\n",
    "\n",
    "- All required text strings are present and exactly correct (spelling, punctuation, capitalization, symbols).\n",
    "- Text is readable (not smeared, clipped, warped, or overlapping).\n",
    "\n",
    "**FAIL if**\n",
    "\n",
    "- Any required text is incorrect/missing/unreadable, or any extra text appears.\n",
    "\n",
    "Tip: if your workflow requires “exact copy,” treat this as a hard gate. Don’t average it away.\n",
    "\n",
    "##### 3) Layout and hierarchy — 0–5\n",
    "\n",
    "Measures whether the flyer communicates clearly at a glance.\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "- Clear priority: headline dominates, subheadline supports, offer stands out, footer is secondary.\n",
    "- Alignment and spacing feel intentional (no crowded clusters, no random floating elements).\n",
    "- Read order is unambiguous.\n",
    "\n",
    "##### 4) Style and brand fit — 0–5\n",
    "\n",
    "Measures whether it matches the requested “vibe” and avoids off-brand looks.\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "- Warm, cozy, minimal; specialty coffee visual language.\n",
    "- Consistent palette and typography feel (not multiple conflicting styles).\n",
    "- Avoids cartoonish illustration if the prompt requested photo-real.\n",
    "\n",
    "##### 5) Visual quality and artifact severity — 0–5\n",
    "\n",
    "A flyer can be “correct” but still unusable if it’s visually broken.\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "- No distorted objects, broken hands/cups, melted foam textures, weird artifacts around text.\n",
    "- Background texture is subtle (doesn’t compete with copy).\n",
    "- Hero image looks appetizing and coherent.\n",
    "\n",
    "##### 6) Human feedback (quick and high leverage)\n",
    "\n",
    "UI \"usability clarity\" is where humans add the most value. Keep it lightweight and focused on interaction intent.\n",
    "\n",
    "- Rubric labels (fast):\n",
    "  - Would you use this mockup to iterate on a real product screen? (Y/N)\n",
    "  - If N, why? (missing section, action button unclear, text unreadable, layout confusing, etc.)\n",
    "  - Overall usability clarity: 1–5\n",
    "- Common failure tags (for debugging + iteration):\n",
    "  - `wrong_screen_type`\n",
    "  - `missing_required_section`\n",
    "  - `extra_ui_elements`\n",
    "  - `primary_cta_not_clear`\n",
    "  - `text_unreadable_or_garbled`\n",
    "  - `affordances_unclear`\n",
    "  - `layout_confusing`\n",
    "\n",
    "**Verdict rules (how to convert metrics into a single pass/fail)**\n",
    "\n",
    "A simple and strict set of rules works well:\n",
    "\n",
    "- **Instruction following must PASS**\n",
    "- **Text rendering must PASS**\n",
    "- **Layout and hierarchy ≥ 3**\n",
    "- **Style and brand fit ≥ 3**\n",
    "- **Visual quality ≥ 3**\n",
    "\n",
    "If any rule fails → overall **FAIL**.  \n",
    "(Do not average scores to override text correctness.)\n",
    "\n",
    "**Example `TestCase` set (small but high-signal)**\n",
    "\n",
    "Start with ~8–12 cases to cover common flyer variants and edge cases.\n",
    "\n",
    "1. Seasonal campaign (the “Winter Latte Week” prompt above)\n",
    "2. Minimal text (only 2 lines + logo + hours) — tests sparse layouts\n",
    "3. Dense info (menu highlights + deal + two CTAs) — tests hierarchy under load\n",
    "4. Strict no-extra-text — tests hallucinated filler copy\n",
    "5. Exact punctuation (“20% OFF • Mon–Thu”) — tests symbol fidelity\n",
    "6. Two offers (“BOGO Tuesdays” + “Happy Hour 2–4”) — tests multi-badge layout\n",
    "7. Colorway constraint (“use only cream + dark brown + muted orange”)\n",
    "8. Accessibility variant (high contrast, large text, simple background)\n",
    "\n",
    "**LLM-as-judge rubric prompt**\n",
    "\n",
    "Below is a judge prompt aligned to your existing `LLMajRubricGrader`. It returns structured metric scores + an overall verdict.\n",
    "\n",
    "You can use this with your existing grader by changing the JSON schema to include the fields below (or create separate graders per metric if you prefer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8193507",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"<core_mission>\n",
    "Evaluate whether a generated marketing flyer is usable for a real coffee shop\n",
    "promotion by checking instruction adherence, exact text correctness, layout clarity,\n",
    "style fit, and artifact severity.\n",
    "</core_mission>\n",
    "\n",
    "<role>\n",
    "You are an expert evaluator of marketing design deliverables.\n",
    "You care about correctness, readability, hierarchy, and brand-fit.\n",
    "You do NOT reward creativity that violates constraints.\n",
    "</role>\n",
    "\n",
    "<scope_constraints>\n",
    "- Judge ONLY against the provided prompt and criteria.\n",
    "- Be strict about required copy: spelling, punctuation, casing, and symbols must match exactly.\n",
    "- Extra or missing text is a serious error.\n",
    "- If unsure, score conservatively (lower score).\n",
    "</scope_constraints>\n",
    "\n",
    "<metrics>\n",
    "1) instruction_following: PASS/FAIL\n",
    "2) text_rendering: PASS/FAIL\n",
    "3) layout_hierarchy: 0-5\n",
    "4) style_brand_fit: 0-5\n",
    "5) visual_quality: 0-5\n",
    "\n",
    "Use these anchors:\n",
    "Layout/Hierarchy 5 = instantly readable; clear order; strong spacing/alignment.\n",
    "3 = understandable but needs iteration (one clear issue).\n",
    "0-2 = confusing or hard to parse.\n",
    "\n",
    "Style/Brand Fit 5 = clearly matches requested vibe; consistent; not off-style.\n",
    "3 = generally matches but with noticeable mismatch.\n",
    "0-2 = wrong style (e.g. cartoonish when photo-real requested).\n",
    "\n",
    "Visual Quality 5 = clean; no distracting artifacts; hero image coherent.\n",
    "3 = minor artifacts but still usable.\n",
    "0-2 = obvious artifacts or distortions that break usability.\n",
    "</metrics>\n",
    "\n",
    "<verdict_rules>\n",
    "Overall verdict is FAIL if:\n",
    "- instruction_following is FAIL, OR\n",
    "- text_rendering is FAIL, OR\n",
    "- any of layout_hierarchy/style_brand_fit/visual_quality is < 3.\n",
    "Otherwise PASS.\n",
    "</verdict_rules>\n",
    "\n",
    "<output_constraints>\n",
    "Return JSON only.\n",
    "No extra text.\n",
    "</output_constraints>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6246f3",
   "metadata": {},
   "source": [
    "#### Recommended JSON Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3238628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verdict': 'PASS',\n",
       " 'instruction_following': True,\n",
       " 'text_rendering': True,\n",
       " 'layout_hierarchy': 4,\n",
       " 'style_brand_fit': 4,\n",
       " 'visual_quality': 3,\n",
       " 'reason': '...'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"verdict\": \"PASS\",\n",
    "  \"instruction_following\": True,\n",
    "  \"text_rendering\": True,\n",
    "  \"layout_hierarchy\": 4,\n",
    "  \"style_brand_fit\": 4,\n",
    "  \"visual_quality\": 3,\n",
    "  \"reason\": \"...\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c946f9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Coffee Example: Harness Setup\n",
    "\n",
    "Define a single marketing flyer test case, a model run, and an output store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b1d81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the harness for a single coffee flyer generation case.\n",
    "\n",
    "coffee_generation_prompt = \"\"\"Create a print-ready vertical A4 flyer for a coffee shop called Sunrise Coffee.\n",
    "Use a warm, cozy, minimal specialty coffee aesthetic (not cartoonish).\n",
    "Required text (must be exact):\n",
    "- WINTER LATTE WEEK\n",
    "- Try our Cinnamon Oat Latte\n",
    "- 20% OFF - Mon-Thu\n",
    "- Order Ahead\n",
    "- 123 Market St - 7am-6pm\n",
    "Do not include any other words, prices, URLs, or QR codes.\n",
    "\"\"\"\n",
    "\n",
    "coffee_criteria = \"\"\"All required text appears exactly as written and is legible.\n",
    "Layout reads clearly: shop name -> headline -> subheadline -> offer -> CTA -> footer.\n",
    "Style matches warm, cozy, specialty coffee and is not cartoonish.\n",
    "No extra text, watermarks, or irrelevant UI-like elements.\"\"\"\n",
    "\n",
    "coffee_case = TestCase(\n",
    "    id=\"coffee_flyer_generation\",\n",
    "    task_type=\"image_generation\",\n",
    "    prompt=coffee_generation_prompt,\n",
    "    criteria=coffee_criteria,\n",
    ")\n",
    "\n",
    "coffee_run = ModelRun(\n",
    "    label=\"gpt-image-1.5\",\n",
    "    task_type=\"image_generation\",\n",
    "    params={\n",
    "        \"model\": \"gpt-image-1.5\",\n",
    "        \"n\": 1,\n",
    "        \"size\": \"1024x1024\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Save artifacts under the repo images/ folder so they render on the site.\n",
    "coffee_store = OutputStore(root=Path(\"../../images\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b6cee",
   "metadata": {},
   "source": [
    "### Coffee Example: Run And Grade\n",
    "\n",
    "Run the harness and score the output with the marketing judge rubric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc3baa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_id': 'coffee_flyer_generation',\n",
       " 'model_label': 'gpt-image-1.5',\n",
       " 'task_type': 'image_generation',\n",
       " 'artifact_paths': ['../../images/gen_coffee_flyer_generation_gpt-image-1.5_1769657762450_0.png'],\n",
       " 'scores': {'instruction_following': True,\n",
       "  'text_rendering': True,\n",
       "  'layout_hierarchy': 5.0,\n",
       "  'style_brand_fit': 5.0,\n",
       "  'visual_quality': 5.0,\n",
       "  'verdict': 'PASS',\n",
       "  'text_rendering_judge': True,\n",
       "  'text_rendering_ocr': False},\n",
       " 'reasons': {'instruction_following': '',\n",
       "  'text_rendering': '',\n",
       "  'layout_hierarchy': '',\n",
       "  'style_brand_fit': '',\n",
       "  'visual_quality': '',\n",
       "  'verdict': 'All required lines appear and match exactly (including casing, hyphen/en-dash, and punctuation), with no extra words, prices, URLs, or QR codes. Text is crisp and legible. Hierarchy follows the requested order: shop name at top, then headline, subheadline, offer, CTA, and footer address/hours. Visual style is warm, cozy, minimal specialty coffee with a realistic latte photo (not cartoonish). No visible watermarks, UI elements, or distracting artifacts; overall print-flyer quality is clean.',\n",
       "  'text_rendering_judge': '',\n",
       "  'text_rendering_ocr': 'missing: 123 Market St • 7am–6pm, 20% OFF • Mon–Thu; extra: 123 Market St - 7am-6pm, 20% OFF - Mon-Thu, SUNRISE COFFEE'},\n",
       " 'run_params': {'model': 'gpt-image-1.5', 'n': 1, 'size': '1024x1024'}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the prebuilt graders + evaluate helpers with a structured marketing schema.\n",
    "from openai import OpenAI\n",
    "\n",
    "marketing_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"verdict\": {\"type\": \"string\"},\n",
    "        \"instruction_following\": {\"type\": \"boolean\"},\n",
    "        \"text_rendering\": {\"type\": \"boolean\"},\n",
    "        \"layout_hierarchy\": {\"type\": \"number\"},\n",
    "        \"style_brand_fit\": {\"type\": \"number\"},\n",
    "        \"visual_quality\": {\"type\": \"number\"},\n",
    "        \"reason\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"verdict\",\n",
    "        \"instruction_following\",\n",
    "        \"text_rendering\",\n",
    "        \"layout_hierarchy\",\n",
    "        \"style_brand_fit\",\n",
    "        \"visual_quality\",\n",
    "        \"reason\",\n",
    "    ],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def parse_marketing_result(data: dict, base_key: str) -> list[Score]:\n",
    "    return [\n",
    "        Score(key=\"instruction_following\", value=bool(data[\"instruction_following\"]), reason=\"\"),\n",
    "        Score(key=\"text_rendering\", value=bool(data[\"text_rendering\"]), reason=\"\"),\n",
    "        Score(key=\"layout_hierarchy\", value=float(data[\"layout_hierarchy\"]), reason=\"\"),\n",
    "        Score(key=\"style_brand_fit\", value=float(data[\"style_brand_fit\"]), reason=\"\"),\n",
    "        Score(key=\"visual_quality\", value=float(data[\"visual_quality\"]), reason=\"\"),\n",
    "        Score(key=\"verdict\", value=str(data[\"verdict\"]), reason=(data.get(\"reason\") or \"\").strip()),\n",
    "    ]\n",
    "\n",
    "\n",
    "marketing_grader = LLMajRubricGrader(\n",
    "    key=\"marketing_eval\",\n",
    "    system_prompt=PROMPT,\n",
    "    content_builder=build_generation_judge_content,\n",
    "    judge_model=\"gpt-5.2\",\n",
    "    json_schema_name=\"marketing_flyer_eval\",\n",
    "    json_schema=marketing_schema,\n",
    "    result_parser=parse_marketing_result,\n",
    ")\n",
    "\n",
    "coffee_results = evaluate(\n",
    "    cases=[coffee_case],\n",
    "    model_runs=[coffee_run],\n",
    "    graders=[marketing_grader],\n",
    "    output_store=coffee_store,\n",
    ")\n",
    "\n",
    "coffee_result = coffee_results[0]\n",
    "\n",
    "# Simple alternative: exact text-rendering check via OCR-style extraction.\n",
    "REQUIRED_TEXT = {\n",
    "    \"WINTER LATTE WEEK\",\n",
    "    \"Try our Cinnamon Oat Latte\",\n",
    "    \"20% OFF • Mon–Thu\",\n",
    "    \"Order Ahead\",\n",
    "    \"123 Market St • 7am–6pm\",\n",
    "}\n",
    "\n",
    "\n",
    "def extract_text_from_flyer(image_path: str | Path, model: str = \"gpt-5.2\") -> list[str]:\n",
    "    judge_client = client if \"client\" in globals() else OpenAI()\n",
    "    image_url = image_to_data_url(Path(image_path))\n",
    "\n",
    "    instructions = (\n",
    "        \"List every piece of text visible in this flyer image. \"\n",
    "        \"Return one line per text item and preserve capitalization, punctuation, and spacing exactly.\"\n",
    "    )\n",
    "\n",
    "    response = judge_client.responses.create(\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_image\", \"image_url\": image_url},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return [line.strip() for line in (response.output_text or \"\").splitlines() if line.strip()]\n",
    "\n",
    "\n",
    "coffee_image_path = Path(coffee_result[\"artifact_paths\"][0])\n",
    "extracted_lines = extract_text_from_flyer(coffee_image_path)\n",
    "\n",
    "missing = sorted(REQUIRED_TEXT - set(extracted_lines))\n",
    "extra = sorted(set(extracted_lines) - REQUIRED_TEXT)\n",
    "text_rendering_pass = not missing and not extra\n",
    "\n",
    "parts = []\n",
    "if missing:\n",
    "    parts.append(\"missing: \" + \", \".join(missing))\n",
    "if extra:\n",
    "    parts.append(\"extra: \" + \", \".join(extra))\n",
    "text_rendering_reason = \"; \".join(parts) if parts else \"Exact text match.\"\n",
    "\n",
    "# Keep both: the judge's text_rendering and the OCR exact-match result.\n",
    "text_rendering_judge_score = coffee_result[\"scores\"].get(\"text_rendering\")\n",
    "text_rendering_judge_reason = coffee_result[\"reasons\"].get(\"text_rendering\", \"\")\n",
    "\n",
    "coffee_result[\"scores\"][\"text_rendering_judge\"] = text_rendering_judge_score\n",
    "coffee_result[\"reasons\"][\"text_rendering_judge\"] = text_rendering_judge_reason\n",
    "\n",
    "coffee_result[\"scores\"][\"text_rendering_ocr\"] = text_rendering_pass\n",
    "coffee_result[\"reasons\"][\"text_rendering_ocr\"] = text_rendering_reason\n",
    "\n",
    "coffee_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545d5ea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Flyer Generation Results\n",
    "\n",
    "Render the generated image, prompt, and scores side by side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd493083",
   "metadata": {},
   "source": [
    "<img src=\"../../images/gen_coffee_flyer_generation_gpt-image-1.5_1769657762450_0.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1e3870b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='font-weight:600; margin:6px 0'>Coffee Flyer: Prompt vs. Scores</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table style='width:100%; table-layout:fixed; border-collapse:collapse;'><colgroup><col style='width:33%'><col style='width:33%'><col style='width:33%'></colgroup><thead><tr><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Input Prompt</th><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Scores</th><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Reasoning</th></tr></thead><tbody><tr><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>Create a print-ready vertical A4 flyer for a coffee shop called Sunrise Coffee.\n",
       "Use a warm, cozy, minimal specialty coffee aesthetic (not cartoonish).\n",
       "Required text (must be exact):\n",
       "- WINTER LATTE WEEK\n",
       "- Try our Cinnamon Oat Latte\n",
       "- 20% OFF - Mon-Thu\n",
       "- Order Ahead\n",
       "- 123 Market St - 7am-6pm\n",
       "Do not include any other words, prices, URLs, or QR codes.\n",
       "\n",
       "\n",
       "Criteria:\n",
       "All required text appears exactly as written and is legible.\n",
       "Layout reads clearly: shop name -&gt; headline -&gt; subheadline -&gt; offer -&gt; CTA -&gt; footer.\n",
       "Style matches warm, cozy, specialty coffee and is not cartoonish.\n",
       "No extra text, watermarks, or irrelevant UI-like elements.</pre></td><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>instruction_following: True\n",
       "layout_hierarchy: 5.0\n",
       "style_brand_fit: 5.0\n",
       "text_rendering: True\n",
       "text_rendering_judge: True\n",
       "text_rendering_ocr: False\n",
       "verdict: PASS\n",
       "visual_quality: 5.0</pre></td><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>All required lines appear and match exactly (including casing, hyphen/en-dash, and punctuation), with no extra words, prices, URLs, or QR codes. Text is crisp and legible. Hierarchy follows the requested order: shop name at top, then headline, subheadline, offer, CTA, and footer address/hours. Visual style is warm, cozy, minimal specialty coffee with a realistic latte photo (not cartoonish). No visible watermarks, UI elements, or distracting artifacts; overall print-flyer quality is clean.</pre></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_result_table(case=coffee_case, result=coffee_result, title=\"Coffee Flyer: Prompt vs. Scores\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400db52",
   "metadata": {},
   "source": [
    "#### Additional Method: Image-to-Text + Text Comparison\n",
    "\n",
    "Another evaluation approach is to shift the problem into a different modality—text. In this setup, images are first converted into a verbose textual description using a vision model, and evaluations are then performed by comparing the generated text against the expected text.\n",
    "\n",
    "This method works particularly well for OCR-style workflows. For example, you can extract text from a generated flyer using a vision model and then compare the resulting text lines directly against the required copy set to verify correctness and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d598e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_rendering_score</th>\n",
       "      <th>missing_text</th>\n",
       "      <th>extra_text</th>\n",
       "      <th>extracted_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>123 Market St • 7am–6pm\\n20% OFF • Mon–Thu</td>\n",
       "      <td>123 Market St - 7am-6pm\\n123 Market St - 7am-6...</td>\n",
       "      <td>SUNRISE COFFEE\\nWINTER LATTE WEEK\\nTry our Cin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_rendering_score                                missing_text  \\\n",
       "0                  True  123 Market St • 7am–6pm\\n20% OFF • Mon–Thu   \n",
       "\n",
       "                                          extra_text  \\\n",
       "0  123 Market St - 7am-6pm\\n123 Market St - 7am-6...   \n",
       "\n",
       "                                      extracted_text  \n",
       "0  SUNRISE COFFEE\\nWINTER LATTE WEEK\\nTry our Cin...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "coffee_image_path = Path(coffee_result[\"artifact_paths\"][0])\n",
    "extracted_lines = extract_text_from_flyer(coffee_image_path)\n",
    "\n",
    "required_lines = sorted(REQUIRED_TEXT)\n",
    "required_set = set(required_lines)\n",
    "extracted_set = set(extracted_lines)\n",
    "\n",
    "missing = sorted(required_set - extracted_set)\n",
    "extra = sorted(extracted_set - required_set)\n",
    "\n",
    "text_rendering_df = pd.DataFrame(\n",
    "    {\n",
    "        \"text_rendering_score\": [coffee_result[\"scores\"].get(\"text_rendering\")],\n",
    "        \"missing_text\": [\"\\n\".join(missing) or \"(none)\"],\n",
    "        \"extra_text\": [\"\\n\".join(extra) or \"(none)\"],\n",
    "        \"extracted_text\": [\"\\n\".join(extracted_lines) or \"(no text extracted)\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "text_rendering_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7ca15",
   "metadata": {},
   "source": [
    "## Image Editing Evals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2e543",
   "metadata": {},
   "source": [
    "### Virtual Try-On\n",
    "\n",
    "Virtual try-on (VTO) is an **image editing** workflow: given a **person\n",
    "photo** (selfie or model) and a **garment reference** (product photo\n",
    "and/or description), generate an output where the garment looks\n",
    "**naturally worn**—while keeping the person’s identity, pose, and scene\n",
    "intact.\n",
    "\n",
    "**Why VTO evals are different**\n",
    "\n",
    "Unlike “creative” edits, VTO is judged on **fidelity + preservation**:\n",
    "\n",
    "- **Preserve the wearer** (face identity, body shape, pose)  \n",
    "- **Preserve the product** (color, pattern, logos, material cues)  \n",
    "- **Edit only what’s needed** (locality/preservation)  \n",
    "- **Look physically plausible** (occlusions, lighting, drape, wrinkles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cae3d6",
   "metadata": {},
   "source": [
    "Example Inputs:\n",
    "\n",
    "| Full Body | Item 1 |\n",
    "|:------------:|:--------------:|\n",
    "| ![](../../images/woman_in_museum.png)  | ![](../../images/jacket.png) |\n",
    "| Item 2 | Item 3 |\n",
    "| ![](../../images/tank_top.png) | ![](../../images/boots.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad26d5",
   "metadata": {},
   "source": [
    "Output Image: \n",
    "\n",
    "<img src=\"../../images/outfit.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22135c28",
   "metadata": {},
   "source": [
    "#### What to evaluate (practical metrics):\n",
    "\n",
    "##### 1) Treat VTO as a multi-reference editing task\n",
    "\n",
    "For each test case, store:\n",
    "\n",
    "- **Input person image** (selfie/model)  \n",
    "- **Product reference** (catalog image(s), flat-lay, mannequin, or a\n",
    "  “worn” reference when available)  \n",
    "- Optional but high-leverage:\n",
    "  - **Mask(s)**: editable region, clothing region, hair/hand occluders  \n",
    "  - **Metadata**: category (top/bottom/outerwear), desired fit\n",
    "    (oversized/slim), colorway, length  \n",
    "  - **Edit instruction**: “Put on *this exact jacket* without changing\n",
    "    background or face.”\n",
    "\n",
    "This lets graders compare **output vs. both inputs** (person + product),\n",
    "not just “does it look good?”\n",
    "\n",
    "##### 2) Graded metrics (use only these three)\n",
    "\n",
    "Use these as **0–5** scores to rank models and track improvement. Keep\n",
    "them **separate** (don’t average them inside the grader); use verdict\n",
    "rules outside if you want gates.\n",
    "\n",
    "##### A) Facial similarity (output vs selfie) — 0–5\n",
    "\n",
    "Measures whether the output preserves the *same person* (identity), not\n",
    "just “a plausible face.”\n",
    "\n",
    "- **5**: Clearly the same person; key facial features unchanged; no\n",
    "  noticeable age/ethnicity/style drift; expression changes (if any) are\n",
    "  minor and realistic.  \n",
    "- **4**: Same person; tiny differences only noticeable on close\n",
    "  inspection (minor shape/texture smoothing, slight eye/mouth drift).  \n",
    "- **3**: Mostly the same person, but at least one noticeable identity\n",
    "  drift (feature proportions, jawline, eyes, nose) that would reduce\n",
    "  user trust.  \n",
    "- **2**: Significant identity drift; looks like a different person or\n",
    "  heavily altered face.  \n",
    "- **1**: Major corruption (melted/blurry face) or clearly different\n",
    "  identity.  \n",
    "- **0**: Face missing, unreadable, or replaced.\n",
    "\n",
    "What to look for:\n",
    "\n",
    "- Facial geometry consistency (eyes/nose/mouth spacing, jawline,\n",
    "  cheekbones)  \n",
    "- Skin texture realism without “beauty filter” identity loss  \n",
    "- No unintended makeup/age/style changes unless requested\n",
    "\n",
    "##### B) Outfit fidelity (output vs provided items) — 0–5\n",
    "\n",
    "Measures whether the output garment matches the *specific* product\n",
    "reference(s) the user selected.\n",
    "\n",
    "- **5**: Item matches reference closely: correct category, colorway,\n",
    "  pattern/print, material cues, and key details (logos, seams, collar,\n",
    "  pockets).  \n",
    "- **4**: Clearly the same item; 1–2 minor deviations (small logo blur,\n",
    "  slight hue shift, minor detail simplification).  \n",
    "- **3**: Generally correct but with a notable mismatch (pattern scale\n",
    "  wrong, material looks different, key design element missing/added).  \n",
    "- **2**: Multiple mismatches; could be a different variant or different\n",
    "  product.  \n",
    "- **1**: Wrong item category or strongly incorrect visual identity.  \n",
    "- **0**: Outfit not applied / missing / replaced with unrelated\n",
    "  clothing.\n",
    "\n",
    "What to look for:\n",
    "\n",
    "- Color/pattern correctness (especially stripes, plaid, small repeats)  \n",
    "- Logo/text integrity (no hallucinated letters)  \n",
    "- Structural details (neckline, sleeves, hem, closures)\n",
    "\n",
    "##### C) Body shape preservation (output vs selfie) — 0–5\n",
    "\n",
    "Measures whether the model preserves the wearer’s body shape, pose, and\n",
    "proportions **outside normal garment effects** (e.g., loose clothing can\n",
    "change silhouette, but shouldn’t reshape anatomy).\n",
    "\n",
    "- **5**: Body proportions and pose are preserved; garment conforms\n",
    "  naturally without warping torso/limbs.  \n",
    "- **4**: Minor, plausible silhouette changes consistent with clothing;\n",
    "  no obvious anatomical distortion.  \n",
    "- **3**: Noticeable reshaping (waist/hips/shoulders/limbs) that feels\n",
    "  slightly “AI-stylized” or inconsistent with the input body.  \n",
    "- **2**: Significant warping (elongated limbs, shifted joints,\n",
    "  compressed torso) that would be unacceptable in product use.  \n",
    "- **1**: Severe anatomical distortion (extra/missing limbs, melted body\n",
    "  regions).  \n",
    "- **0**: Body is not recognizable or is fundamentally corrupted.\n",
    "\n",
    "What to look for:\n",
    "\n",
    "- Shoulder/hip width consistency relative to input  \n",
    "- Limb length/joint placement stability (elbows, knees, wrists)  \n",
    "- No “body slimming” or “body inflation” artifacts\n",
    "\n",
    "If you want a single overall pass/fail, a common rule is:\n",
    "\n",
    "- **Fail** if any metric ≤ 2  \n",
    "- **Pass** if all metrics ≥ 3  \n",
    "  (and optionally require outfit fidelity ≥ 4 for commerce-critical\n",
    "  flows).\n",
    "\n",
    "##### 3) Human feedback: keep it simple but consistent\n",
    "\n",
    "Humans are best at “would I trust this in a shopping flow?”\n",
    "\n",
    "Use two label types:\n",
    "\n",
    "- **Rubric labels** (quick, structured):\n",
    "  - Identity preserved? (Y/N)  \n",
    "  - Garment matches reference? (Y/N + what’s wrong)  \n",
    "  - Any bad artifacts? (none/minor/major)  \n",
    "  - Overall usable for e-commerce? (Y/N)  \n",
    "- **Pairwise preference** (A vs B):\n",
    "  - Which output is more faithful to the product while keeping the person unchanged?\n",
    "\n",
    "Add periodic **calibration**: keep a small set of “anchor” examples that\n",
    "raters re-score to prevent drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0242b4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Virtual Try-On Example: Harness Setup\n",
    "\n",
    "Use existing images from `images/` as the person and garment references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2082b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vto_person_path = Path(\"../../images/base_woman.png\")\n",
    "vto_garment_path = Path(\"../../images/jacket.png\")\n",
    "\n",
    "vto_prompt = \"\"\"Put the person in the first image into the jacket shown in the second image.\n",
    "Keep the person's face, pose, body shape, and background unchanged.\n",
    "Preserve the garment's color, pattern, and key details.\n",
    "Do not add extra accessories, text, or new elements.\"\"\"\n",
    "\n",
    "vto_criteria = \"\"\"The output preserves the same person and background.\n",
    "The jacket matches the reference garment closely.\n",
    "Body shape and pose remain consistent outside normal garment effects.\n",
    "The result looks physically plausible.\"\"\"\n",
    "\n",
    "vto_case = TestCase(\n",
    "    id=\"vto_jacket_tryon\",\n",
    "    task_type=\"image_editing\",\n",
    "    prompt=vto_prompt,\n",
    "    criteria=vto_criteria,\n",
    "    image_inputs=ImageInputs(image_paths=[vto_person_path, vto_garment_path]),\n",
    ")\n",
    "\n",
    "vto_run = ModelRun(\n",
    "    label=\"gpt-image-1.5-vto\",\n",
    "    task_type=\"image_editing\",\n",
    "    params={\n",
    "        \"model\": \"gpt-image-1.5\",\n",
    "        \"n\": 1,\n",
    "    },\n",
    ")\n",
    "\n",
    "vto_store = OutputStore(root=Path(\"../../images\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb6f43",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Virtual Try-On Example: Run And Grade\n",
    "\n",
    "Define a VTO judge prompt aligned to the VTO metrics and run the harness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e67dbf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_id': 'vto_jacket_tryon',\n",
       " 'model_label': 'gpt-image-1.5-vto',\n",
       " 'task_type': 'image_editing',\n",
       " 'artifact_paths': ['../../images/edit_vto_jacket_tryon_gpt-image-1.5-vto_1769658750053_0.png'],\n",
       " 'scores': {'facial_similarity': 5.0,\n",
       "  'outfit_fidelity': 4.0,\n",
       "  'body_shape_preservation': 4.0,\n",
       "  'verdict': 'PASS'},\n",
       " 'reasons': {'facial_similarity': '',\n",
       "  'outfit_fidelity': '',\n",
       "  'body_shape_preservation': '',\n",
       "  'verdict': 'The edited output preserves the same face, hairstyle, pose, and plain studio background. The applied jacket closely matches the reference: camel/beige color, notch lapels, single-breasted look with dark buttons, and flap pockets are present and placed plausibly. Minor deviations include slightly different button count/placement and subtle differences in lapel/hem shaping compared to the flat lay. Body proportions and stance remain consistent, with only natural silhouette changes from wearing a structured blazer.'},\n",
       " 'run_params': {'model': 'gpt-image-1.5', 'n': 1}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vto_judge_prompt = \"\"\"<core_mission>\n",
    "Evaluate whether a virtual try-on edit preserves the person while accurately applying the reference garment.\n",
    "</core_mission>\n",
    "\n",
    "<role>\n",
    "You are an expert evaluator of virtual try-on outputs.\n",
    "You focus on identity preservation, garment fidelity, and body-shape preservation.\n",
    "</role>\n",
    "\n",
    "<metrics>\n",
    "1) facial_similarity: 0-5\n",
    "2) outfit_fidelity: 0-5\n",
    "3) body_shape_preservation: 0-5\n",
    "</metrics>\n",
    "\n",
    "<verdict_rules>\n",
    "FAIL if any metric <= 2.\n",
    "PASS if all metrics >= 3.\n",
    "</verdict_rules>\n",
    "\n",
    "<output_constraints>\n",
    "Return JSON only with the fields specified in the schema.\n",
    "</output_constraints>\n",
    "\"\"\"\n",
    "\n",
    "vto_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"verdict\": {\"type\": \"string\"},\n",
    "        \"facial_similarity\": {\"type\": \"number\"},\n",
    "        \"outfit_fidelity\": {\"type\": \"number\"},\n",
    "        \"body_shape_preservation\": {\"type\": \"number\"},\n",
    "        \"reason\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"verdict\",\n",
    "        \"facial_similarity\",\n",
    "        \"outfit_fidelity\",\n",
    "        \"body_shape_preservation\",\n",
    "        \"reason\",\n",
    "    ],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def parse_vto_result(data: dict, base_key: str) -> list[Score]:\n",
    "    return [\n",
    "        Score(key=\"facial_similarity\", value=float(data[\"facial_similarity\"]), reason=\"\"),\n",
    "        Score(key=\"outfit_fidelity\", value=float(data[\"outfit_fidelity\"]), reason=\"\"),\n",
    "        Score(key=\"body_shape_preservation\", value=float(data[\"body_shape_preservation\"]), reason=\"\"),\n",
    "        Score(key=\"verdict\", value=str(data[\"verdict\"]), reason=(data.get(\"reason\") or \"\").strip()),\n",
    "    ]\n",
    "\n",
    "vto_grader = LLMajRubricGrader(\n",
    "    key=\"vto_eval\",\n",
    "    system_prompt=vto_judge_prompt,\n",
    "    content_builder=build_editing_judge_content,\n",
    "    judge_model=\"gpt-5.2\",\n",
    "    json_schema_name=\"vto_eval\",\n",
    "    json_schema=vto_schema,\n",
    "    result_parser=parse_vto_result,\n",
    ")\n",
    "\n",
    "vto_results = evaluate(\n",
    "    cases=[vto_case],\n",
    "    model_runs=[vto_run],\n",
    "    graders=[vto_grader],\n",
    "    output_store=vto_store,\n",
    ")\n",
    "\n",
    "vto_result = vto_results[0]\n",
    "vto_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2bed8e",
   "metadata": {},
   "source": [
    "### Virtual Try-On: Optional Code Interpreter Crop Tool\n",
    "\n",
    "If you want finer-grained evidence (logos, seams, fit, face details), you can run a\n",
    "secondary judge that uses the Code Interpreter `crop` tool to zoom into regions.\n",
    "This is useful for close-up checks on garment fidelity and identity preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0265bf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_03756a1c45c8427000697ad91445ec8196a58b39ee7e0b05b1', created_at=1769658644.0, error=None, incomplete_details=None, instructions=\"\\nTools available:\\n- crop(image_id, x1, y1, x2, y2): Use to zoom into a specific image's region. Coordinates are integer pixels relative to the top-left of the CURRENT view of that image. Use as few precise crops as necessary to gather evidence.\\n    - When using crop(image_id, x1, y1, x2, y2), ensure that x2 > x1 and y2 > y1. The coordinates must define a valid rectangle: x1 (left, inclusive), y1 (top, inclusive), x2 (right, exclusive), y2 (bottom, exclusive), with x2 strictly greater than x1 and y2 strictly greater than y1. If you are unsure, double-check your coordinates before cropping to avoid errors.\\n\\nImages provided:\\n- Up to 5 user reference photos (ref_1..ref_5) for identity/baseline body context\\n- 1 clothing-only image (clothing)\\n- 1 reconstruction image (recon)\\n\\nYour goals:\\n1) Judge similarity between the clothing-only image and how it appears on the user in the reconstruction.\\n2) Judge identity realism (face/hair/skin) vs. user reference photos.\\n3) Judge overall realism (lighting, shadows, artifacts).\\n\\nIMPORTANT:\\n- Use the crop tool when you need more detail. After crop, a new grid overlay is returned for that image.\\n- You may use the crop tool as many times as needed to gather evidence.\\n- When confident, produce the final STRICT JSON only. No extra text.\\n\", metadata={}, model='gpt-5.2-2025-12-11', object='response', output=[ResponseCodeInterpreterToolCall(id='ci_03756a1c45c8427000697ad91aaf108196974c45daf37a9a18', code=\"from PIL import Image, ImageOps\\nimg1=Image.open('/mnt/data/143ba8edc474910b395d0d44e2f06a9d-image.png')\\nimg2=Image.open('/mnt/data/ababce34a72604310f9dac11e9e5f00f-image.png')\\nimg3=Image.open('/mnt/data/6c920ec973b3b1abb80da8b29ee5aa3e-image.png')\\nimg1.size, img2.size, img3.size\\n\", container_id='cntr_697ad91842dc8193a9e214852cbb3fda02ddf8755852a20c', outputs=None, status='completed', type='code_interpreter_call'), ResponseCodeInterpreterToolCall(id='ci_03756a1c45c8427000697ad93101b48196a090b8acf49e7948', code=\"import matplotlib.pyplot as plt\\nplt.figure(figsize=(12,6))\\nfor i,img in enumerate([img1,img2,img3],1):\\n    plt.subplot(1,3,i); plt.imshow(img); plt.axis('off'); plt.title(i)\\nplt.show()\\n\", container_id='cntr_697ad91842dc8193a9e214852cbb3fda02ddf8755852a20c', outputs=None, status='completed', type='code_interpreter_call'), ResponseCodeInterpreterToolCall(id='ci_03756a1c45c8427000697ad94bf50481969ec6383dc9bd0a40', code=\"import numpy as np, matplotlib.pyplot as plt\\n# crop face regions\\nface1=img1.crop((70,20,230,140))\\nface3=img3.crop((330,70,690,330))\\nplt.figure(figsize=(8,4))\\nplt.subplot(1,2,1); plt.imshow(face1); plt.axis('off'); plt.title('ref face')\\nplt.subplot(1,2,2); plt.imshow(face3); plt.axis('off'); plt.title('recon face')\\nplt.show()\\n\", container_id='cntr_697ad91842dc8193a9e214852cbb3fda02ddf8755852a20c', outputs=None, status='completed', type='code_interpreter_call'), ResponseCodeInterpreterToolCall(id='ci_03756a1c45c8427000697ad9522568819681f59d41d921f102', code=\"# crop blazer from clothing and recon for comparison\\ncloth=img2.crop((40,40,260,410))\\nrecon_blazer=img3.crop((250,250,800,1100))\\nplt.figure(figsize=(10,4))\\nplt.subplot(1,2,1); plt.imshow(cloth); plt.axis('off'); plt.title('clothing')\\nplt.subplot(1,2,2); plt.imshow(recon_blazer); plt.axis('off'); plt.title('recon blazer')\\nplt.show()\\n\", container_id='cntr_697ad91842dc8193a9e214852cbb3fda02ddf8755852a20c', outputs=None, status='completed', type='code_interpreter_call'), ResponseOutputMessage(id='msg_03756a1c45c8427000697ad958266c8196ab64e9c75379f4ce', content=[ResponseOutputText(annotations=[AnnotationContainerFileCitation(container_id='cntr_697ad91842dc8193a9e214852cbb3fda02ddf8755852a20c', end_index=0, file_id='cfile_697ad956d5d4819194b112d8f80b8a9d', filename='cfile_697ad956d5d4819194b112d8f80b8a9d.png', start_index=0, type='container_file_citation')], text='{\\n  \"clothing_similarity\": {\\n    \"score\": 0.9,\\n    \"notes\": [\\n      \"Blazer color matches well (camel/tan) with similar fabric look.\",\\n      \"Key design elements align: notch lapels, two-button front with dark buttons, flap pockets, and overall single-breasted silhouette.\",\\n      \"Proportions look consistent (hip-length, straight hem, structured shoulders).\",\\n      \"Minor differences: lapel/edge sharpness and pocket flap geometry look slightly simplified in the try-on.\"\\n    ]\\n  },\\n  \"identity_realism\": {\\n    \"score\": 0.72,\\n    \"notes\": [\\n      \"Overall face shape, hair color/part, and general look are fairly consistent with the reference.\",\\n      \"Some identity drift: facial details (eyes/nose/mouth definition) appear smoother/idealized in the try-on compared to the reference.\",\\n      \"Skin texture is more airbrushed in the try-on; less natural micro-detail.\"\\n    ]\\n  },\\n  \"overall_realism\": {\\n    \"score\": 0.84,\\n    \"notes\": [\\n      \"Lighting and shadows are mostly coherent with the studio background; blazer shading reads plausibly on-body.\",\\n      \"Good garment-body integration at shoulders and torso; sleeve placement looks natural.\",\\n      \"Small AI artifacts: slightly softened/blurred edges around lapels and pocket areas; fine fabric texture is reduced.\"\\n    ]\\n  }\\n}', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[CodeInterpreter(container=CodeInterpreterContainerCodeInterpreterToolAuto(type='auto', file_ids=None, memory_limit=None), type='code_interpreter')], top_p=0.98, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='none', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=5397, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=717, output_tokens_details=OutputTokensDetails(reasoning_tokens=417), total_tokens=6114), user=None, billing={'payer': 'developer'}, completed_at=1769658722, frequency_penalty=0.0, presence_penalty=0.0, store=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vto_output_path = Path(vto_result[\"artifact_paths\"][0])\n",
    "\n",
    "instructions = \"\"\"\n",
    "Tools available:\n",
    "- crop(image_id, x1, y1, x2, y2): Use to zoom into a specific image's region. Coordinates are integer pixels relative to the top-left of the CURRENT view of that image. Use as few precise crops as necessary to gather evidence.\n",
    "    - When using crop(image_id, x1, y1, x2, y2), ensure that x2 > x1 and y2 > y1. The coordinates must define a valid rectangle: x1 (left, inclusive), y1 (top, inclusive), x2 (right, exclusive), y2 (bottom, exclusive), with x2 strictly greater than x1 and y2 strictly greater than y1. If you are unsure, double-check your coordinates before cropping to avoid errors.\n",
    "\n",
    "Images provided:\n",
    "- Up to 5 user reference photos (ref_1..ref_5) for identity/baseline body context\n",
    "- 1 clothing-only image (clothing)\n",
    "- 1 reconstruction image (recon)\n",
    "\n",
    "Your goals:\n",
    "1) Judge similarity between the clothing-only image and how it appears on the user in the reconstruction.\n",
    "2) Judge identity realism (face/hair/skin) vs. user reference photos.\n",
    "3) Judge overall realism (lighting, shadows, artifacts).\n",
    "\n",
    "IMPORTANT:\n",
    "- Use the crop tool when you need more detail. After crop, a new grid overlay is returned for that image.\n",
    "- You may use the crop tool as many times as needed to gather evidence.\n",
    "- When confident, produce the final STRICT JSON only. No extra text.\n",
    "\"\"\"\n",
    "judge_client = client if \"client\" in globals() else OpenAI()\n",
    "response_crop = judge_client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    instructions=instructions,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"code_interpreter\",\n",
    "            \"container\": {\"type\": \"auto\", \"memory_limit\": \"4g\"},\n",
    "        }\n",
    "    ],\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"Order: ref_1 (person), clothing (garment reference), recon (try-on output).\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": image_to_data_url(vto_person_path),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": image_to_data_url(vto_garment_path),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": image_to_data_url(vto_output_path),\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "response_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b00a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Virtual Try-On Eval Results\n",
    "\n",
    "Show the edit result and VTO scores in a single pandas table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc3fb0",
   "metadata": {},
   "source": [
    "| Full Body | Item 1 |\n",
    "|:------------:|:--------------:|\n",
    "| ![](../../images/base_woman.png)  | ![](../../images/jacket.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a45a2",
   "metadata": {},
   "source": [
    "Edited Image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8f035",
   "metadata": {},
   "source": [
    "<img src=\"../../images/edit_vto_jacket_tryon_gpt-image-1.5-vto_1769658750053_0.png\" width=\"400\"/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2081890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='font-weight:600; margin:6px 0'>Virtual Try-On: Prompt vs. Scores</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table style='width:100%; table-layout:fixed; border-collapse:collapse;'><colgroup><col style='width:33%'><col style='width:33%'><col style='width:33%'></colgroup><thead><tr><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Input Prompt</th><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Scores</th><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Reasoning</th></tr></thead><tbody><tr><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>Put the person in the first image into the jacket shown in the second image.\n",
       "Keep the person&#x27;s face, pose, body shape, and background unchanged.\n",
       "Preserve the garment&#x27;s color, pattern, and key details.\n",
       "Do not add extra accessories, text, or new elements.\n",
       "\n",
       "Criteria:\n",
       "The output preserves the same person and background.\n",
       "The jacket matches the reference garment closely.\n",
       "Body shape and pose remain consistent outside normal garment effects.\n",
       "The result looks physically plausible.</pre></td><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>body_shape_preservation: 4.0\n",
       "facial_similarity: 5.0\n",
       "outfit_fidelity: 4.0\n",
       "verdict: PASS</pre></td><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>The edited output preserves the same face, hairstyle, pose, and plain studio background. The applied jacket closely matches the reference: camel/beige color, notch lapels, single-breasted look with dark buttons, and flap pockets are present and placed plausibly. Minor deviations include slightly different button count/placement and subtle differences in lapel/hem shaping compared to the flat lay. Body proportions and stance remain consistent, with only natural silhouette changes from wearing a structured blazer.</pre></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_result_table(case=vto_case, result=vto_result, title=\"Virtual Try-On: Prompt vs. Scores\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba23a4",
   "metadata": {},
   "source": [
    "### Logo Editing\n",
    "\n",
    "Logo editing is a high-precision image editing task. Given an **existing\n",
    "logo** and a **narrowly scoped instruction**, the model must apply the\n",
    "exact requested change while preserving everything else perfectly.\n",
    "Unlike creative design tasks, logo editing typically has a single\n",
    "correct answer. Any deviation, even subtle, is a failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251d846",
   "metadata": {},
   "source": [
    "<img src=\"../../images/logo_sample.png\" width=\"400\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcfe47",
   "metadata": {},
   "source": [
    "**Why Logo Editing evals are different**:\n",
    "\n",
    "Logo editing is judged on exactness, locality, and preservation rather\n",
    "than visual appeal:\n",
    "\n",
    "- Preserve the original asset identity  \n",
    "- Preserve all unedited text, geometry, spacing, and styling  \n",
    "- Edit only the explicitly requested region  \n",
    "- Produce character-level correctness with zero tolerance for drift\n",
    "\n",
    "Small errors carry outsized risk. A single letter distortion, number\n",
    "change, or spill outside the intended region can break brand integrity\n",
    "and create downstream rework.\n",
    "\n",
    "**Example Tasks:**\n",
    "\n",
    "- Inputs: logo image + mask or region description  \n",
    "- Tasks: “change the year from 2024 to 2026,” “replace C with S,” “add\n",
    "  TM”\n",
    "\n",
    "#### What to evaluate (practical metrics):\n",
    "\n",
    "##### 1) Treat logo editing as a constrained, single-reference task\n",
    "\n",
    "For each test case, store:\n",
    "\n",
    "- **Input logo image**  \n",
    "- **Edit instruction** with explicit scope  \n",
    "- Optional but high-leverage:\n",
    "  - Mask(s) or region description defining where edits are allowed  \n",
    "  - Expected target text or symbol for exact comparison  \n",
    "  - **Metadata**: font type, font size, or color\n",
    "\n",
    "The grader should compare the output directly against the original logo.\n",
    "\n",
    "##### 2) Graded metrics\n",
    "\n",
    "Use these **0–5** scores to rank models and track improvement. Scores\n",
    "are applied across all requested steps, not per step, so partial\n",
    "completion is penalized in a controlled and explainable way.\n",
    "\n",
    "##### A) Edit intent correctness — 0–5\n",
    "\n",
    "Measures whether every requested edit step was applied correctly to the\n",
    "correct target.\n",
    "\n",
    "- **5**: All edit steps are applied exactly as specified.\n",
    "  Character-level accuracy is perfect for every step.  \n",
    "- **4**: All steps applied correctly; extremely minor visual\n",
    "  imperfections only visible on close inspection.  \n",
    "- **3**: All steps applied, but at least one step shows noticeable\n",
    "  degradation in clarity or precision.  \n",
    "- **2**: Most steps applied correctly, but one or more steps contain a\n",
    "  meaningful error.  \n",
    "- **1**: One or more steps are incorrect or applied to the wrong\n",
    "  element.  \n",
    "- **0**: Most steps missing, incorrect, or misapplied.\n",
    "\n",
    "##### B) Non-target invariance — 0–5\n",
    "\n",
    "Measures whether content outside the requested edits remains unchanged\n",
    "across all steps.\n",
    "\n",
    "- **5**: No detectable changes outside the requested edits.  \n",
    "- **4**: Extremely minor drift visible only on close inspection.  \n",
    "- **3**: Noticeable but limited drift in nearby elements.  \n",
    "- **2**: Clear unrequested changes affecting adjacent text, symbols, or\n",
    "  background.  \n",
    "- **1**: Widespread unintended changes across the logo.  \n",
    "- **0**: Logo identity compromised.\n",
    "\n",
    "##### C) Character and style integrity — 0–5\n",
    "\n",
    "Logo editing is not creative transformation. The output must preserve\n",
    "the original asset’s identity including color, stroke, letterform, and\n",
    "icon consistency.\n",
    "\n",
    "- **5**: Edited characters and symbols perfectly match the original\n",
    "  style. Colors, strokes, letterforms, and icons are indistinguishable\n",
    "  from the original.  \n",
    "- **4**: Extremely minor deviation visible only on close inspection,\n",
    "  with no impact on brand perception.  \n",
    "- **3**: Noticeable but limited deviation in one or more properties that\n",
    "  does not break recognition.  \n",
    "- **2**: Clear inconsistency in color, stroke, letterform, or icon\n",
    "  geometry that affects visual cohesion.  \n",
    "- **1**: Major inconsistency that materially alters the logo’s\n",
    "  appearance.  \n",
    "- **0**: Visual system is corrupted or no longer recognizable.\n",
    "\n",
    "**LLM-as-judge rubric prompt**\n",
    "\n",
    "Below is a judge prompt aligned to your existing `LLMajRubricGrader`. It\n",
    "returns structured metric scores + an overall verdict.\n",
    "\n",
    "You can use this with your existing grader by changing the JSON schema\n",
    "to include the fields below (or create separate graders per metric if\n",
    "you prefer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30890d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"<core_mission>\n",
    "Evaluate whether a logo edit was executed with exact correctness,\n",
    "strict preservation, and high visual integrity.\n",
    "\n",
    "Logo editing is a precision task.\n",
    "Small errors matter.\n",
    "Near-misses are failures.\n",
    "</core_mission>\n",
    "\n",
    "<role>\n",
    "You are an expert evaluator of high-precision logo and brand asset editing.\n",
    "You specialize in detecting subtle text errors, unintended changes,\n",
    "and preservation drift across single-step and multi-step edits.\n",
    "</role>\n",
    "\n",
    "<scope_constraints>\n",
    "- Judge only against the provided edit instruction and input logo.\n",
    "- Do NOT judge aesthetics or visual appeal.\n",
    "- Do NOT infer intent beyond what is explicitly stated.\n",
    "- Be strict, conservative, and consistent across cases.\n",
    "</scope_constraints>\n",
    "\n",
    "<metrics_and_scoring>\n",
    "\n",
    "Evaluate EACH metric independently using the definitions below.\n",
    "All metrics are scored from 0 to 5.\n",
    "Scores apply across ALL requested edit steps.\n",
    "\n",
    "--------------------------------\n",
    "1) Edit Intent Correctness (0–5)\n",
    "--------------------------------\n",
    "Measures whether every requested edit step was applied correctly\n",
    "to the correct target.\n",
    "\n",
    "5: All edit steps applied exactly as specified. Character-level\n",
    "   accuracy is perfect for every step.\n",
    "4: All steps applied correctly with extremely minor visual\n",
    "   imperfections visible only on close inspection.\n",
    "3: All steps applied, but one or more steps show noticeable\n",
    "   degradation in clarity or precision.\n",
    "2: Most steps applied correctly, but one or more steps contain\n",
    "   a meaningful error.\n",
    "1: One or more steps are incorrect or applied to the wrong element.\n",
    "0: Most steps missing, incorrect, or misapplied.\n",
    "\n",
    "What to consider:\n",
    "- Exact character identity (letters, numbers, symbols)\n",
    "- Correct sequencing and targeting of multi-step edits\n",
    "- No ambiguous characters (Common confusions: 0 vs 6, O vs D, R vs B)\n",
    "\n",
    "--------------------------------\n",
    "2) Non-Target Invariance (0–5)\n",
    "--------------------------------\n",
    "Measures whether content outside the requested edits remains unchanged.\n",
    "\n",
    "5: No detectable changes outside the requested edits.\n",
    "4: Extremely minor drift visible only on close inspection.\n",
    "3: Noticeable but limited drift in nearby elements.\n",
    "2: Clear unrequested changes affecting adjacent text,\n",
    "   symbols, or background.\n",
    "1: Widespread unintended changes across the logo.\n",
    "0: Logo identity compromised.\n",
    "\n",
    "What to consider:\n",
    "- Adjacent letter deformation or spacing shifts\n",
    "- Background, texture, or color changes\n",
    "- Cumulative drift from multi-step edits\n",
    "\n",
    "--------------------------------\n",
    "3) Character and Style Integrity (0–5)\n",
    "--------------------------------\n",
    "Measures whether the edited content preserves the original\n",
    "logo’s visual system.\n",
    "\n",
    "This includes color, stroke weight, letterform structure,\n",
    "and icon geometry.\n",
    "\n",
    "5: Edited characters and symbols perfectly match the original\n",
    "   style. Colors, strokes, letterforms, and icons are\n",
    "   indistinguishable from the original.\n",
    "4: Extremely minor deviation visible only on close inspection,\n",
    "   with no impact on brand perception.\n",
    "3: Noticeable but limited deviation in one or more properties\n",
    "   that does not break recognition.\n",
    "2: Clear inconsistency in color, stroke, letterform, or icon\n",
    "   geometry that affects visual cohesion.\n",
    "1: Major inconsistency that materially alters the logo’s appearance.\n",
    "0: Visual system is corrupted or no longer recognizable.\n",
    "\n",
    "</metrics_and_scoring>\n",
    "\n",
    "<verdict_rules>\n",
    "- Edit Intent Correctness must be ≥ 4.\n",
    "- Non-Target Invariance must be ≥ 4.\n",
    "- Character and Style Integrity must be ≥ 4.\n",
    "\n",
    "If ANY metric falls below threshold, the overall verdict is FAIL.\n",
    "Do not average scores to determine the verdict.\n",
    "</verdict_rules>\n",
    "\n",
    "<consistency_rules>\n",
    "- Score conservatively.\n",
    "- If uncertain between two scores, choose the lower one.\n",
    "- Base all scores on concrete visual observations.\n",
    "- Penalize cumulative degradation across multi-step edits.\n",
    "</consistency_rules>\n",
    "\n",
    "<output_constraints>\n",
    "Return JSON only.\n",
    "No additional text.\n",
    "</output_constraints>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac8dec",
   "metadata": {},
   "source": [
    "#### Recommended JSON Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e41e448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verdict': 'PASS',\n",
       " 'edit_intent_correctness': 5,\n",
       " 'non_target_invariance': 5,\n",
       " 'character_and_style_integrity': 5,\n",
       " 'reason': '...'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"verdict\": \"PASS\",\n",
    "  \"edit_intent_correctness\": 5,\n",
    "  \"non_target_invariance\": 5,\n",
    "  \"character_and_style_integrity\": 5,\n",
    "  \"reason\": \"...\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96972358",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Logo Editing Example: Harness Setup\n",
    "\n",
    "Use an existing logo image and a narrowly scoped edit instruction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d2e52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the logo judge prompt.\n",
    "logo_judge_prompt = PROMPT\n",
    "\n",
    "logo_input_path = Path(\"../../images/logo_generation_1.png\")\n",
    "\n",
    "logo_prompt = \"\"\"Edit the logo by changing the text from FIELD to BUTTER .\n",
    "Do not change any other text, colors, shapes, or layout.\"\"\"\n",
    "\n",
    "logo_criteria = \"\"\"The requested edit is applied exactly.\n",
    "All non-target content remains unchanged.\n",
    "Character style, color, and geometry remain consistent with the original.\"\"\"\n",
    "\n",
    "logo_case = TestCase(\n",
    "    id=\"logo_year_edit\",\n",
    "    task_type=\"image_editing\",\n",
    "    prompt=logo_prompt,\n",
    "    criteria=logo_criteria,\n",
    "    image_inputs=ImageInputs(image_paths=[logo_input_path]),\n",
    ")\n",
    "\n",
    "logo_run = ModelRun(\n",
    "    label=\"gpt-image-1.5-logo\",\n",
    "    task_type=\"image_editing\",\n",
    "    params={\n",
    "        \"model\": \"gpt-image-1.5\",\n",
    "        \"n\": 1,\n",
    "    },\n",
    ")\n",
    "\n",
    "logo_store = OutputStore(root=Path(\"../../images\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba0af59",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Logo Editing Example: Run And Grade\n",
    "\n",
    "Run the harness and score the logo edit using the logo judge rubric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33a9a145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_id': 'logo_year_edit',\n",
       " 'model_label': 'gpt-image-1.5-logo',\n",
       " 'task_type': 'image_editing',\n",
       " 'artifact_paths': ['../../images/edit_logo_year_edit_gpt-image-1.5-logo_1769659071403_0.png'],\n",
       " 'scores': {'edit_intent_correctness': 5.0,\n",
       "  'non_target_invariance': 0.0,\n",
       "  'character_and_style_integrity': 2.0,\n",
       "  'verdict': 'FAIL'},\n",
       " 'reasons': {'edit_intent_correctness': '',\n",
       "  'non_target_invariance': '',\n",
       "  'character_and_style_integrity': '',\n",
       "  'verdict': 'Target text was correctly changed from “FIELD” to “BUTTER” (now reads “BUTTER & FLOUR”). However, major unrequested changes occurred: the background changed from a gray gradient to solid black, and the logo’s overall rendering/contrast differs (the original had a soft glow/embossed look, while the edited version is flatter with different tonal values). These violate the instruction to not change any other colors, shapes, or layout.'},\n",
       " 'run_params': {'model': 'gpt-image-1.5', 'n': 1}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logo_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"verdict\": {\"type\": \"string\"},\n",
    "        \"edit_intent_correctness\": {\"type\": \"number\"},\n",
    "        \"non_target_invariance\": {\"type\": \"number\"},\n",
    "        \"character_and_style_integrity\": {\"type\": \"number\"},\n",
    "        \"reason\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"verdict\",\n",
    "        \"edit_intent_correctness\",\n",
    "        \"non_target_invariance\",\n",
    "        \"character_and_style_integrity\",\n",
    "        \"reason\",\n",
    "    ],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def parse_logo_result(data: dict, base_key: str) -> list[Score]:\n",
    "    return [\n",
    "        Score(key=\"edit_intent_correctness\", value=float(data[\"edit_intent_correctness\"]), reason=\"\"),\n",
    "        Score(key=\"non_target_invariance\", value=float(data[\"non_target_invariance\"]), reason=\"\"),\n",
    "        Score(key=\"character_and_style_integrity\", value=float(data[\"character_and_style_integrity\"]), reason=\"\"),\n",
    "        Score(key=\"verdict\", value=str(data[\"verdict\"]), reason=(data.get(\"reason\") or \"\").strip()),\n",
    "    ]\n",
    "\n",
    "logo_grader = LLMajRubricGrader(\n",
    "    key=\"logo_eval\",\n",
    "    system_prompt=logo_judge_prompt,\n",
    "    content_builder=build_editing_judge_content,\n",
    "    judge_model=\"gpt-5.2\",\n",
    "    json_schema_name=\"logo_eval\",\n",
    "    json_schema=logo_schema,\n",
    "    result_parser=parse_logo_result,\n",
    ")\n",
    "\n",
    "logo_results = evaluate(\n",
    "    cases=[logo_case],\n",
    "    model_runs=[logo_run],\n",
    "    graders=[logo_grader],\n",
    "    output_store=logo_store,\n",
    ")\n",
    "\n",
    "logo_result = logo_results[0]\n",
    "logo_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761e8601",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Logo Editing Eval Results\n",
    "\n",
    "Show the edited logo and logo edit scores in a single pandas table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a05e1",
   "metadata": {},
   "source": [
    "<img src=\"../../images/edit_logo_year_edit_gpt-image-1.5-logo_1769659071403_0.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89ebfd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='font-weight:600; margin:6px 0'>Logo Editing: Prompt vs. Scores</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table style='width:100%; table-layout:fixed; border-collapse:collapse;'><colgroup><col style='width:33%'><col style='width:33%'><col style='width:33%'></colgroup><thead><tr><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Input Prompt</th><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Scores</th><th style='text-align:left; padding:8px; border-bottom:1px solid #ddd'>Reasoning</th></tr></thead><tbody><tr><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>Edit the logo by changing the text from FIELD to BUTTER .\n",
       "Do not change any other text, colors, shapes, or layout.\n",
       "\n",
       "Criteria:\n",
       "The requested edit is applied exactly.\n",
       "All non-target content remains unchanged.\n",
       "Character style, color, and geometry remain consistent with the original.</pre></td><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>character_and_style_integrity: 2.0\n",
       "edit_intent_correctness: 5.0\n",
       "non_target_invariance: 0.0\n",
       "verdict: FAIL</pre></td><td style='text-align:left; padding:8px; vertical-align:top'><pre style='white-space:pre-wrap; word-break:break-word; margin:0'>Target text was correctly changed from “FIELD” to “BUTTER” (now reads “BUTTER &amp; FLOUR”). However, major unrequested changes occurred: the background changed from a gray gradient to solid black, and the logo’s overall rendering/contrast differs (the original had a soft glow/embossed look, while the edited version is flatter with different tonal values). These violate the instruction to not change any other colors, shapes, or layout.</pre></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_result_table(case=logo_case, result=logo_result, title=\"Logo Editing: Prompt vs. Scores\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7856f51",
   "metadata": {},
   "source": [
    "##### 3) Alternative When You Have Ground Truth Images\n",
    "\n",
    "If you already have canonical references, you can compare directly to a\n",
    "**design source-of-truth** rather than model outputs.\n",
    "\n",
    "Ground truth should come from the design system, not generated images:\n",
    "\n",
    "- Brand library assets (vector or high-res canonical raster)\n",
    "- Figma frames or components representing the correct asset state\n",
    "\n",
    "This maps cleanly to logo editing requirements like “still the same asset”\n",
    "and “unintended drift is a failure.”\n",
    "\n",
    "**Example: OpenAI Logo – Lunar New Year**\n",
    "\n",
    "- Reference image: canonical OpenAI logo (from Figma or brand assets)\n",
    "- Generated image: model output with the requested transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1116fb1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Image models are shifting from “cool demos” to production tools that\n",
    "generate real artifacts—screens, flyers, product visuals, and brand\n",
    "edits that influence decisions and ship to customers. The core lesson of\n",
    "this cookbook is simple: you can’t evaluate these systems with generic\n",
    "“looks good” scores. You need **workflow-specific evals** that are\n",
    "**repeatable** across model versions, prompts, and settings. Multimodal\n",
    "LLMs make this practical by acting as scalable judges—when paired with\n",
    "tight rubrics, structured outputs, and human calibration.\n",
    "\n",
    "A practical vision-eval program starts small and gets sharper over time:\n",
    "\n",
    "- **Start with gates.** Add strict pass/fail graders for the failure\n",
    "  modes that break real work: missing required components, incorrect\n",
    "  copy, edits spilling outside the intended region, or unintended\n",
    "  changes to preserved areas. This prevents “pretty but wrong” outputs\n",
    "  from masking regressions.  \n",
    "- **Layer in graded metrics.** Once hard failures are controlled, use\n",
    "  0–5 rubrics to capture what matters for usability and quality in each\n",
    "  workflow (e.g., hierarchy in UI mockups, brand fit in marketing, or\n",
    "  fidelity/preservation in editing).  \n",
    "- **Tag failures to iterate faster.** Consistent failure tags turn a\n",
    "  pile of outputs into actionable engineering work: you can quantify\n",
    "  what’s breaking, find clustered root causes, and track progress as you\n",
    "  tune prompts, model settings, masks, or post-processing.  \n",
    "- **Use humans strategically.** Humans add the most value on subjective\n",
    "  or ambiguous dimensions (“vibe,” usability clarity, trustworthiness),\n",
    "  but only if you keep rubrics tight and use calibration anchors to\n",
    "  prevent drift.  \n",
    "- **Treat the harness as a product.** The reusable harness you\n",
    "  built—test cases, runners, graders, and stored artifacts—creates the\n",
    "  foundation for regression testing, parameter sweeps, and side-by-side\n",
    "  comparisons. Over time, your eval suite becomes your safety net: it\n",
    "  catches subtle failures early and makes improvements measurable.\n",
    "- **Evolve as needed.** You can redesign your grader prompts with the\n",
    "  complex metrics over time. One natural next step would be splitting \n",
    "  the single-shot grader prompt into multiple calls to focus on a\n",
    "  specific metric.\n",
    "\n",
    "Build evals that reflect how images are actually used, enforce\n",
    "correctness before aesthetics, and make iteration data-driven. When your\n",
    "evals are aligned with real workflow requirements, image generation and\n",
    "editing stop being unpredictable art projects and become tools teams can\n",
    "trust.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
