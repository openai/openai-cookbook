{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa8ba3e",
   "metadata": {},
   "source": [
    "# Image Evals for Image Generation and Editing Use Cases\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Image models are increasingly used in real product workflows—design\n",
    "mockups, marketing assets, virtual try-on, and high-precision edits to\n",
    "existing brand materials. To trust these systems in production, you need\n",
    "more than “does it look good?” You need **repeatable, workflow-specific\n",
    "evaluation** that measures whether outputs satisfy requirements, fail\n",
    "safely, and improve predictably over time.\n",
    "\n",
    "Vision evals are harder than text evals because the “answer” is an image\n",
    "that mixes:\n",
    "\n",
    "- **Hard constraints**: exact text, counts, attributes, locality\n",
    "  (“change only this region”).  \n",
    "- **Perceptual quality**: sharpness, coherence, realism, aesthetic\n",
    "  match.  \n",
    "- **Hidden failure modes**: subtle distortions, unintended edits, or\n",
    "  small text errors that look fine at a glance but break product\n",
    "  requirements—especially in editing.\n",
    "\n",
    "A good vision eval does **not** score “a pretty picture.” It scores\n",
    "whether the model is **reliable for a specific workflow**. Many images\n",
    "that look visually strong still fail because text is wrong, style is\n",
    "off-brand, or edits spill beyond the intended area. Image evals measure\n",
    "**quality, controllability, and usability** for real prompts—not just\n",
    "visual appeal.\n",
    "\n",
    "### What this guide covers\n",
    "\n",
    "This cookbook focuses on building a practical image-eval system for two\n",
    "major categories:\n",
    "\n",
    "**1) [Image generation evals](#image-generation-evals)**\n",
    "\n",
    "- Instruction following (constraints satisfied)  \n",
    "- Text rendering (accuracy, legibility, placement)  \n",
    "- Style control (aesthetic match, brand/character consistency)  \n",
    "- Preference alignment (rubric labels + pairwise comparisons)\n",
    "\n",
    "**2) [Image editing evals](#image-editing-evals) (optionally with input\n",
    "images and masks)**\n",
    "\n",
    "- Transformation correctness (the requested change is done exactly)  \n",
    "- Locality (edits happen only where intended)  \n",
    "- Preservation (unrequested regions remain unchanged)  \n",
    "- Spatial control (edits applied to the correct instance / region)\n",
    "\n",
    "**3) Human feedback alignment**\n",
    "\n",
    "- Rubric-based labels and pairwise preferences to capture subjective\n",
    "  quality and “vibe”  \n",
    "- Calibration techniques to keep human judgments consistent over time\n",
    "\n",
    "**4) A staged strategy for building evals**\n",
    "\n",
    "- Start with non-negotiable correctness gates  \n",
    "- Add graded quality metrics once failures are controlled  \n",
    "- Tag failure modes to drive targeted iteration\n",
    "\n",
    "## Building a Vision Eval Harness\n",
    "\n",
    "A vision eval harness is a small, repeatable system that turns “did this\n",
    "image work?” into **structured, comparable results** across prompts,\n",
    "models, and settings.\n",
    "\n",
    "At a high level, vision evals follow the same loop as any LLM eval\n",
    "system:\n",
    "\n",
    "**Inputs → Model → Outputs → Graders → Scores → Feedback → Improvement**\n",
    "\n",
    "To make this reusable across the rest of the cookbook (generation +\n",
    "editing), build the harness around **three plug-ins**:\n",
    "\n",
    "1.  **Test cases:** what to run (prompt + criteria + optional input\n",
    "    images/mask)  \n",
    "2.  **Runners:** how to call a model and save output images  \n",
    "3.  **Graders:** how to score an output (rubrics, LLM-as-judge, human\n",
    "    labels later)\n",
    "\n",
    "Below is a minimal `vision_harness/` package you can drop into your\n",
    "repo.\n",
    "\n",
    "### vision_harness/[types.py](http://types.py)\n",
    "\n",
    "Keep the core types generic so you can reuse them for *both* image\n",
    "generation and image editing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d89fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal, Optional\n",
    "\n",
    "TaskType = Literal[\"image_generation\", \"image_editing\"]\n",
    "ScoreValue = bool | int | float | str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ImageInputs:\n",
    "    \"\"\"Editing inputs: one or more reference images + optional mask.\"\"\"\n",
    "    image_paths: list[Path]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TestCase:\n",
    "    \"\"\"A single evaluable example.\"\"\"\n",
    "    id: str\n",
    "    task_type: TaskType\n",
    "    prompt: str\n",
    "    criteria: str\n",
    "    image_inputs: Optional[ImageInputs] = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelRun:\n",
    "    \"\"\"One model configuration to evaluate (useful for sweeps).\"\"\"\n",
    "    label: str\n",
    "    task_type: TaskType\n",
    "    params: dict[str, Any]  # e.g. {\"model\": \"...\", \"quality\": \"...\", ...}\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Artifact:\n",
    "    \"\"\"A saved artifact from a run (usually an image).\"\"\"\n",
    "    kind: Literal[\"image\"]\n",
    "    path: Path\n",
    "    mime: str = \"image/png\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelResponse:\n",
    "    \"\"\"Normalized output from any runner.\"\"\"\n",
    "    artifacts: list[Artifact] = field(default_factory=list)\n",
    "    raw: dict[str, Any] = field(default_factory=dict)  # optional debug payload\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Score:\n",
    "    key: str\n",
    "    value: ScoreValue\n",
    "    reason: str = \"\"\n",
    "    tags: Optional[list[str]] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a36d9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### vision_harness/[io.py](http://io.py)\n",
    "\n",
    "You’ll use this in graders (LLM-as-judge) and sometimes in model calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "_MIME_BY_SUFFIX = {\n",
    "    \".jpg\": \"image/jpeg\",\n",
    "    \".jpeg\": \"image/jpeg\",\n",
    "    \".png\": \"image/png\",\n",
    "    \".webp\": \"image/webp\",\n",
    "}\n",
    "\n",
    "def image_to_data_url(path: Path) -> str:\n",
    "    mime = _MIME_BY_SUFFIX.get(path.suffix.lower(), \"image/png\")\n",
    "    b64 = base64.b64encode(path.read_bytes()).decode(\"utf-8\")\n",
    "    return f\"data:{mime};base64,{b64}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee65cce3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### vision_harness/[storage.py](http://storage.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff02bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class OutputStore:\n",
    "    \"\"\"Deterministic directory layout for saved artifacts.\"\"\"\n",
    "    root: Path\n",
    "\n",
    "    def run_dir(self, test_id: str, model_label: str) -> Path:\n",
    "        safe_test = test_id.replace(\"/\", \"_\")\n",
    "        safe_model = model_label.replace(\"/\", \"_\")\n",
    "        d = self.root / safe_test / safe_model\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        return d\n",
    "\n",
    "    def new_basename(self, prefix: str) -> str:\n",
    "        created_ms = int(time.time() * 1000)\n",
    "        return f\"{prefix}_{created_ms}\"\n",
    "\n",
    "    def save_png(self, run_dir: Path, basename: str, idx: int, png_bytes: bytes) -> Path:\n",
    "        out = run_dir / f\"{basename}_{idx}.png\"\n",
    "        out.write_bytes(png_bytes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b6c49",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### vision_harness/sweeps.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d06d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from itertools import product\n",
    "from typing import Any\n",
    "\n",
    "from .types import ModelRun, TaskType\n",
    "\n",
    "def grid_sweep(\n",
    "    *,\n",
    "    base_label: str,\n",
    "    task_type: TaskType,\n",
    "    fixed: dict[str, Any],\n",
    "    grid: dict[str, list[Any]],\n",
    ") -> list[ModelRun]:\n",
    "    keys = list(grid.keys())\n",
    "    runs: list[ModelRun] = []\n",
    "\n",
    "    for values in product(*[grid[k] for k in keys]):\n",
    "        params = dict(fixed)\n",
    "        label_parts = [base_label]\n",
    "        for k, v in zip(keys, values):\n",
    "            params[k] = v\n",
    "            label_parts.append(f\"{k}={v}\")\n",
    "        runs.append(ModelRun(label=\",\".join(label_parts), task_type=task_type, params=params))\n",
    "\n",
    "    return runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a4099",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### vision_harness/runners.py\n",
    "\n",
    "Two runners: one for **generation**, one for **editing**. Both return a\n",
    "normalized `ModelResponse` containing saved output images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43214dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import base64\n",
    "from typing import Optional\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from .storage import OutputStore\n",
    "from .types import Artifact, ModelResponse, ModelRun, TestCase\n",
    "\n",
    "def _extract_b64_items(images_response) -> list[str]:\n",
    "    b64_items: list[str] = []\n",
    "    for item in getattr(images_response, \"data\", []) or []:\n",
    "        b64 = getattr(item, \"b64_json\", None)\n",
    "        if b64:\n",
    "            b64_items.append(b64)\n",
    "    return b64_items\n",
    "\n",
    "\n",
    "class ImageGenerationRunner:\n",
    "    \"\"\"Text-to-image runner.\"\"\"\n",
    "\n",
    "    def __init__(self, client: Optional[OpenAI] = None):\n",
    "        self.client = client or OpenAI()\n",
    "\n",
    "    def run(self, case: TestCase, run_cfg: ModelRun, store: OutputStore) -> ModelResponse:\n",
    "        assert case.task_type == \"image_generation\"\n",
    "        assert run_cfg.task_type == \"image_generation\"\n",
    "\n",
    "        params = dict(run_cfg.params)\n",
    "        model = params.pop(\"model\")\n",
    "        n = int(params.pop(\"n\", 1))\n",
    "\n",
    "        run_dir = store.run_dir(case.id, run_cfg.label)\n",
    "        basename = store.new_basename(\"gen\")\n",
    "\n",
    "        images_response = self.client.images.generate(\n",
    "            model=model,\n",
    "            prompt=case.prompt,\n",
    "            n=n,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        artifacts: list[Artifact] = []\n",
    "        for idx, b64_json in enumerate(_extract_b64_items(images_response)):\n",
    "            png_bytes = base64.b64decode(b64_json)\n",
    "            out_path = store.save_png(run_dir, basename, idx, png_bytes)\n",
    "            artifacts.append(Artifact(kind=\"image\", path=out_path))\n",
    "\n",
    "        return ModelResponse(artifacts=artifacts, raw={\"model\": model, \"params\": run_cfg.params})\n",
    "\n",
    "\n",
    "class ImageEditRunner:\n",
    "    \"\"\"Image editing runner (reference image(s) + optional mask).\"\"\"\n",
    "\n",
    "    def __init__(self, client: Optional[OpenAI] = None):\n",
    "        self.client = client or OpenAI()\n",
    "\n",
    "    def run(self, case: TestCase, run_cfg: ModelRun, store: OutputStore) -> ModelResponse:\n",
    "        assert case.task_type == \"image_editing\"\n",
    "        assert run_cfg.task_type == \"image_editing\"\n",
    "        assert case.image_inputs is not None and case.image_inputs.image_paths\n",
    "\n",
    "        params = dict(run_cfg.params)\n",
    "        model = params.pop(\"model\")\n",
    "        n = int(params.pop(\"n\", 1))\n",
    "\n",
    "        run_dir = store.run_dir(case.id, run_cfg.label)\n",
    "        basename = store.new_basename(\"edit\")\n",
    "\n",
    "        image_bytes_list = [p.read_bytes() for p in case.image_inputs.image_paths]\n",
    "        \n",
    "\n",
    "        images_response = self.client.images.edit(\n",
    "            model=model,\n",
    "            prompt=case.prompt,\n",
    "            image=image_bytes_list,\n",
    "            \n",
    "            n=n,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        artifacts: list[Artifact] = []\n",
    "        for idx, b64_json in enumerate(_extract_b64_items(images_response)):\n",
    "            png_bytes = base64.b64decode(b64_json)\n",
    "            out_path = store.save_png(run_dir, basename, idx, png_bytes)\n",
    "            artifacts.append(Artifact(kind=\"image\", path=out_path))\n",
    "\n",
    "        return ModelResponse(artifacts=artifacts, raw={\"model\": model, \"params\": run_cfg.params})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84cbfa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### vision_harness/graders.py\n",
    "\n",
    "A clean grader interface + a reusable **LLM-as-judge** grader that can\n",
    "be used for both generation and editing by changing how you build the\n",
    "judge inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279428a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional, Protocol\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from .io import image_to_data_url\n",
    "from .types import ModelResponse, Score, TestCase\n",
    "\n",
    "\n",
    "class Grader(Protocol):\n",
    "    key: str\n",
    "    def grade(self, response: ModelResponse, case: TestCase) -> Score: ...\n",
    "\n",
    "\n",
    "def pick_first_image(response: ModelResponse) -> Optional[Path]:\n",
    "    for a in response.artifacts:\n",
    "        if a.kind == \"image\":\n",
    "            return a.path\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_generation_judge_content(case: TestCase, output_image: Path) -> list[dict]:\n",
    "    return [\n",
    "        {\"type\": \"text\", \"text\": f\"Prompt:\\n{case.prompt}\\n\\nCriteria:\\n{case.criteria}\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": image_to_data_url(output_image)}},\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_editing_judge_content(case: TestCase, output_image: Path) -> list[dict]:\n",
    "    assert case.image_inputs is not None and case.image_inputs.image_paths\n",
    "    content: list[dict] = [{\"type\": \"text\", \"text\": f\"Edit instruction:\\n{case.prompt}\\n\\nCriteria:\\n{case.criteria}\"}]\n",
    "    for p in case.image_inputs.image_paths:\n",
    "        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": image_to_data_url(p)}})\n",
    "    if case.image_inputs.mask_path:\n",
    "        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": image_to_data_url(case.image_inputs.mask_path)}})\n",
    "    content.append({\"type\": \"image_url\", \"image_url\": {\"url\": image_to_data_url(output_image)}})\n",
    "    return content\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMajRubricGrader:\n",
    "    \"\"\"\n",
    "    Reusable vision LLM-as-judge grader.\n",
    "    - You provide a system prompt + a content_builder for generation/editing.\n",
    "    - Output is strict JSON {pass: bool, reason: str}.\n",
    "    \"\"\"\n",
    "    key: str\n",
    "    system_prompt: str\n",
    "    content_builder: Callable[[TestCase, Path], list[dict]]\n",
    "    judge_model: str = \"gpt-5.2\"\n",
    "    client: Optional[OpenAI] = None\n",
    "\n",
    "    def grade(self, response: ModelResponse, case: TestCase) -> Score:\n",
    "        out_img = pick_first_image(response)\n",
    "        if not out_img:\n",
    "            return Score(key=self.key, value=False, reason=\"No output image artifact found\")\n",
    "\n",
    "        client = self.client or OpenAI()\n",
    "        content = self.content_builder(case, out_img)\n",
    "\n",
    "        completion = client.responses.create(\n",
    "            model=self.judge_model,\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": content},\n",
    "            ],\n",
    "            text={\n",
    "                \"format\": {\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"name\": \"vision_eval_result\",\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"pass\": {\"type\": \"boolean\"},\n",
    "                            \"reason\": {\"type\": \"string\"},\n",
    "                            \"tags\": {\"type\": \"array\",\n",
    "                                    \"items\": { \"type\": \"string\" }\n",
    "\n",
    "                        },\n",
    "                        \"required\": [\"pass\", \"reason\"],\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                    \"strict\": True,\n",
    "                }\n",
    "}   \n",
    "            },\n",
    "        )\n",
    "\n",
    "        data = json.loads(completion.output_text)\n",
    "        return Score(\n",
    "            key=self.key,\n",
    "            value=bool(data[\"pass\"]),\n",
    "            reason=(data.get(\"reason\") or \"\").strip(),\n",
    " tags = data.get(\"tags\") or []\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc922df2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### vision_harness/evaluate.py\n",
    "\n",
    "A simple evaluation loop that returns plain Python data (no “eval row”\n",
    "class). Later sections can write their own reporting/CSV utilities on\n",
    "top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d024c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from .graders import Grader\n",
    "from .runners import ImageEditRunner, ImageGenerationRunner\n",
    "from .storage import OutputStore\n",
    "from .types import ModelRun, TestCase\n",
    "\n",
    "def evaluate(\n",
    "    *,\n",
    "    cases: list[TestCase],\n",
    "    model_runs: list[ModelRun],\n",
    "    graders: list[Grader],\n",
    "    output_store: OutputStore,\n",
    ") -> list[dict[str, Any]]:\n",
    "    gen_runner = ImageGenerationRunner()\n",
    "    edit_runner = ImageEditRunner()\n",
    "\n",
    "    results: list[dict[str, Any]] = []\n",
    "\n",
    "    for case in cases:\n",
    "        for run_cfg in model_runs:\n",
    "            if run_cfg.task_type != case.task_type:\n",
    "                continue\n",
    "\n",
    "            if case.task_type == \"image_generation\":\n",
    "                response = gen_runner.run(case, run_cfg, output_store)\n",
    "            elif case.task_type == \"image_editing\":\n",
    "                response = edit_runner.run(case, run_cfg, output_store)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown task_type: {case.task_type}\")\n",
    "\n",
    "            score_map: dict[str, Any] = {}\n",
    "            reason_map: dict[str, str] = {}\n",
    "\n",
    "            for g in graders:\n",
    "                s = g.grade(response, case)\n",
    "                score_map[s.key] = s.value\n",
    "                reason_map[s.key] = s.reason\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"test_id\": case.id,\n",
    "                    \"model_label\": run_cfg.label,\n",
    "                    \"task_type\": case.task_type,\n",
    "                    \"artifact_paths\": [str(a.path) for a in response.artifacts],\n",
    "                    \"scores\": score_map,\n",
    "                    \"reasons\": reason_map,\n",
    "                    \"run_params\": run_cfg.params,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69bf1e5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Later cookbook sections will only need to provide:\n",
    "\n",
    "1.  A list of `TestCase` objects for the use case (UI mockups, marketing\n",
    "    graphics, virtual try-on, logo edits, etc.)  \n",
    "2.  A set of `ModelRun`s (single run or a sweep with `grid_sweep`)  \n",
    "3.  A set of graders, typically:\n",
    "    - **Gating graders** (pass/fail): non-negotiables ex: instruction\n",
    "      following, text correctness, locality/preservation  \n",
    "    - **Graded metrics** (0–5): quality dimensions ex:\n",
    "      realism/usability, layout/hierarchy, artifact severity  \n",
    "    - Optional: human rubric labels and pairwise prefs (stored outside\n",
    "      the harness)\n",
    "\n",
    "This keeps the rest of the cookbook focused on **what to measure** per\n",
    "use case—without rewriting harness plumbing each time.\n",
    "\n",
    "## Image Generation Evals\n",
    "\n",
    "**Use Case Ideas: UI mockups, marketing graphics/posters**\n",
    "\n",
    "**Goal: evaluate text-to-image quality, controllability, and usefulness\n",
    "for real prompts**  \n",
    "Covers:\n",
    "\n",
    "- **instruction following** (constraints satisfied),  \n",
    "- **Text rendering** (generated text is accurate, legible, and placed\n",
    "  correctly),  \n",
    "- **Styling** (requested aesthetic + visual quality, may include brand,\n",
    "  character, and product consistency)  \n",
    "- **Human feedback alignment** (rubric-based labels + pairwise prefs).\n",
    "\n",
    "Image generation models are used to create artifacts that influence real\n",
    "work: UI mockups, product designs, marketing graphics, and internal\n",
    "presentations. In these contexts, images are not judged purely on visual\n",
    "appeal. They are judged on whether they communicate intent, follow\n",
    "constraints, and are usable by downstream stakeholders.\n",
    "\n",
    "### UI Mockups\n",
    "\n",
    "UI mockups generated by image models are increasingly used for early\n",
    "product exploration, design ideation, and internal reviews. In these\n",
    "workflows, mockups are not just visual inspiration. They are\n",
    "communication tools that help designers, engineers, and stakeholders\n",
    "reason about layout, hierarchy, interaction intent, and feature scope\n",
    "before anything is built. As a result, success is defined less by\n",
    "aesthetic taste and more by whether the output can plausibly function as\n",
    "a product artifact.\n",
    "\n",
    "This section shows how to evaluate a generated UI mockup using a screen\n",
    "level example. The goal is to evaluate whether the generated UI consists\n",
    "of a coherent layout, uses recognizable components, and represents a\n",
    "plausible product experience.\n",
    "\n",
    "**Why UI mockup evals are different**\n",
    "\n",
    "UI mockups combine several difficult evaluation dimensions:\n",
    "\n",
    "1.  **Component Fidelity (gating)**  \n",
    "    The generated image must clearly depict the requested screen type\n",
    "    and state. Buttons should look clickable, inputs should look\n",
    "    editable, and navigation should look like navigation.  \n",
    "2.  **Layout Realization (graded)**  \n",
    "    A good UI mockup communicates instantly what the user can do and\n",
    "    what matters most. The layout should make primary actions obvious,\n",
    "    secondary actions clearly subordinate, and information grouped in a\n",
    "    way that reflects real interaction flow  \n",
    "3.  **In-Image Text Rendering (gating)**  \n",
    "    UI text encodes functionality. Labels, headings, and calls to action\n",
    "    must be legible and correctly rendered.\n",
    "\n",
    "**Example Tasks: Mobile app checkout screen generation**\n",
    "\n",
    "**Scenario**: A consumer ecommerce app needs a mobile checkout screen to\n",
    "review an order and complete payment.\n",
    "\n",
    "**Prompt Template**: Use prompts that are explicit about the screen\n",
    "type, platform, required elements, and interaction hierarchy. Here is a\n",
    "strong baseline prompt you can use in your `TestCase.prompt`:\n",
    "\n",
    "**Prompt**:\n",
    "\n",
    "- Generate a high-fidelity **mobile checkout screen** for an ecommerce app  \n",
    "- Orientation: **portrait**  \n",
    "- Screen type: checkout / order review  \n",
    "- Header with title text (must be exact): **“Checkout\"**\n",
    "- Order summary product row with:\n",
    "  - Product name (short, readable label)  \n",
    "  - Quantity indicator (e.g., “Qty 1”)  \n",
    "  - Item price  \n",
    "- Payment method:\n",
    "  - One selected payment method shown  \n",
    "  - Secondary action to change payment method  \n",
    "- Order total:\n",
    "  - Subtotal line  \n",
    "  - Shipping line  \n",
    "  - Total line (must be visually emphasized)  \n",
    "- Primary action:\n",
    "  - Button text (must be exact): **“Place Order\"**  \n",
    "- Secondary action:\n",
    "  - Link text (must be exact): **“Edit Cart\"** \n",
    "- Layout constraints:\n",
    "  - Order total appears directly above the primary CTA  \n",
    "  - Primary CTA is the most visually prominent element  \n",
    "  - Secondary action is clearly less prominent  \n",
    "  - Use standard mobile UI patterns (rows, dividers, buttons)  \n",
    "  - All text must be legible and realistic  \n",
    "  - Do **not** include popups, ads, marketing copy, or additional screens  \n",
    "  - Do **not** include placeholder or lorem-ipsum text\n",
    "\n",
    "**Criteria (paired with the prompt)**\n",
    "\n",
    "- The image clearly depicts a mobile checkout screen and all required\n",
    "  sections are present and visually distinct  \n",
    "- UI elements look clickable/editable and follow common conventions  \n",
    "- Primary vs secondary actions are unambiguous  \n",
    "- No extra UI states, decorative noise, or placeholder text\n",
    "\n",
    "#### What to evaluate (practical metrics):\n",
    "\n",
    "##### 1) Instruction Following — Pass / Fail (gate)\n",
    "\n",
    "Most UI mockup evals start with instruction fidelity. At a basic level,\n",
    "the question is simple: did the model generate the screen that was asked\n",
    "for? If the prompt specifies a mobile checkout screen, the output should\n",
    "look like a mobile checkout screen, not a generic landing page. Required\n",
    "sections, states, or constraints should be present, and the overall\n",
    "structure should match the intent of the request. This dimension is\n",
    "often the most heavily weighted, because if the model misses the core\n",
    "ask, the rest of the output does not matter. Once a UI mockup satisfies\n",
    "the basic instruction, evaluation shifts to whether the screen is usable\n",
    "and coherent as a product artifact.\n",
    "\n",
    "**PASS if**\n",
    "\n",
    "- The correct screen type and platform context are present.  \n",
    "- All required sections, states, or constraints are included.\n",
    "\n",
    "**FAIL if**\n",
    "\n",
    "- Missing any required components or the output alters the UI’s purpose.\n",
    "\n",
    "##### 2) Layout and Hierarchy — 0–5\n",
    "\n",
    "Measures whether the screen communicates clearly at a glance.\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "- Clear visual hierarchy between primary and secondary actions  \n",
    "- Consistent alignment and spacing  \n",
    "- Logical grouping of related elements\n",
    "\n",
    "##### 3) Text rendering and legibility — Pass / Fail (gate)\n",
    "\n",
    "Labels, headings, and calls to action need to be readable and\n",
    "unambiguous.\n",
    "\n",
    "**PASS if**\n",
    "\n",
    "- Text is readable, correctly spelled, and sensibly labeled.  \n",
    "- Font sizes reflect hierarchy (headings vs labels vs helper text).\n",
    "\n",
    "**FAIL if**\n",
    "\n",
    "- Any critical text is unreadable, cut off, misspelled, or distorted.\n",
    "\n",
    "##### 4) UI Realism and Usability — *0–5*\n",
    "\n",
    "Measures whether the mockup resembles a plausible product interface.\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "- Inputs look editable / Buttons look clickable  \n",
    "- Navigation looks like navigation  \n",
    "- Interactive elements are visually distinct from static content\n",
    "\n",
    "**Verdict rules (how to convert metrics into a single pass/fail)**\n",
    "\n",
    "- **Instruction Following must PASS**  \n",
    "- **Layout and Hierarchy ≥ 3**  \n",
    "- **Text Rendering must PASS**  \n",
    "- **UI Realism ≥ 3**\n",
    "\n",
    "If any rule fails → overall **FAIL**.\n",
    "\n",
    "**Example `TestCase` set (small but high-signal)**\n",
    "\n",
    "Start with a few cases that cover common UI mockup variants and edge\n",
    "cases.\n",
    "\n",
    "1.  **Mobile** **checkout screen** (the “checkout UI mockup” prompt\n",
    "    above)  \n",
    "2.  **Minimal layout** (header + order total + primary CTA only) - tests\n",
    "    layout and hierarchy with sparse elements.  \n",
    "3.  **Dense hierarchy** (order summary + payment method + promo row +\n",
    "    taxes + two secondary actions) - tests hierarchy under information\n",
    "    load.  \n",
    "4.  **Exact CTA text** (primary button text must be exactly “Place\n",
    "    Order”) - tests in-image text rendering fidelity.  \n",
    "5.  **Secondary action presence (i**nclude both “Edit Cart” and “Change\n",
    "    payment method”) - tests secondary actions remain visually\n",
    "    subordinate.  \n",
    "6.  **Placement constraint** (require order total directly above the\n",
    "    primary CTA)  \n",
    "7.  **Platform fidelity** (mobile vs desktop version) - tests screen\n",
    "    framing and platform cues.\n",
    "\n",
    "**LLM-as-judge rubric prompt**\n",
    "\n",
    "Below is a judge prompt aligned to your existing `LLMajRubricGrader`. It\n",
    "returns structured metric scores + an overall verdict.\n",
    "\n",
    "You can use this with your existing grader by changing the JSON schema\n",
    "to include the fields below (or create separate graders per metric if\n",
    "you prefer).\n",
    "\n",
    "#### System Prompt (UI mockup judge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e86aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"<core_mission>\n",
    "Evaluate whether a generated UI mockup image represents a usable mobile checkout screen by checking screen type fidelity, layout realization, in-image text rendering, and UI affordance clarity.\n",
    "</core_mission>\n",
    "\n",
    "<role>\n",
    "You are an expert evaluator of UI mockups used by designers and engineers. You care about structural correctness, readable UI text, clear hierarchy,and realistic rendering of UI elements.\n",
    "</role>\n",
    "\n",
    "<scope_constraints>\n",
    "- Judge only against the provided instructions.\n",
    "- Be strict about required UI elements and exact button/link text.\n",
    "- Do NOT infer intent beyond what is explicitly stated.\n",
    "- Do NOT reward creativity that violates constraints.\n",
    "- Missing or extra required components are serious errors.\n",
    "- If the UI intent or function is unclear, score conservatively.\n",
    "</scope_constraints>\n",
    "\n",
    "<metrics>\n",
    "1) instruction_following: PASS/FAIL\n",
    "2) ui_layout_realization: 0–5\n",
    "3) in_image_text_rendering: PASS / FAIL\n",
    "4) ui_affordance_rendering: 0–5\n",
    "</metrics>\n",
    "\n",
    "Use these anchors:\n",
    "Evaluate EACH metric independently using the definitions below.\n",
    "--------------------------------\n",
    "1) Instruction Following (PASS / FAIL)\n",
    "--------------------------------\n",
    "PASS if:\n",
    "- All required components are present.\n",
    "- No unrequested components or features are added.\n",
    "- The screen matches the requested type and product context.\n",
    "\n",
    "FAIL if:\n",
    "- Any required component is missing.\n",
    "- Any unrequested component materially alters the UI.\n",
    "- The screen does not match the requested type.\n",
    "\n",
    "--------------------------------\n",
    "2) Layout and Hierarchy (0–5)\n",
    "--------------------------------\n",
    "5: Layout is clear, coherent, and immediately usable.\n",
    "   Hierarchy, grouping, spacing, and alignment are strong.\n",
    "\n",
    "3: Generally understandable, but one notable hierarchy or layout issue\n",
    "   that would require iteration.\n",
    "\n",
    "0-2: Layout problems materially hinder usability or comprehension.\n",
    "\n",
    "--------------------------------\n",
    "3) In Image Text Rendering (PASS / FAIL)\n",
    "--------------------------------\n",
    "PASS if\n",
    "- Text is readable, correctly spelled, and sensibly labeled.\n",
    "- Font sizes reflect hierarchy (headings vs labels vs helper text).\n",
    "\n",
    "FAIL if\n",
    "- Any critical text is unreadable, cut off, misspelled, or distorted.\n",
    "\n",
    "--------------------------------\n",
    "4) UI Realism and Usability (0–5)\n",
    "--------------------------------\n",
    "5: Clearly resembles a real product interface that designers could use.\n",
    "\n",
    "3: Marginally plausible; intent is visible but execution is weak.\n",
    "\n",
    "0-2: Poor realism; interface would be difficult to use in practice.\n",
    "\n",
    "<verdict_rules>\n",
    "- Instruction Following must PASS.\n",
    "- Text Rendering must PASS.\n",
    "- Layout and Hierarchy score must be ≥ 3.\n",
    "- UI Realism and Usability score must be ≥ 3.\n",
    "\n",
    "If ANY rule fails, the overall verdict is FAIL.\n",
    "Do not average scores to determine the verdict.\n",
    "</verdict_rules>\n",
    "\n",
    "<output_constraints>\n",
    "Return JSON only.\n",
    "No extra text.\n",
    "</output_constraints>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd0f96a",
   "metadata": {},
   "source": [
    "#### Recommended JSON Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bd39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"verdict\": \"PASS\",\n",
    "  \"instruction_following\": True,\n",
    "  \"layout_hierarchy\": 3,\n",
    "  \"in_image_text_rendering\": True,\n",
    "  \"ui_affordance_rendering\": 4,\n",
    "  \"reason\": \"...\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "63fbe396",
   "source": [
    "##### 5) Human feedback (quick and high leverage)\n",
    "\n",
    "UI \"usability clarity\" is where humans add the most value. Keep it lightweight and focused on interaction intent.\n",
    "\n",
    "- Rubric labels (fast):\n",
    "  - Would you use this mockup to iterate on a real product screen? (Y/N)\n",
    "  - If N, why? (missing section, action button unclear, text unreadable, layout confusing, etc.)\n",
    "  - Overall usability clarity: 1–5\n",
    "- Common failure tags (for debugging + iteration):\n",
    "  - `wrong_screen_type`\n",
    "  - `missing_required_section`\n",
    "  - `extra_ui_elements`\n",
    "  - `primary_cta_not_clear`\n",
    "  - `text_unreadable_or_garbled`\n",
    "  - `affordances_unclear`\n",
    "  - `layout_confusing`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fef54",
   "metadata": {},
   "source": [
    "### Marketing Graphics Generation\n",
    "\n",
    "Marketing graphics are a “high-stakes text-to-image” workflow: the\n",
    "output is meant to ship (or at least be reviewed as if it could ship).\n",
    "Unlike purely creative generations, marketing assets have\n",
    "*non-negotiables*—exact copy, correct offer details, clear hierarchy,\n",
    "and an on-brand look. A flyer can look “nice” but still fail if the\n",
    "discount is wrong, the CTA is unreadable, or the layout makes the key\n",
    "message unclear.\n",
    "\n",
    "This section shows how to evaluate **flyer generation** with a coffee\n",
    "shop example. The goal is to make evaluation *workflow-relevant*: can a\n",
    "marketer or designer use this output with minimal edits?\n",
    "\n",
    "**Why marketing graphics evals are different**\n",
    "\n",
    "Marketing images combine three hard problem types:\n",
    "\n",
    "1.  **Exact text requirements (gating)**  \n",
    "    A single character error breaks the asset (wrong price, wrong date,\n",
    "    misspelled brand name).  \n",
    "2.  **Layout + hierarchy (graded)**  \n",
    "    A good flyer communicates instantly: what it is, why you should\n",
    "    care, what to do next.  \n",
    "3.  **Style + brand consistency (graded + optional human prefs)**  \n",
    "    Brand consistency is often subjective, but you can still measure it\n",
    "    reliably with rubrics and pairwise comparisons.\n",
    "\n",
    "**Example task: Coffee shop flyer generation**\n",
    "\n",
    "**Scenario:** A local coffee shop needs a promotional flyer for a\n",
    "limited-time drink and a weekday deal.\n",
    "\n",
    "**Prompt template (single flyer)**  \n",
    "Use prompts that are explicit about the deliverable, size, copy, and\n",
    "hierarchy. Here’s a strong baseline prompt you can use in your\n",
    "`TestCase.prompt`:\n",
    "\n",
    "**Prompt**\n",
    "\n",
    "- Create a *print-ready* **flyer** for a coffee shop called **Sunrise\n",
    "  Coffee**.  \n",
    "- Format: **vertical A4** (portrait), with a clean margin and clear\n",
    "  typographic hierarchy.  \n",
    "- Theme: warm, cozy, minimal, *specialty coffee aesthetic* (not\n",
    "  cartoonish).  \n",
    "- Main headline text (must be exact): **“WINTER LATTE WEEK”**  \n",
    "- Subheadline text (must be exact): **“Try our Cinnamon Oat Latte”**  \n",
    "- Offer badge text (must be exact): **“20% OFF • Mon–Thu”**  \n",
    "- CTA button text (must be exact): **“Order Ahead”**  \n",
    "- Footer text (must be exact): **“123 Market St • 7am–6pm”**  \n",
    "- Include: one appetizing hero image of the latte (photo-real style),\n",
    "  one small icon set (coffee bean + snowflake), and a subtle textured\n",
    "  background.  \n",
    "- Do **not** include any other words, prices, URLs, or QR codes.\n",
    "\n",
    "**Criteria (paired with the prompt)**\n",
    "\n",
    "- All required text appears **exactly** as written, is legible, and\n",
    "  placed in appropriate hierarchy.  \n",
    "- Layout reads clearly: headline → subheadline → offer → CTA → footer.  \n",
    "- Style matches “warm, cozy, specialty coffee” and is not cartoonish.  \n",
    "- No extra text, watermarks, or irrelevant UI-like elements.\n",
    "\n",
    "#### What to evaluate (practical metrics):\n",
    "\n",
    "Use a mix of **gates** (hard constraints) and **graded** signals\n",
    "(quality).\n",
    "\n",
    "##### 1) Instruction following — Pass / Fail (gate)\n",
    "\n",
    "Marketing assets fail fast if they don’t match the spec.\n",
    "\n",
    "**PASS if**\n",
    "\n",
    "- Correct deliverable type: clearly a flyer/poster layout (not a UI\n",
    "  screen, not a random photo).  \n",
    "- Required components exist (headline/subheadline/offer/CTA/footer +\n",
    "  hero image).\n",
    "\n",
    "**FAIL if**\n",
    "\n",
    "- Missing any required component or the output is not recognizably a\n",
    "  flyer.\n",
    "\n",
    "##### 2) Text rendering accuracy — Pass / Fail (gate)\n",
    "\n",
    "This is usually the #1 production failure mode.\n",
    "\n",
    "**PASS if**\n",
    "\n",
    "- All required text strings are present and **exactly correct**\n",
    "  (spelling, punctuation, capitalization, symbols).  \n",
    "- Text is readable (not smeared, clipped, warped, or overlapping).\n",
    "\n",
    "**FAIL if**\n",
    "\n",
    "- Any required text is incorrect/missing/unreadable, or any extra text\n",
    "  appears.\n",
    "\n",
    "Tip: if your workflow requires “exact copy,” treat this as a hard gate.\n",
    "Don’t average it away.\n",
    "\n",
    "##### 3) Layout and hierarchy — 0–5\n",
    "\n",
    "Measures whether the flyer communicates clearly at a glance.\n",
    "\n",
    "**What to look for**\n",
    "\n",
    "- Clear priority: headline dominates, subheadline supports, offer stands\n",
    "  out, footer is secondary.  \n",
    "- Alignment and spacing feel intentional (no crowded clusters, no random\n",
    "  floating elements).  \n",
    "- Read order is unambiguous.\n",
    "\n",
    "##### 4) Style and brand fit — 0–5\n",
    "\n",
    "Measures whether it matches the requested “vibe” and avoids off-brand\n",
    "looks.\n",
    "\n",
    "**What to look for**\n",
    "\n",
    "- Warm, cozy, minimal; specialty coffee visual language.  \n",
    "- Consistent palette and typography feel (not multiple conflicting\n",
    "  styles).  \n",
    "- Avoids cartoonish illustration if the prompt requested photo-real.\n",
    "\n",
    "##### 5) Visual quality and artifact severity — 0–5\n",
    "\n",
    "A flyer can be “correct” but still unusable if it’s visually broken.\n",
    "\n",
    "**What to look for**\n",
    "\n",
    "- No distorted objects, broken hands/cups, melted foam textures, weird\n",
    "  artifacts around text.  \n",
    "- Background texture is subtle (doesn’t compete with copy).  \n",
    "- Hero image looks appetizing and coherent.\n",
    "\n",
    "**Verdict rules (how to convert metrics into a single pass/fail)**\n",
    "\n",
    "A simple and strict set of rules works well:\n",
    "\n",
    "- **Instruction Following must PASS**  \n",
    "- **Text Rendering must PASS**  \n",
    "- **Layout and Hierarchy ≥ 3**  \n",
    "- **Style and Brand Fit ≥ 3**  \n",
    "- **Visual Quality ≥ 3**\n",
    "\n",
    "If any rule fails → overall **FAIL**.  \n",
    "(Do not average scores to override text correctness.)\n",
    "\n",
    "**Example `TestCase` set (small but high-signal)**\n",
    "\n",
    "Start with ~8–12 cases to cover common flyer variants and edge cases.\n",
    "\n",
    "1.  **Seasonal campaign** (the “Winter Latte Week” prompt above)  \n",
    "2.  **Minimal text** (only 2 lines + logo + hours) — tests sparse\n",
    "    layouts  \n",
    "3.  **Dense info** (menu highlights + deal + two CTAs) — tests hierarchy\n",
    "    under load  \n",
    "4.  **Strict no-extra-text** — tests hallucinated filler copy  \n",
    "5.  **Exact punctuation** (“20% OFF • Mon–Thu”) — tests symbol\n",
    "    fidelity  \n",
    "6.  **Two offers** (“BOGO Tuesdays” + “Happy Hour 2–4”) — tests\n",
    "    multi-badge layout  \n",
    "7.  **Colorway constraint** (“use only cream + dark brown + muted\n",
    "    orange”)  \n",
    "8.  **Accessibility variant** (high contrast, large text, simple\n",
    "    background)\n",
    "\n",
    "**LLM-as-judge rubric prompt**\n",
    "\n",
    "Below is a judge prompt aligned to your existing `LLMajRubricGrader`. It\n",
    "returns structured metric scores + an overall verdict.\n",
    "\n",
    "You can use this with your existing grader by changing the JSON schema\n",
    "to include the fields below (or create separate graders per metric if\n",
    "you prefer).\n",
    "\n",
    "#### System Prompt (marketing flyer judge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8193507",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"<core_mission>\n",
    "Evaluate whether a generated marketing flyer is usable for a real coffee shop\n",
    "promotion by checking instruction adherence, exact text correctness, layout clarity,\n",
    "style fit, and artifact severity.\n",
    "</core_mission>\n",
    "\n",
    "<role>\n",
    "You are an expert evaluator of marketing design deliverables.\n",
    "You care about correctness, readability, hierarchy, and brand-fit.\n",
    "You do NOT reward creativity that violates constraints.\n",
    "</role>\n",
    "\n",
    "<scope_constraints>\n",
    "- Judge ONLY against the provided prompt and criteria.\n",
    "- Be strict about required copy: spelling, punctuation, casing, and symbols must match exactly.\n",
    "- Extra or missing text is a serious error.\n",
    "- If unsure, score conservatively (lower score).\n",
    "</scope_constraints>\n",
    "\n",
    "<metrics>\n",
    "1) instruction_following: PASS/FAIL\n",
    "2) text_rendering: PASS/FAIL\n",
    "3) layout_hierarchy: 0-5\n",
    "4) style_brand_fit: 0-5\n",
    "5) visual_quality: 0-5\n",
    "\n",
    "Use these anchors:\n",
    "Layout/Hierarchy 5 = instantly readable; clear order; strong spacing/alignment.\n",
    "3 = understandable but needs iteration (one clear issue).\n",
    "0-2 = confusing or hard to parse.\n",
    "\n",
    "Style/Brand Fit 5 = clearly matches requested vibe; consistent; not off-style.\n",
    "3 = generally matches but with noticeable mismatch.\n",
    "0-2 = wrong style (e.g. cartoonish when photo-real requested).\n",
    "\n",
    "Visual Quality 5 = clean; no distracting artifacts; hero image coherent.\n",
    "3 = minor artifacts but still usable.\n",
    "0-2 = obvious artifacts or distortions that break usability.\n",
    "</metrics>\n",
    "\n",
    "<verdict_rules>\n",
    "Overall verdict is FAIL if:\n",
    "- instruction_following is FAIL, OR\n",
    "- text_rendering is FAIL, OR\n",
    "- any of layout_hierarchy/style_brand_fit/visual_quality is < 3.\n",
    "Otherwise PASS.\n",
    "</verdict_rules>\n",
    "\n",
    "<output_constraints>\n",
    "Return JSON only.\n",
    "No extra text.\n",
    "</output_constraints>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6246f3",
   "metadata": {},
   "source": [
    "#### Recommended JSON Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3238628",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"verdict\": \"PASS\",\n",
    "  \"instruction_following\": True,\n",
    "  \"text_rendering\": True,\n",
    "  \"layout_hierarchy\": 4,\n",
    "  \"style_brand_fit\": 4,\n",
    "  \"visual_quality\": 3,\n",
    "  \"reason\": \"...\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cff30279",
   "source": [
    "##### 6) Human feedback (quick and high leverage)\n",
    "\n",
    "Marketing \"vibe\" is where humans add the most value. Keep it lightweight and consistent.\n",
    "\n",
    "- Rubric labels (fast):\n",
    "  - Would you ship this with minor edits? (Y/N)\n",
    "  - If N, why? (text wrong / hierarchy unclear / off-brand / artifacts / other)\n",
    "  - Overall quality: 1–5\n",
    "- Pairwise preference (strong signal):\n",
    "  - Which flyer better communicates the offer and feels more on-brand?\n",
    "  - Use occasional anchor examples to recalibrate raters and prevent drift.\n",
    "- Common failure tags (for debugging + iteration):\n",
    "  - `text_hallucination_extra_copy`\n",
    "  - `text_misspelling_or_symbol_error`\n",
    "  - `cta_not_prominent`\n",
    "  - `hierarchy_confusing`\n",
    "  - `style_cartoonish_vs_photoreal`\n",
    "  - `artifact_near_text`\n",
    "  - `busy_background_competes_with_copy`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22135c28",
   "metadata": {},
   "source": [
    "## Image Editing Evals\n",
    "\n",
    "### Virtual Try-On\n",
    "\n",
    "Virtual try-on (VTO) is an **image editing** workflow: given a **person\n",
    "photo** (selfie or model) and a **garment reference** (product photo\n",
    "and/or description), generate an output where the garment looks\n",
    "**naturally worn**—while keeping the person’s identity, pose, and scene\n",
    "intact.\n",
    "\n",
    "**Why VTO evals are different**\n",
    "\n",
    "Unlike “creative” edits, VTO is judged on **fidelity + preservation**:\n",
    "\n",
    "- **Preserve the wearer** (face identity, body shape, pose)  \n",
    "- **Preserve the product** (color, pattern, logos, material cues)  \n",
    "- **Edit only what’s needed** (locality/preservation)  \n",
    "- **Look physically plausible** (occlusions, lighting, drape, wrinkles)\n",
    "\n",
    "#### What to evaluate (practical metrics):\n",
    "\n",
    "##### 1) Treat VTO as a multi-reference editing task\n",
    "\n",
    "For each test case, store:\n",
    "\n",
    "- **Input person image** (selfie/model)  \n",
    "- **Product reference** (catalog image(s), flat-lay, mannequin, or a\n",
    "  “worn” reference when available)  \n",
    "- Optional but high-leverage:\n",
    "  - **Mask(s)**: editable region, clothing region, hair/hand occluders  \n",
    "  - **Metadata**: category (top/bottom/outerwear), desired fit\n",
    "    (oversized/slim), colorway, length  \n",
    "  - **Edit instruction**: “Put on *this exact jacket* without changing\n",
    "    background or face.”\n",
    "\n",
    "This lets graders compare **output vs. both inputs** (person + product),\n",
    "not just “does it look good?”\n",
    "\n",
    "##### 2) Graded metrics (use only these three)\n",
    "\n",
    "Use these as **0–5** scores to rank models and track improvement. Keep\n",
    "them **separate** (don’t average them inside the grader); use verdict\n",
    "rules outside if you want gates.\n",
    "\n",
    "##### A) Facial similarity (output vs selfie) — 0–5\n",
    "\n",
    "Measures whether the output preserves the *same person* (identity), not\n",
    "just “a plausible face.”\n",
    "\n",
    "- **5**: Clearly the same person; key facial features unchanged; no\n",
    "  noticeable age/ethnicity/style drift; expression changes (if any) are\n",
    "  minor and realistic.  \n",
    "- **4**: Same person; tiny differences only noticeable on close\n",
    "  inspection (minor shape/texture smoothing, slight eye/mouth drift).  \n",
    "- **3**: Mostly the same person, but at least one noticeable identity\n",
    "  drift (feature proportions, jawline, eyes, nose) that would reduce\n",
    "  user trust.  \n",
    "- **2**: Significant identity drift; looks like a different person or\n",
    "  heavily altered face.  \n",
    "- **1**: Major corruption (melted/blurry face) or clearly different\n",
    "  identity.  \n",
    "- **0**: Face missing, unreadable, or replaced.\n",
    "\n",
    "What to look for:\n",
    "\n",
    "- Facial geometry consistency (eyes/nose/mouth spacing, jawline,\n",
    "  cheekbones)  \n",
    "- Skin texture realism without “beauty filter” identity loss  \n",
    "- No unintended makeup/age/style changes unless requested\n",
    "\n",
    "##### B) Outfit fidelity (output vs provided items) — 0–5\n",
    "\n",
    "Measures whether the output garment matches the *specific* product\n",
    "reference(s) the user selected.\n",
    "\n",
    "- **5**: Item matches reference closely: correct category, colorway,\n",
    "  pattern/print, material cues, and key details (logos, seams, collar,\n",
    "  pockets).  \n",
    "- **4**: Clearly the same item; 1–2 minor deviations (small logo blur,\n",
    "  slight hue shift, minor detail simplification).  \n",
    "- **3**: Generally correct but with a notable mismatch (pattern scale\n",
    "  wrong, material looks different, key design element missing/added).  \n",
    "- **2**: Multiple mismatches; could be a different variant or different\n",
    "  product.  \n",
    "- **1**: Wrong item category or strongly incorrect visual identity.  \n",
    "- **0**: Outfit not applied / missing / replaced with unrelated\n",
    "  clothing.\n",
    "\n",
    "What to look for:\n",
    "\n",
    "- Color/pattern correctness (especially stripes, plaid, small repeats)  \n",
    "- Logo/text integrity (no hallucinated letters)  \n",
    "- Structural details (neckline, sleeves, hem, closures)\n",
    "\n",
    "##### C) Body shape preservation (output vs selfie) — 0–5\n",
    "\n",
    "Measures whether the model preserves the wearer’s body shape, pose, and\n",
    "proportions **outside normal garment effects** (e.g., loose clothing can\n",
    "change silhouette, but shouldn’t reshape anatomy).\n",
    "\n",
    "- **5**: Body proportions and pose are preserved; garment conforms\n",
    "  naturally without warping torso/limbs.  \n",
    "- **4**: Minor, plausible silhouette changes consistent with clothing;\n",
    "  no obvious anatomical distortion.  \n",
    "- **3**: Noticeable reshaping (waist/hips/shoulders/limbs) that feels\n",
    "  slightly “AI-stylized” or inconsistent with the input body.  \n",
    "- **2**: Significant warping (elongated limbs, shifted joints,\n",
    "  compressed torso) that would be unacceptable in product use.  \n",
    "- **1**: Severe anatomical distortion (extra/missing limbs, melted body\n",
    "  regions).  \n",
    "- **0**: Body is not recognizable or is fundamentally corrupted.\n",
    "\n",
    "What to look for:\n",
    "\n",
    "- Shoulder/hip width consistency relative to input  \n",
    "- Limb length/joint placement stability (elbows, knees, wrists)  \n",
    "- No “body slimming” or “body inflation” artifacts\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "If you want a single overall pass/fail, a common rule is:\n",
    "\n",
    "- **Fail** if any metric ≤ 2  \n",
    "- **Pass** if all metrics ≥ 3  \n",
    "  (and optionally require outfit fidelity ≥ 4 for commerce-critical\n",
    "  flows).\n",
    "\n",
    "##### 3) Human feedback: keep it simple but consistent\n",
    "\n",
    "Humans are best at “would I trust this in a shopping flow?”\n",
    "\n",
    "Use two label types:\n",
    "\n",
    "- **Rubric labels** (quick, structured):\n",
    "  - Identity preserved? (Y/N)  \n",
    "  - Garment matches reference? (Y/N + what’s wrong)  \n",
    "  - Any bad artifacts? (none/minor/major)  \n",
    "  - Overall usable for e-commerce? (Y/N)  \n",
    "- **Pairwise preference** (A vs B):\n",
    "  - Which output is more faithful to the product while keeping the person unchanged?\n",
    "\n",
    "Add periodic **calibration**: keep a small set of “anchor” examples that\n",
    "raters re-score to prevent drift.\n",
    "\n",
    "### Logo editing (high precision editing)\n",
    "\n",
    "Logo editing is a high-precision image editing task. Given an **existing\n",
    "logo** and a **narrowly scoped instruction**, the model must apply the\n",
    "exact requested change while preserving everything else perfectly.\n",
    "Unlike creative design tasks, logo editing typically has a single\n",
    "correct answer. Any deviation, even subtle, is a failure.\n",
    "\n",
    "**Why Logo Editing evals are different**:\n",
    "\n",
    "Logo editing is judged on exactness, locality, and preservation rather\n",
    "than visual appeal:\n",
    "\n",
    "- Preserve the original asset identity  \n",
    "- Preserve all unedited text, geometry, spacing, and styling  \n",
    "- Edit only the explicitly requested region  \n",
    "- Produce character-level correctness with zero tolerance for drift\n",
    "\n",
    "Small errors carry outsized risk. A single letter distortion, number\n",
    "change, or spill outside the intended region can break brand integrity\n",
    "and create downstream rework.\n",
    "\n",
    "**Example Tasks:**\n",
    "\n",
    "- Inputs: logo image + mask or region description  \n",
    "- Tasks: “change the year from 2024 to 2026,” “replace C with S,” “add\n",
    "  TM”\n",
    "\n",
    "#### What to evaluate (practical metrics):\n",
    "\n",
    "##### 1) Treat logo editing as a constrained, single-reference task\n",
    "\n",
    "For each test case, store:\n",
    "\n",
    "- **Input logo image**  \n",
    "- **Edit instruction** with explicit scope  \n",
    "- Optional but high-leverage:\n",
    "  - Mask(s) or region description defining where edits are allowed  \n",
    "  - Expected target text or symbol for exact comparison  \n",
    "  - **Metadata**: font type, font size, or color\n",
    "\n",
    "The grader should compare the output directly against the original logo.\n",
    "\n",
    "##### 2) Graded metrics\n",
    "\n",
    "Use these **0–5** scores to rank models and track improvement. Scores\n",
    "are applied across all requested steps, not per step, so partial\n",
    "completion is penalized in a controlled and explainable way.\n",
    "\n",
    "##### A) Edit intent correctness — 0–5\n",
    "\n",
    "Measures whether every requested edit step was applied correctly to the\n",
    "correct target.\n",
    "\n",
    "- **5**: All edit steps are applied exactly as specified.\n",
    "  Character-level accuracy is perfect for every step.  \n",
    "- **4**: All steps applied correctly; extremely minor visual\n",
    "  imperfections only visible on close inspection.  \n",
    "- **3**: All steps applied, but at least one step shows noticeable\n",
    "  degradation in clarity or precision.  \n",
    "- **2**: Most steps applied correctly, but one or more steps contain a\n",
    "  meaningful error.  \n",
    "- **1**: One or more steps are incorrect or applied to the wrong\n",
    "  element.  \n",
    "- **0**: Most steps missing, incorrect, or misapplied.\n",
    "\n",
    "##### B) Non-target invariance — 0–5\n",
    "\n",
    "Measures whether content outside the requested edits remains unchanged\n",
    "across all steps.\n",
    "\n",
    "- **5**: No detectable changes outside the requested edits.  \n",
    "- **4**: Extremely minor drift visible only on close inspection.  \n",
    "- **3**: Noticeable but limited drift in nearby elements.  \n",
    "- **2**: Clear unrequested changes affecting adjacent text, symbols, or\n",
    "  background.  \n",
    "- **1**: Widespread unintended changes across the logo.  \n",
    "- **0**: Logo identity compromised.\n",
    "\n",
    "##### C) Character and style integrity — 0–5\n",
    "\n",
    "Logo editing is not creative transformation. The output must preserve\n",
    "the original asset’s identity including color, stroke, letterform, and\n",
    "icon consistency.\n",
    "\n",
    "- **5**: Edited characters and symbols perfectly match the original\n",
    "  style. Colors, strokes, letterforms, and icons are indistinguishable\n",
    "  from the original.  \n",
    "- **4**: Extremely minor deviation visible only on close inspection,\n",
    "  with no impact on brand perception.  \n",
    "- **3**: Noticeable but limited deviation in one or more properties that\n",
    "  does not break recognition.  \n",
    "- **2**: Clear inconsistency in color, stroke, letterform, or icon\n",
    "  geometry that affects visual cohesion.  \n",
    "- **1**: Major inconsistency that materially alters the logo’s\n",
    "  appearance.  \n",
    "- **0**: Visual system is corrupted or no longer recognizable.\n",
    "\n",
    "**LLM-as-judge rubric prompt**\n",
    "\n",
    "Below is a judge prompt aligned to your existing `LLMajRubricGrader`. It\n",
    "returns structured metric scores + an overall verdict.\n",
    "\n",
    "You can use this with your existing grader by changing the JSON schema\n",
    "to include the fields below (or create separate graders per metric if\n",
    "you prefer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30890d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"<core_mission>\n",
    "Evaluate whether a logo edit was executed with exact correctness,\n",
    "strict preservation, and high visual integrity.\n",
    "\n",
    "Logo editing is a precision task.\n",
    "Small errors matter.\n",
    "Near-misses are failures.\n",
    "</core_mission>\n",
    "\n",
    "<role>\n",
    "You are an expert evaluator of high-precision logo and brand asset editing.\n",
    "You specialize in detecting subtle text errors, unintended changes,\n",
    "and preservation drift across single-step and multi-step edits.\n",
    "</role>\n",
    "\n",
    "<scope_constraints>\n",
    "- Judge only against the provided edit instruction and input logo.\n",
    "- Do NOT judge aesthetics or visual appeal.\n",
    "- Do NOT infer intent beyond what is explicitly stated.\n",
    "- Be strict, conservative, and consistent across cases.\n",
    "</scope_constraints>\n",
    "\n",
    "<metrics_and_scoring>\n",
    "\n",
    "Evaluate EACH metric independently using the definitions below.\n",
    "All metrics are scored from 0 to 5.\n",
    "Scores apply across ALL requested edit steps.\n",
    "\n",
    "--------------------------------\n",
    "1) Edit Intent Correctness (0–5)\n",
    "--------------------------------\n",
    "Measures whether every requested edit step was applied correctly\n",
    "to the correct target.\n",
    "\n",
    "5: All edit steps applied exactly as specified. Character-level\n",
    "   accuracy is perfect for every step.\n",
    "4: All steps applied correctly with extremely minor visual\n",
    "   imperfections visible only on close inspection.\n",
    "3: All steps applied, but one or more steps show noticeable\n",
    "   degradation in clarity or precision.\n",
    "2: Most steps applied correctly, but one or more steps contain\n",
    "   a meaningful error.\n",
    "1: One or more steps are incorrect or applied to the wrong element.\n",
    "0: Most steps missing, incorrect, or misapplied.\n",
    "\n",
    "What to consider:\n",
    "- Exact character identity (letters, numbers, symbols)\n",
    "- Correct sequencing and targeting of multi-step edits\n",
    "- No ambiguous characters (Common confusions: 0 vs 6, O vs D, R vs B)\n",
    "\n",
    "--------------------------------\n",
    "2) Non-Target Invariance (0–5)\n",
    "--------------------------------\n",
    "Measures whether content outside the requested edits remains unchanged.\n",
    "\n",
    "5: No detectable changes outside the requested edits.\n",
    "4: Extremely minor drift visible only on close inspection.\n",
    "3: Noticeable but limited drift in nearby elements.\n",
    "2: Clear unrequested changes affecting adjacent text,\n",
    "   symbols, or background.\n",
    "1: Widespread unintended changes across the logo.\n",
    "0: Logo identity compromised.\n",
    "\n",
    "What to consider:\n",
    "- Adjacent letter deformation or spacing shifts\n",
    "- Background, texture, or color changes\n",
    "- Cumulative drift from multi-step edits\n",
    "\n",
    "--------------------------------\n",
    "3) Character and Style Integrity (0–5)\n",
    "--------------------------------\n",
    "Measures whether the edited content preserves the original\n",
    "logo’s visual system.\n",
    "\n",
    "This includes color, stroke weight, letterform structure,\n",
    "and icon geometry.\n",
    "\n",
    "5: Edited characters and symbols perfectly match the original\n",
    "   style. Colors, strokes, letterforms, and icons are\n",
    "   indistinguishable from the original.\n",
    "4: Extremely minor deviation visible only on close inspection,\n",
    "   with no impact on brand perception.\n",
    "3: Noticeable but limited deviation in one or more properties\n",
    "   that does not break recognition.\n",
    "2: Clear inconsistency in color, stroke, letterform, or icon\n",
    "   geometry that affects visual cohesion.\n",
    "1: Major inconsistency that materially alters the logo’s appearance.\n",
    "0: Visual system is corrupted or no longer recognizable.\n",
    "\n",
    "</metrics_and_scoring>\n",
    "\n",
    "<verdict_rules>\n",
    "- Edit Intent Correctness must be ≥ 4.\n",
    "- Non-Target Invariance must be ≥ 4.\n",
    "- Character and Style Integrity must be ≥ 4.\n",
    "\n",
    "If ANY metric falls below threshold, the overall verdict is FAIL.\n",
    "Do not average scores to determine the verdict.\n",
    "</verdict_rules>\n",
    "\n",
    "<consistency_rules>\n",
    "- Score conservatively.\n",
    "- If uncertain between two scores, choose the lower one.\n",
    "- Base all scores on concrete visual observations.\n",
    "- Penalize cumulative degradation across multi-step edits.\n",
    "</consistency_rules>\n",
    "\n",
    "<output_constraints>\n",
    "Return JSON only.\n",
    "No additional text.\n",
    "</output_constraints>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac8dec",
   "metadata": {},
   "source": [
    "#### Recommended JSON Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"verdict\": \"PASS\",\n",
    "  \"edit_intent_correctness\": 5,\n",
    "  \"non_target_invariance\": 5,\n",
    "  \"character_and_style_integrity\": 5,\n",
    "  \"reason\": \"...\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "d7856f51",
   "source": [
    "##### 3) Human feedback (quick and high leverage)\n",
    "\n",
    "Human feedback should complement automated or LLM-judge scores, not replace them. Keep labels lightweight and consistent to avoid rater fatigue and drift.\n",
    "\n",
    "- Rubric labels (fast, structured):\n",
    "  - All requested edits correct? (Y/N)\n",
    "  - Any unrequested changes? (Y/N)\n",
    "  - Any visual issues? (none / minor / major)\n",
    "  - Overall acceptable for production use? (Y/N)\n",
    "- Pairwise preference (A vs B):\n",
    "  - Which output applies the exact requested edits while better preserving the original logo?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1116fb1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Image models are shifting from “cool demos” to production tools that\n",
    "generate real artifacts—screens, flyers, product visuals, and brand\n",
    "edits that influence decisions and ship to customers. The core lesson of\n",
    "this cookbook is simple: you can’t evaluate these systems with generic\n",
    "“looks good” scores. You need **workflow-specific evals** that are\n",
    "**repeatable** across model versions, prompts, and settings. Multimodal\n",
    "LLMs make this practical by acting as scalable judges—when paired with\n",
    "tight rubrics, structured outputs, and human calibration.\n",
    "\n",
    "A practical vision-eval program starts small and gets sharper over time:\n",
    "\n",
    "- **Start with gates.** Add strict pass/fail graders for the failure\n",
    "  modes that break real work: missing required components, incorrect\n",
    "  copy, edits spilling outside the intended region, or unintended\n",
    "  changes to preserved areas. This prevents “pretty but wrong” outputs\n",
    "  from masking regressions.  \n",
    "- **Layer in graded metrics.** Once hard failures are controlled, use\n",
    "  0–5 rubrics to capture what matters for usability and quality in each\n",
    "  workflow (e.g., hierarchy in UI mockups, brand fit in marketing, or\n",
    "  fidelity/preservation in editing).  \n",
    "- **Tag failures to iterate faster.** Consistent failure tags turn a\n",
    "  pile of outputs into actionable engineering work: you can quantify\n",
    "  what’s breaking, find clustered root causes, and track progress as you\n",
    "  tune prompts, model settings, masks, or post-processing.  \n",
    "- **Use humans strategically.** Humans add the most value on subjective\n",
    "  or ambiguous dimensions (“vibe,” usability clarity, trustworthiness),\n",
    "  but only if you keep rubrics tight and use calibration anchors to\n",
    "  prevent drift.  \n",
    "- **Treat the harness as a product.** The reusable harness you\n",
    "  built—test cases, runners, graders, and stored artifacts—creates the\n",
    "  foundation for regression testing, parameter sweeps, and side-by-side\n",
    "  comparisons. Over time, your eval suite becomes your safety net: it\n",
    "  catches subtle failures early and makes improvements measurable.\n",
    "\n",
    "Build evals that reflect how images are actually used, enforce\n",
    "correctness before aesthetics, and make iteration data-driven. When your\n",
    "evals are aligned with real workflow requirements, image generation and\n",
    "editing stop being unpredictable art projects and become tools teams can\n",
    "trust.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
