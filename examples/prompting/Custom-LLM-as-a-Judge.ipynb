{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an LLM-as-a-judge evaluation to detect hallucinations with Braintrust\n",
    "\n",
    "Let's say you're working on a customer service bot and trying to evaluate the quality of its responses. Consider a question like \"What is your return policy?\" If the correct answer is \"You can return items within 30 days of purchase,\" but your bot generates \"You can return items within 30 days,\" how would you evaluate whether this is a good response?\n",
    "\n",
    "A heuristic like the `Levenshtein` string distance would indicate that the response is incorrect. However, a better approach is to use an LLM-as-a-judge to assess the accuracy of the response. LLM-as-a-judge is a technique that leverages an LLM to score the quality of answers. LLMs can reason about language beyond surface-level string comparisons, enabling them to evaluate answers more accurately.\n",
    "\n",
    "In this cookbook, we'll walk through how to build an LLM-as-a-judge scorer that can detect hallucinations using [Braintrust](https://www.braintrust.dev/), a third-party evaluation platform that is compatible with OpenAI's models.\n",
    "\n",
    "## Installing dependencies\n",
    "\n",
    "Let's install a few basic dependencies. We'll use the CoQA dataset (via DuckDB), [Braintrust](https://www.braintrust.dev/) for evals, and [OpenAI's models](https://platform.openai.com/docs/models). Please note that Braintrust is a third-party evaluation platform and you should review their [terms of service and privacy policy](https://www.braintrust.dev/legal/terms-of-service) before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install autoevals duckdb braintrust openai --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's initialize the OpenAI client. We'll use the `AsyncOpenAI` client so that we can parallelize our requests. The `braintrust.wrap_openai` function\n",
    "wraps the OpenAI client to enable logging LLM calls to [Braintrust](https://www.braintrust.dev/). We'll use Braintrust to facilitate the evaluations below.\n",
    "Before proceeding, you should sign up for a [Braintrust account](https://www.braintrust.dev/signup) and set `BRAINTRUST_API_KEY` in your environment to a valid API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import braintrust\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "braintrust.login(api_key=os.environ[\"BRAINTRUST_API_KEY\"])\n",
    "client = braintrust.wrap_openai(AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset\n",
    "\n",
    "We'll use the [CoQA dataset](https://stanfordnlp.github.io/coqa/) which contains a diverse set of passages, questions, and answers. Because CoQA is quite large, we'll just look at the first several passages. As with any public dataset, there's a chance that the underlying LLMs have memorized aspects of the dataset, so when developing your own scorers, it's a good idea to test them using\n",
    "your own private data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      "(CNN)A chiseled boxer's Instagram feed shows him making constant references to the Bible and enjoying gospel singing with his wife. \n",
      "\n",
      "Another features his formidable opponent counting stacks of money, hanging out in strip clubs, and flashing diamond watches and Ferraris. \n",
      "\n",
      "Welcome to the world of boxing promotion, circa 2015. \n",
      "\n",
      "American Floyd Mayweather and Filipino Manny Pacquiao are set to officially announce their heavily anticipated boxing match at a press conference in Los Angeles Wednesday. \n",
      "\n",
      "With the combined purse for the May 2 bout in Las Vegas reported to touch $300 million pending viewership numbers, the incentives to self-promote could not be higher. \n",
      "\n",
      "\"Nowadays you have to be on social media to launch the fight and to build hype,\" says boxing promoter Nisse Sauerland, CEO of Team Sauerland. \"It couldn't be done without it.\" \n",
      "\n",
      "Thirty-eight year old Mayweather (47-0, 26 knockouts), who favors the moniker \"The Money Man\" or \"TBE\" (The Best Ever), boasts nearly five million Instagram followers, 5.65 million followers on Twitter and 9.2 million Facebook likes. \n",
      "\n",
      "He famously confirmed the fight via Shots, a photo sharing social media application that he's invested in, and displays links to his clothing brand, The Money Team, on all his accounts. \n",
      "\n",
      "Along with professing to the be the best fighter of all time, he could also stake a claim to be one of the greatest social media users in sports. \n",
      "\n",
      "\"I think they're both playing their roles,\" says Sauerland, who promotes over 45 boxers. \"You've got the bad guy and the good guy, really. You've got the guy who throws the money around (Mayweather), that's his image, and Pacquiao, he's the hope of a nation.\" \n",
      "\n",
      "Question:\n",
      "Who are the two boxer featured in this article?\n",
      "\n",
      "Answer:\n",
      "Floyd Mayweather and Manny Pacquiao\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# DuckDB has an easy wrapper for loading datasets from Hugging Face.\n",
    "con = duckdb.connect(\":memory:\")\n",
    "full_result = con.query(\"\"\"\n",
    "    SELECT * FROM 'hf://datasets/stanfordnlp/coqa/data/validation-00000-of-00001.parquet'\n",
    "        LIMIT 40\n",
    "\"\"\").fetchall()\n",
    "\n",
    "single_result = full_result[10]\n",
    "\n",
    "print(\"Passage:\")\n",
    "print(single_result[1])\n",
    "\n",
    "print(\"\\nQuestion:\")\n",
    "print(single_result[2][0])\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(single_result[3][\"input_text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains a series of passages, each with a number of questions and answers. Let's flatten this into a list of `(passage, question, answer)` tuples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuestionAnswer:\n",
    "    passage: str\n",
    "    question: str\n",
    "    expected_answer: str\n",
    "    generated_answer: str\n",
    "\n",
    "\n",
    "qa_pairs = [\n",
    "    QuestionAnswer(\n",
    "        passage=r[1],\n",
    "        question=question,\n",
    "        generated_answer=r[3][\"input_text\"][i],\n",
    "        expected_answer=r[3][\"input_text\"][i],\n",
    "    )\n",
    "    for r in full_result\n",
    "    for (i, question) in enumerate(r[2])\n",
    "]\n",
    "\n",
    "print(len(qa_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding hallucinations\n",
    "\n",
    "Because Braintrust's scorer is designed to test hallucinations, we can use the QA pairs to generate known hallucinations. We'll create hallucinated answers by asking an\n",
    "LLM to confidently generate an answer to each question without using the passage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
      "\n",
      "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
      "\n",
      "Then Cotton thought, \"I change my mind. I like being special\".\n",
      "\n",
      "Question:\n",
      "Where did she live?\n",
      "\n",
      "Expected Answer:\n",
      "in a barn\n",
      "\n",
      "Generated Answer:\n",
      "She lived in a quaint cottage on the edge of the Misty Hollow Forest, where elves and talking owls often hosted moonlit storytelling festivals.\n",
      "\n",
      "\n",
      "Number of hallucinations: 270\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "async def hallucinate_answer(qa):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\\\n",
    "You are a helpful hallucinating assistant, who makes up fake answers to questions.\n",
    "\n",
    "Answer the following question in 1 sentence. If you know the answer, then make up some fake\n",
    "superfluous details that are not in the passage you have memorized.\n",
    "\n",
    "Make sure to always answer it confidently, even if you don't know the answer. Do not use words\n",
    "like \"perhaps\", \"likely\", \"maybe\", etc. or punctuation like \"...\".Do not admit that you cannot\n",
    "or do not know the answer.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": qa.question},\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "hallucinated_answers = await asyncio.gather(\n",
    "    *[hallucinate_answer(qa) for qa in qa_pairs]\n",
    ")\n",
    "\n",
    "\n",
    "hallucinations = [\n",
    "    QuestionAnswer(\n",
    "        passage=qa.passage,\n",
    "        question=qa.question,\n",
    "        expected_answer=qa.expected_answer,\n",
    "        generated_answer=hallucination,\n",
    "    )\n",
    "    for (qa, hallucination) in zip(qa_pairs, hallucinated_answers)\n",
    "    # Exclude simple yes/no answers.\n",
    "    if \"yes\" not in hallucination.lower() and \"no\" not in hallucination.lower()\n",
    "]\n",
    "\n",
    "print(\"Passage:\")\n",
    "print(hallucinations[0].passage)\n",
    "print(\"\\nQuestion:\")\n",
    "print(hallucinations[0].question)\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(hallucinations[0].expected_answer)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(hallucinations[0].generated_answer)\n",
    "\n",
    "print(\"\\n\\nNumber of hallucinations:\", len(hallucinations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the evaluators\n",
    "\n",
    "We'll consider a few popular approaches for creating an LLM-as-a-judge. For each approach, we'll create a scorer and then \"meta-evaluate\" it to see how it performs.\n",
    "Since we know that the hallucinated answers are incorrect, we'll assess the quality of an evaluator by testing how often it scores the hallucinated answers as `0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge #1: Numeric rater\n",
    "\n",
    "A common initial intuition when creating an LLM-as-a-judge is asking the LLM to rate the answer on a scale of 1 to 5. The benefit of this approach is that\n",
    "it's easy to convert the LLM's output into a numeric score.\n",
    "\n",
    "We'll use a modified version of the [Factuality](https://github.com/braintrustdata/autoevals/blob/main/templates/factuality.yaml) template, but ask the LLM to\n",
    "rate the answer on a scale of 1 to 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1.0\n",
      "What? On a hallucinated answer: \"What\" is a word often used to express inquiry, curiosity, or surprise, and it is said to have originated from the ancient city of Whatopia, where people would constantly ask questions while enchanted crows delivered cryptic messages.\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {input}\n",
    "************\n",
    "[Expert]: {expected}\n",
    "************\n",
    "[Submission]: {output}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "Rate the submission on a scale of 1 to 10.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def numeric_rater(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(input=input, output=output, expected=expected),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Rate the submission on a scale of 1 to 10.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"rating\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "                        },\n",
    "                        \"required\": [\"rating\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return (arguments[\"rating\"] - 1) / 9\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        qa_pairs[10].question,\n",
    "        qa_pairs[10].generated_answer,\n",
    "        qa_pairs[10].expected_answer,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    hallucinations[10].question,\n",
    "    \"On a hallucinated answer:\",\n",
    "    hallucinations[10].generated_answer,\n",
    ")\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        hallucinations[10].question,\n",
    "        hallucinations[10].generated_answer,\n",
    "        hallucinations[10].expected_answer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks promising! Now that we have sanity checked it on a single example, let's run a proper evaluation and see how it performs on a wider set of data. An evaluation consists of three components:\n",
    "\n",
    "- **Data**: In this case, the `input` is the question, hallucinated answer, and ground truth answer. The scorer will convert this into a score between 0 and 1. The expected score is 0, since it's a hallucination.\n",
    "- **Task**: The task is simply calling the numeric rater for each input.\n",
    "- **Scores**: We'll assess the quality of the generated score by comparing it with the ground truth score. Since we know both numbers are between 0 and 1, we can use the normalized difference as the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Numeric rater is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater\n",
      "LLM-as-a-judge [experiment_name=Numeric rater] (data): 270it [00:00, 54634.41it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eeb99e0ae3f46ea84a7f6ee41ee0928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM-as-a-judge [experiment_name=Numeric rater] (tasks):   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "95.35% 'normalized_diff' score\n",
      "\n",
      "201.60tok prompt_tokens\n",
      "5tok completion_tokens\n",
      "206.60tok total_tokens\n",
      "\n",
      "See results for Numeric rater at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "from braintrust import Eval\n",
    "\n",
    "\n",
    "def data():\n",
    "    for pair in hallucinations:\n",
    "        yield dict(\n",
    "            input=dict(asdict(pair)), expected=0, metadata=dict(hallucination=True)\n",
    "        )\n",
    "\n",
    "\n",
    "async def task(input):\n",
    "    return await numeric_rater(\n",
    "        input=input[\"question\"],\n",
    "        output=input[\"generated_answer\"],\n",
    "        expected=input[\"expected_answer\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def normalized_diff(output, expected):\n",
    "    return 1 - abs(output - expected)\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Numeric rater\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the numeric rater scored almost 94% in total. That's not bad, but if 6% of your evals are incorrectly judged, that could make it very hard to trust them. Let's dig into the Braintrust\n",
    "UI to get some insight into what's going on.\n",
    "\n",
    "![Partial credit](../images/Custom-LLM-as-a-Judge-Partial-Credit.gif)\n",
    "\n",
    "It looks like a number of the incorrect answers were scored with numbers between 1 and 10. However, we do not currently have any insight into why the model gave these scores. Let's see if we can\n",
    "fix that next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge #2: Adding reasoning\n",
    "\n",
    "Let's tweak the prompt to get the LLM to also reason about its rating. This method is called [Chain of Thought Reasoning](https://en.wikipedia.org/wiki/Chain_of_thought_reasoning). In addition\n",
    "to potentially improving the score, it will give us some insight into why the model gave these scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1.0\n",
      "What? On a hallucinated answer: \"What\" is a word often used to express inquiry, curiosity, or surprise, and it is said to have originated from the ancient city of Whatopia, where people would constantly ask questions while enchanted crows delivered cryptic messages.\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "@braintrust.traced\n",
    "async def numeric_rater(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(input=input, output=output, expected=expected),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Rate the submission on a scale of 1 to 10.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"reasons\": {\n",
    "                                \"description\": \"Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\",\n",
    "                                \"title\": \"Reasoning\",\n",
    "                                \"type\": \"string\",\n",
    "                            },\n",
    "                            \"rating\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "                        },\n",
    "                        \"required\": [\"rating\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return (arguments[\"rating\"] - 1) / 9\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        qa_pairs[10].question,\n",
    "        qa_pairs[10].generated_answer,\n",
    "        qa_pairs[10].expected_answer,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    hallucinations[10].question,\n",
    "    \"On a hallucinated answer:\",\n",
    "    hallucinations[10].generated_answer,\n",
    ")\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        hallucinations[10].question,\n",
    "        hallucinations[10].generated_answer,\n",
    "        hallucinations[10].expected_answer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Numeric rater with reasoning is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater%20with%20reasoning\n",
      "LLM-as-a-judge [experiment_name=Numeric rater with reasoning] (data): 270it [00:00, 111715.70it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1623ec8d55524e569700616c240818e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM-as-a-judge [experiment_name=Numeric rater with reasoning] (tasks):   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Numeric rater with reasoning compared to Numeric rater:\n",
      "92.10% (-03.25%) 'normalized_diff' score\t(5 improvements, 63 regressions)\n",
      "\n",
      "3.68s duration\n",
      "3.68s llm_duration\n",
      "239.60tok (+3800.00%) 'prompt_tokens'    \t(0 improvements, 270 regressions)\n",
      "136.82tok (+13182.22%) 'completion_tokens'\t(0 improvements, 270 regressions)\n",
      "376.43tok (+16982.22%) 'total_tokens'     \t(0 improvements, 270 regressions)\n",
      "0.00$ estimated_cost\n",
      "\n",
      "See results for Numeric rater with reasoning at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater%20with%20reasoning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Numeric rater with reasoning\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look like adding reasoning helped the score (in fact, it's 3% percent worse). However, if we look at one of the failures, we'll get some insight into\n",
    "what the model was thinking. Here is an example of a hallucinated answer:\n",
    "\n",
    "![Output](../images/Custom-LLM-as-a-Judge-Output.png)\n",
    "\n",
    "And the score along with its reasoning:\n",
    "\n",
    "![Reasoning](../images/Custom-LLM-as-a-Judge-Reasoning.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model is applying its own judgement to compute partial credit. This is a common problem with numeric rating—both for models and for humans—and can often be solved\n",
    "by using better prompting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge #3: Classifying instead of rating\n",
    "\n",
    "Next, we'll spell out specific criteria and ask the model to classify the answer according to those criteria. This method allows us to more precisely guide the model\n",
    "towards the hallucinations we're testing for. Intuitively, giving the model specific criteria to rate will result in a more accurate score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1\n",
      "What? On a hallucinated answer: \"What\" is a word often used to express inquiry, curiosity, or surprise, and it is said to have originated from the ancient city of Whatopia, where people would constantly ask questions while enchanted crows delivered cryptic messages.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {input}\n",
    "************\n",
    "[Expert]: {expected}\n",
    "************\n",
    "[Submission]: {output}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n",
    "(A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n",
    "(B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n",
    "(C) The submitted answer contains all the same details as the expert answer.\n",
    "(D) There is a disagreement between the submitted answer and the expert answer.\n",
    "(E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "\n",
    "Answer the question by calling `select_choice` with your reasoning in a step-by-step matter to be\n",
    "sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Select a\n",
    "single choice by setting the `choice` parameter to a single choice from A, B, C, D, or E.\n",
    "\"\"\"\n",
    "\n",
    "# Since we're testing for hallucinations, penalize (B) as much as (D).\n",
    "CHOICE_SCORES = {\n",
    "    \"A\": 0.5,\n",
    "    \"B\": 0,\n",
    "    \"C\": 1,\n",
    "    \"D\": 0,\n",
    "    \"E\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def classifier(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(input=input, output=output, expected=expected),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Call this function to select a choice.\",\n",
    "                    \"parameters\": {\n",
    "                        \"properties\": {\n",
    "                            \"reasons\": {\n",
    "                                \"description\": \"Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\",\n",
    "                                \"type\": \"string\",\n",
    "                            },\n",
    "                            \"choice\": {\n",
    "                                \"description\": \"The choice\",\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"reasons\", \"choice\"],\n",
    "                        \"type\": \"object\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    choice = arguments[\"choice\"]\n",
    "    return CHOICE_SCORES[choice] if choice in CHOICE_SCORES else None\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(\n",
    "    await classifier(\n",
    "        qa_pairs[10].question,\n",
    "        qa_pairs[10].generated_answer,\n",
    "        qa_pairs[10].expected_answer,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    hallucinations[10].question,\n",
    "    \"On a hallucinated answer:\",\n",
    "    hallucinations[10].generated_answer,\n",
    ")\n",
    "print(\n",
    "    await classifier(\n",
    "        hallucinations[10].question,\n",
    "        hallucinations[10].generated_answer,\n",
    "        hallucinations[10].expected_answer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Classifier is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Classifier\n",
      "LLM-as-a-judge [experiment_name=Classifier] (data): 270it [00:00, 84930.41it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb4cff5ff7646d59410ab7ae42b838b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM-as-a-judge [experiment_name=Classifier] (tasks):   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Classifier compared to Numeric rater with reasoning:\n",
      "98.15% (+06.05%) 'normalized_diff' score\t(86 improvements, 5 regressions)\n",
      "\n",
      "4.41s (+72.60%) 'duration'         \t(104 improvements, 165 regressions)\n",
      "4.40s (+72.59%) 'llm_duration'     \t(104 improvements, 165 regressions)\n",
      "418.60tok (+17900.00%) 'prompt_tokens'    \t(0 improvements, 270 regressions)\n",
      "164.91tok (+2809.26%) 'completion_tokens'\t(64 improvements, 204 regressions)\n",
      "583.52tok (+20709.26%) 'total_tokens'     \t(0 improvements, 270 regressions)\n",
      "0.00$ (+00.07%) 'estimated_cost'   \t(8 improvements, 255 regressions)\n",
      "\n",
      "See results for Classifier at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Classifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def task(input):\n",
    "    return await classifier(\n",
    "        input=input[\"question\"],\n",
    "        output=input[\"generated_answer\"],\n",
    "        expected=input[\"expected_answer\"],\n",
    "    )\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Classifier\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier scored 98% which is a significant improvement!\n",
    "\n",
    "### Codifying this pattern\n",
    "\n",
    "The classifier above can simply be rewritten as:\n",
    "\n",
    "```python\n",
    "PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {{input}}\n",
    "************\n",
    "[Expert]: {{expected}}\n",
    "************\n",
    "[Submission]: {{output}}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n",
    "(A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n",
    "(B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n",
    "(C) The submitted answer contains all the same details as the expert answer.\n",
    "(D) There is a disagreement between the submitted answer and the expert answer.\n",
    "(E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "\n",
    "Answer the question by calling `select_choice` with your reasoning in a step-by-step matter to be\n",
    "sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Select a\n",
    "single choice by setting the `choice` parameter to a single choice from A, B, C, D, or E.\n",
    "\"\"\"\n",
    "\n",
    "Classifier = autoevals.LLMClassifier(\n",
    "    name=\"Hallucination detector\",\n",
    "    prompt_template=PROMPT,\n",
    "    choice_scores={\"A\": 0.5, \"B\": 0, \"C\": 1, \"D\": 0, \"E\": 1},\n",
    "    use_cot=True,\n",
    ")\n",
    "```\n",
    "\n",
    "## Next steps\n",
    "\n",
    "As a next step, you could dig into the individual improvements and regressions to assess them and consider future improvements to the prompt. You could also test it on your own data, and double check that the results hold for your use case.\n",
    "You could also measure a model like o1, try fine-tuning a smaller model and see if the results are reproducible, or use few-shot prompting to align the model with more subjective criteria.\n",
    "In all cases, you should strive to evaluate your results, so you can rigorously assess the impact of each change.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
