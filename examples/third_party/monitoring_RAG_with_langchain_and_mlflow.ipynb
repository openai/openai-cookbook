{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montioring a RAG chain with OpenAI, LangChain, and MLflow\n",
    "\n",
    "This notebook is a quick tutorial on how to use [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) to improve observability in your Retrieval Augmented Generation (RAG) application. RAG chains can be complex with many steps involved, and when failures or unexpected responses happen, it can be difficult to pinpoint what exactly went wrong.\n",
    "\n",
    "MLflow Tracing helps by allowing you to view the inputs and outputs of each intermediate step in your workflow, which enables more effective debugging and iteration. More than that, MLflow has an [integration with Jupyter Notebooks](https://mlflow.org/docs/latest/llms/tracing/index.html#jupyter-notebook-integration) that allows the trace UI to show up inside notebook output cells, allowing you to browse the trace without having to context switch.\n",
    "\n",
    "To demonstrate these functionalities, we'll be building a simple question answering app. For convenience, we're using [LangChain](https://www.langchain.com/) here (as MLflow has a built-in integration with it), but traces can be instrumented manually to suit any use-case. Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "The following cells simply set up our dev environment, installing the necessary libraries, and setting our OpenAI key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f22b58-df54-4edf-828b-9b5a73bc2c59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -Uqq mlflow langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def774e0-018a-45a5-b946-b1716b34748c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the MLflow Tracking Server (optional but recommended)\n",
    "\n",
    "The [MLflow Tracking Server](https://mlflow.org/docs/latest/tracking/server.html) is a simple HTTP server that provides a web UI and several REST endpoints for tracking runs and experiments. For the purposes of this tutorial, it enables us to view a visualization of our trace, making it easy to browse through the data.\n",
    "\n",
    "You can run the cell below to start the server, but it might be more convenient (for terminating / cleaning up the process) to run `mlflow server` in an external shell in the directory that contains this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen\n",
    "\n",
    "p = Popen([\"mlflow\", \"server\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to terminate the server, you can simply restart the kernel that the notebook is attached to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling tracing\n",
    "\n",
    "When using LangChain, tracing is enabled automatically by simply calling `mlflow.langchain.autolog()`. Manual trace instrumentation is also possible via the `@mlflow.trace()` function decorator. For more details, please check out the [MLflow Docs](https://mlflow.org/docs/latest/llms/tracing/index.html#trace-decorator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this cell if not running the tracking server\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c06d675c-7f12-4fda-aabf-80deed153f9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/04 14:48:19 INFO mlflow.tracking.fluent: Experiment with name 'openai-rag-demo' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"openai-rag-demo\")\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our vector store\n",
    "\n",
    "The first step when building a RAG app is embedding our documents. Here, we'll be using OpenAI's `text-embedding-3-small` model to generate the embeddings, and storing them in an in-memory vector store. We'll be querying this vector store later to fetch documents that are relevant to the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c189e01-aaab-4e68-baeb-84b8a99b9b9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"NumPy is a powerful Python library used for numerical computing. It provides support for large multidimensional arrays and matrices along with a collection of high-level mathematical functions.\"\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Pandas is a Python library primarily used for data manipulation and analysis. It provides data structures like DataFrame, which makes working with structured data intuitive and efficient.\"\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"PyTorch is an open-source machine learning library. It is widely used for deep learning applications and provides dynamic computational graphs for flexibility.\"\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\"\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Langchain is an open-source library designed to build applications powered by language models. It provides a flexible interface to chain together components like prompts, memory, and tools.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Provide the `persist_directory` argument to save the vector store to disk\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents,\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our chain\n",
    "\n",
    "Our full RAG chain has the following steps:\n",
    "\n",
    "1. Embed the question and fetch relevant documents\n",
    "2. Construct a prompt using the fetched documents\n",
    "3. Feed the prompt to the chat model (in this case, we'll be using OpenAI's `gpt-4o-mini`)\n",
    "\n",
    "The following cell sets each of these steps up, and links them together to produce the final chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29626f0c-2b1d-4511-b8c6-3851fafb0574",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# This handles Step #1. The retriever will be invoked using the user's question,\n",
    "# and will perform a similarity search over the embedded documents to retrieve\n",
    "# the top 3 most relevant ones.\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# This handles Step #2. Placeholders are used to inject\n",
    "# both the user's question and the retrieved context.\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question using the provided context. If the information\n",
    "required to answer the question is not contained within the context, simply\n",
    "respond that you do not know.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\")\n",
    "\n",
    "# This handles Step #3. Feel free to change the\n",
    "# parameters of the modelto whatever you wish!\n",
    "chat_model = ChatOpenAI(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  temperature=0,\n",
    "  max_tokens=50,\n",
    ")\n",
    "\n",
    "# Finally, we link them all together using LangChain Expression Language's pipe operator \n",
    "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | chat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the chain is executed (i.e. when you run the cell below), MLflow automatically traces the execution of each of the chain's constituent parts. If you're using a tracking server, you should see a UI appear in the cell output.\n",
    "\n",
    "This visualization allows you to easily step through the various intermediate steps in the chain's execution, and to see the inputs and outputs at a glance. The UI also supports searching through spans to make it easy to debug unexpected responses even in very large and complex applications.\n",
    "\n",
    "![Trace UI in Jupyter Notebook](../../images/mlflow_trace_jupyter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65551fad-e8e7-4f43-86fa-6b78ef672a96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 240, 'total_tokens': 273, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-330c09ea-413a-43cc-a6e3-c1eb6e73abdf-0', usage_metadata={'input_tokens': 240, 'output_tokens': 33, 'total_tokens': 273, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we can simply invoke the chain with our question,\n",
    "# and the full pipeline will be executed!\n",
    "chain.invoke(\"What is MLflow?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the trace in the MLflow Web UI\n",
    "\n",
    "Alternatively, if you'd like to browse historical traces, you can do so via the MLflow Web UI (located at the URI where your tracking server is running--by default, this should be http://127.0.0.1:5000).\n",
    "\n",
    "In this tutorial, we've created the \"openai-rag-demo\" experiment, and this is where the trace data is stored. Navigate to the \"Traces\" tab within this experiment:\n",
    "\n",
    "![MLflow Web UI](../../images/mlflow_ui.png)\n",
    "\n",
    "Clicking on the links in the \"Request ID\" column will open the trace visualization, which is identical to the one that appears in the notebook:\n",
    "\n",
    "![MLflow Trace UI](../../images/mlflow_trace_ui.png)\n",
    "\n",
    "Of course, the trace data is accessible via the python client as well. Simply call `mlflow.get_last_active_trace()`, and call `.to_dict()` on the result to convert it to a python dictionary. The `mlflow.search_traces()` and `mlflow.get_trace()` [APIs](https://mlflow.org/docs/latest/llms/tracing/index.html#searching-and-retrieving-traces) are also available to retrieve traces given certain filter conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0a95ced-fbaa-4c87-9f39-039b5875a16d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'info': {'request_id': '89f68d8ff9e245a58708294482b154b8',\n",
       "  'experiment_id': '423469103627487276',\n",
       "  'timestamp_ms': 1738651707198,\n",
       "  'execution_time_ms': 2662,\n",
       "  'status': 'OK',\n",
       "  'request_metadata': {'mlflow.trace_schema.version': '2',\n",
       "   'mlflow.traceInputs': '\"What is MLflow?\"',\n",
       "   'mlflow.traceOutputs': '{\"content\": \"MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\", \"additional_kwargs\": {\"refusal\": null}, \"response...'},\n",
       "  'tags': {'mlflow.source.name': '/Users/daniel.lok/miniconda3/envs/dev/lib/python3.9/site-packages/ipykernel_launcher.py',\n",
       "   'mlflow.source.type': 'LOCAL',\n",
       "   'mlflow.traceName': 'RunnableSequence',\n",
       "   'mlflow.artifactLocation': 'file:///Users/daniel.lok/openai-cookbook/examples/third_party/mlruns/423469103627487276/traces/89f68d8ff9e245a58708294482b154b8/artifacts'}},\n",
       " 'data': {'spans': [{'name': 'RunnableSequence',\n",
       "    'context': {'span_id': '0x0314d41e4a778840',\n",
       "     'trace_id': '0xee15383ca752e6158a4dcd961b904a17'},\n",
       "    'parent_id': None,\n",
       "    'start_time': 1738651707198371000,\n",
       "    'end_time': 1738651709860461000,\n",
       "    'status_code': 'OK',\n",
       "    'status_message': '',\n",
       "    'attributes': {'mlflow.traceRequestId': '\"89f68d8ff9e245a58708294482b154b8\"',\n",
       "     'mlflow.spanType': '\"CHAIN\"',\n",
       "     'mlflow.spanInputs': '\"What is MLflow?\"',\n",
       "     'mlflow.spanOutputs': '{\"content\": \"MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\", \"additional_kwargs\": {\"refusal\": null}, \"response_metadata\": {\"token_usage\": {\"completion_tokens\": 33, \"prompt_tokens\": 240, \"total_tokens\": 273, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}, \"model_name\": \"gpt-4o-mini-2024-07-18\", \"system_fingerprint\": \"fp_72ed7ab54c\", \"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"ai\", \"name\": null, \"id\": \"run-330c09ea-413a-43cc-a6e3-c1eb6e73abdf-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 240, \"output_tokens\": 33, \"total_tokens\": 273, \"input_token_details\": {\"audio\": 0, \"cache_read\": 0}, \"output_token_details\": {\"audio\": 0, \"reasoning\": 0}}}'},\n",
       "    'events': []},\n",
       "   {'name': 'RunnableParallel<context,question>',\n",
       "    'context': {'span_id': '0x35e16a5a0fb3be02',\n",
       "     'trace_id': '0xee15383ca752e6158a4dcd961b904a17'},\n",
       "    'parent_id': '0x0314d41e4a778840',\n",
       "    'start_time': 1738651707198912000,\n",
       "    'end_time': 1738651708262334000,\n",
       "    'status_code': 'OK',\n",
       "    'status_message': '',\n",
       "    'attributes': {'mlflow.traceRequestId': '\"89f68d8ff9e245a58708294482b154b8\"',\n",
       "     'mlflow.spanType': '\"CHAIN\"',\n",
       "     'mlflow.spanInputs': '\"What is MLflow?\"',\n",
       "     'mlflow.spanOutputs': '{\"context\": [{\"id\": \"6c84dc48-765c-4b6d-b8ad-7377b2592408\", \"metadata\": {}, \"page_content\": \"MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\", \"type\": \"Document\"}, {\"id\": \"be0898a8-a750-42a3-95a2-05e2d32d502b\", \"metadata\": {}, \"page_content\": \"Langchain is an open-source library designed to build applications powered by language models. It provides a flexible interface to chain together components like prompts, memory, and tools.\", \"type\": \"Document\"}, {\"id\": \"ac0edd58-1b98-46c4-8b08-b2312620fb27\", \"metadata\": {}, \"page_content\": \"PyTorch is an open-source machine learning library. It is widely used for deep learning applications and provides dynamic computational graphs for flexibility.\", \"type\": \"Document\"}], \"question\": \"What is MLflow?\"}'},\n",
       "    'events': []},\n",
       "   {'name': 'VectorStoreRetriever',\n",
       "    'context': {'span_id': '0x7876847d7717e727',\n",
       "     'trace_id': '0xee15383ca752e6158a4dcd961b904a17'},\n",
       "    'parent_id': '0x35e16a5a0fb3be02',\n",
       "    'start_time': 1738651707199507000,\n",
       "    'end_time': 1738651708261209000,\n",
       "    'status_code': 'OK',\n",
       "    'status_message': '',\n",
       "    'attributes': {'mlflow.traceRequestId': '\"89f68d8ff9e245a58708294482b154b8\"',\n",
       "     'mlflow.spanType': '\"RETRIEVER\"',\n",
       "     'metadata': '{\"ls_retriever_name\": \"vectorstore\", \"ls_vector_store_provider\": \"InMemoryVectorStore\", \"ls_embedding_provider\": \"OpenAIEmbeddings\"}',\n",
       "     'mlflow.spanInputs': '\"What is MLflow?\"',\n",
       "     'mlflow.spanOutputs': '[{\"page_content\": \"MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\", \"metadata\": {}, \"id\": \"6c84dc48-765c-4b6d-b8ad-7377b2592408\"}, {\"page_content\": \"Langchain is an open-source library designed to build applications powered by language models. It provides a flexible interface to chain together components like prompts, memory, and tools.\", \"metadata\": {}, \"id\": \"be0898a8-a750-42a3-95a2-05e2d32d502b\"}, {\"page_content\": \"PyTorch is an open-source machine learning library. It is widely used for deep learning applications and provides dynamic computational graphs for flexibility.\", \"metadata\": {}, \"id\": \"ac0edd58-1b98-46c4-8b08-b2312620fb27\"}]'},\n",
       "    'events': []},\n",
       "   {'name': 'RunnablePassthrough',\n",
       "    'context': {'span_id': '0x8877bf0b77c56962',\n",
       "     'trace_id': '0xee15383ca752e6158a4dcd961b904a17'},\n",
       "    'parent_id': '0x35e16a5a0fb3be02',\n",
       "    'start_time': 1738651707200834000,\n",
       "    'end_time': 1738651707200956000,\n",
       "    'status_code': 'OK',\n",
       "    'status_message': '',\n",
       "    'attributes': {'mlflow.traceRequestId': '\"89f68d8ff9e245a58708294482b154b8\"',\n",
       "     'mlflow.spanType': '\"CHAIN\"',\n",
       "     'mlflow.spanInputs': '\"What is MLflow?\"',\n",
       "     'mlflow.spanOutputs': '\"What is MLflow?\"'},\n",
       "    'events': []},\n",
       "   {'name': 'ChatPromptTemplate',\n",
       "    'context': {'span_id': '0x6301538139f373fe',\n",
       "     'trace_id': '0xee15383ca752e6158a4dcd961b904a17'},\n",
       "    'parent_id': '0x0314d41e4a778840',\n",
       "    'start_time': 1738651708264205000,\n",
       "    'end_time': 1738651708265458000,\n",
       "    'status_code': 'OK',\n",
       "    'status_message': '',\n",
       "    'attributes': {'mlflow.traceRequestId': '\"89f68d8ff9e245a58708294482b154b8\"',\n",
       "     'mlflow.spanType': '\"CHAIN\"',\n",
       "     'mlflow.spanInputs': '{\"context\": [{\"id\": \"6c84dc48-765c-4b6d-b8ad-7377b2592408\", \"metadata\": {}, \"page_content\": \"MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\", \"type\": \"Document\"}, {\"id\": \"be0898a8-a750-42a3-95a2-05e2d32d502b\", \"metadata\": {}, \"page_content\": \"Langchain is an open-source library designed to build applications powered by language models. It provides a flexible interface to chain together components like prompts, memory, and tools.\", \"type\": \"Document\"}, {\"id\": \"ac0edd58-1b98-46c4-8b08-b2312620fb27\", \"metadata\": {}, \"page_content\": \"PyTorch is an open-source machine learning library. It is widely used for deep learning applications and provides dynamic computational graphs for flexibility.\", \"type\": \"Document\"}], \"question\": \"What is MLflow?\"}',\n",
       "     'mlflow.spanOutputs': '{\"messages\": [{\"content\": \"\\\\nAnswer the following question using the provided context. If the information\\\\nrequired to answer the question is not contained within the context, simply\\\\nrespond that you do not know.\\\\n\\\\nWhat is MLflow?\\\\n\\\\nContext:\\\\n[Document(id=\\'6c84dc48-765c-4b6d-b8ad-7377b2592408\\', metadata={}, page_content=\\'MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\\'), Document(id=\\'be0898a8-a750-42a3-95a2-05e2d32d502b\\', metadata={}, page_content=\\'Langchain is an open-source library designed to build applications powered by language models. It provides a flexible interface to chain together components like prompts, memory, and tools.\\'), Document(id=\\'ac0edd58-1b98-46c4-8b08-b2312620fb27\\', metadata={}, page_content=\\'PyTorch is an open-source machine learning library. It is widely used for deep learning applications and provides dynamic computational graphs for flexibility.\\')]\\\\n\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null}]}'},\n",
       "    'events': []},\n",
       "   {'name': 'ChatOpenAI',\n",
       "    'context': {'span_id': '0x85c544fcffcbf4ff',\n",
       "     'trace_id': '0xee15383ca752e6158a4dcd961b904a17'},\n",
       "    'parent_id': '0x0314d41e4a778840',\n",
       "    'start_time': 1738651708266723000,\n",
       "    'end_time': 1738651709859846000,\n",
       "    'status_code': 'OK',\n",
       "    'status_message': '',\n",
       "    'attributes': {'mlflow.traceRequestId': '\"89f68d8ff9e245a58708294482b154b8\"',\n",
       "     'mlflow.spanType': '\"CHAT_MODEL\"',\n",
       "     'invocation_params': '{\"model\": \"gpt-4o-mini\", \"model_name\": \"gpt-4o-mini\", \"stream\": false, \"temperature\": 0.0, \"max_completion_tokens\": 50, \"_type\": \"openai-chat\", \"stop\": null}',\n",
       "     'options': '{\"stop\": null}',\n",
       "     'batch_size': '1',\n",
       "     'metadata': '{\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4o-mini\", \"ls_model_type\": \"chat\", \"ls_temperature\": 0.0, \"ls_max_tokens\": 50}',\n",
       "     'mlflow.spanInputs': '[[{\"content\": \"\\\\nAnswer the following question using the provided context. If the information\\\\nrequired to answer the question is not contained within the context, simply\\\\nrespond that you do not know.\\\\n\\\\nWhat is MLflow?\\\\n\\\\nContext:\\\\n[Document(id=\\'6c84dc48-765c-4b6d-b8ad-7377b2592408\\', metadata={}, page_content=\\'MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\\'), Document(id=\\'be0898a8-a750-42a3-95a2-05e2d32d502b\\', metadata={}, page_content=\\'Langchain is an open-source library designed to build applications powered by language models. It provides a flexible interface to chain together components like prompts, memory, and tools.\\'), Document(id=\\'ac0edd58-1b98-46c4-8b08-b2312620fb27\\', metadata={}, page_content=\\'PyTorch is an open-source machine learning library. It is widely used for deep learning applications and provides dynamic computational graphs for flexibility.\\')]\\\\n\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}]]',\n",
       "     'mlflow.chat.messages': '[{\"role\": \"user\", \"content\": \"\\\\nAnswer the following question using the provided context. If the information\\\\nrequired to answer the question is not contained within the context, simply\\\\nrespond that you do not know.\\\\n\\\\nWhat is MLflow?\\\\n\\\\nContext:\\\\n[Document(id=\\'6c84dc48-765c-4b6d-b8ad-7377b2592408\\', metadata={}, page_content=\\'MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\\'), Document(id=\\'be0898a8-a750-42a3-95a2-05e2d32d502b\\', metadata={}, page_content=\\'Langchain is an open-source library designed to build applications powered by language models. It provides a flexible interface to chain together components like prompts, memory, and tools.\\'), Document(id=\\'ac0edd58-1b98-46c4-8b08-b2312620fb27\\', metadata={}, page_content=\\'PyTorch is an open-source machine learning library. It is widely used for deep learning applications and provides dynamic computational graphs for flexibility.\\')]\\\\n\"}, {\"role\": \"assistant\", \"content\": \"MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\"}]',\n",
       "     'mlflow.spanOutputs': '{\"generations\": [[{\"text\": \"MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"ChatGeneration\", \"message\": {\"content\": \"MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\", \"additional_kwargs\": {\"refusal\": null}, \"response_metadata\": {\"token_usage\": {\"completion_tokens\": 33, \"prompt_tokens\": 240, \"total_tokens\": 273, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}, \"model_name\": \"gpt-4o-mini-2024-07-18\", \"system_fingerprint\": \"fp_72ed7ab54c\", \"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"ai\", \"name\": null, \"id\": \"run-330c09ea-413a-43cc-a6e3-c1eb6e73abdf-0\"}}]], \"llm_output\": {\"token_usage\": {\"completion_tokens\": 33, \"prompt_tokens\": 240, \"total_tokens\": 273, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}, \"model_name\": \"gpt-4o-mini-2024-07-18\", \"system_fingerprint\": \"fp_72ed7ab54c\"}, \"run\": null, \"type\": \"LLMResult\"}'},\n",
       "    'events': []}],\n",
       "  'request': '\"What is MLflow?\"',\n",
       "  'response': '{\"content\": \"MLflow is an open-source platform to manage the machine learning lifecycle. It supports tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\", \"additional_kwargs\": {\"refusal\": null}, \"response_metadata\": {\"token_usage\": {\"completion_tokens\": 33, \"prompt_tokens\": 240, \"total_tokens\": 273, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}, \"model_name\": \"gpt-4o-mini-2024-07-18\", \"system_fingerprint\": \"fp_72ed7ab54c\", \"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"ai\", \"name\": null, \"id\": \"run-330c09ea-413a-43cc-a6e3-c1eb6e73abdf-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 240, \"output_tokens\": 33, \"total_tokens\": 273, \"input_token_details\": {\"audio\": 0, \"cache_read\": 0}, \"output_token_details\": {\"audio\": 0, \"reasoning\": 0}}}'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_last_active_trace().to_dict()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "openai cookbook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
