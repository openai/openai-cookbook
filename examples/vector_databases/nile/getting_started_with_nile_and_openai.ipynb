{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Nile as a vector store with OpenAI\n",
    "\n",
    "This notebook will walk you through using Nile's Serverless Postgres as a vector store for embeddings generated by OpenAI. \n",
    "We'll show you how to:\n",
    "    - Create a database with a table to store your embeddings in Nile (using the built-in `pg_vector` extension)\n",
    "    - Load pre-computed embeddings, generated by OpenAI, to Nile\n",
    "    - Generate embeddings for a text query\n",
    "    - Search the stored embeddings with vector distance functions to find documents relevant to the text query\n",
    "\n",
    "## What is Nile?\n",
    "\n",
    "Nile is a Postgres platform specifically designed for AI-Native B2B companies, enabling to launch and scale quickly, securely, and in the most cost-effective manner.\n",
    "\n",
    "Nile's architecture provides the following key benefits:\n",
    "\n",
    "- Built-in tenant virtualization in Postgres for better data and vector embedding isolation.\n",
    "- Vector embedding store that is cost-effective and auto-scales to billions of embeddings across customers\n",
    "- Seamlessly autoscales as your customer's usage increases and scales to zero with no cold start time\n",
    "- User management built for multitenancy with user data stored in your Postgres database and unlimited active tenants and users.\n",
    "\n",
    "You can read more about Nile, its architecture and concepts in our [documentation](https://www.thenile.dev/docs/getting-started)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting this tutorial, you'll need to have the following:\n",
    "\n",
    "- Connection string to a Nile Postgres database. You can create one by signing up to [Nile](https://console.thenile.dev). After you create a database, generate credentials and copy the connection string from \"Settings\" page.\n",
    "- OpenAI API Key. If you don't have an OpenAI API key, you can get one from [https://beta.openai.com/account/api-keys](https://beta.openai.com/account/api-keys).\n",
    "\n",
    "After you have both strings, set them as environment variables. Note that if you are running this notebook locally, you'll need to reload the terminal and the notebook to pick up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! export OPENAI_API_KEY=\"sk-xxxxxxxxxxxxxxxxxxxxx\"\n",
    "! export DATABASE_URL=postgres://nile:password@db.thenile.dev:5432/nile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements\n",
    "\n",
    "This notebook uses OpenAI's client to generate embeddings, and `psycopg2`, Python's popular Postgres client, to store and query the embeddings in Nile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (1.40.6)\n",
      "Requirement already satisfied: psycopg2-binary in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (2.9.9)\n",
      "Requirement already satisfied: pandas in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (2.0.1)\n",
      "Requirement already satisfied: wget in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (3.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gwen/workspaces/niledatabase/examples/quickstart/django/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai psycopg2-binary pandas numpy wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is ready\n"
     ]
    }
   ],
   "source": [
    "# Test that your OpenAI API key is correctly set as an environment variable\n",
    "# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\n",
    "import os\n",
    "\n",
    "# Note. alternatively you can set a temporary env variable like this:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    print(\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY environment variable not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Nile\n",
    "\n",
    "We start by connecting to our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfull connection\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "## If you are running this notebook locally, you will need to set the DATABASE_URL env variable, or set it here directly:\n",
    "## os.environ[\"DATABASE_URL\"] = \"postgres://user:password@localhost:5432/nile\"\n",
    "\n",
    "connection = psycopg2.connect(os.getenv(\"DATABASE_URL\"))\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT 'successfull connection';\")\n",
    "print(cursor.fetchone()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings\n",
    "\n",
    "We'll use a file with pre-generated embeddings of wikipedia articles prepared with OpenAI's `text-embedding-3-small` model. By using prepared embeddings, we'll save on the cost of generating the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "# the embedding file is 700MB, so it may take a while to download\n",
    "embeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n",
    "wget.download(embeddings_url)\n",
    "\n",
    "# unzip the file\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data/\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table for embedding data\n",
    "\n",
    "We'll need a table to store the wikipedia articles and their embeddings. Nile has both tenant-aware tables, in which each row belongs to one customer, access controls are applied and they automatically scales out as tenants are added, and shared tables, which every tenant can access. Both types of tables can have `vector` columns for embeddings. \n",
    "\n",
    "In this example, we'll create a tenant called `Simple Wiki` and store the data in a tenant-aware table. This model will let us load additional data from other Wikis in the future and isolate them to their own tenants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateTable",
     "evalue": "relation \"articles\" already exists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateTable\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m tenant_id \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchone()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# create the table\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m               CREATE TABLE articles (\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m                   tenant_id UUID NOT NULL, -- this makes the table tenant-aware\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m                   id integer,\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m                   url TEXT,\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m                   title TEXT,\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m                   content TEXT,\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m                   title_vector VECTOR(1536),\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m                   content_vector VECTOR(1536),\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m                   vector_id INTEGER,\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m                  PRIMARY KEY (tenant_id, id)\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m               );\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m connection\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[0;31mDuplicateTable\u001b[0m: relation \"articles\" already exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "connection = psycopg2.connect(os.getenv(\"DATABASE_URL\"))\n",
    "cursor = connection.cursor()\n",
    "# create tenant\n",
    "cursor.execute(\"INSERT INTO tenants (name) VALUES ('Simple Wiki') RETURNING id;\")\n",
    "tenant_id = cursor.fetchone()[0]\n",
    "\n",
    "# create the table\n",
    "cursor.execute(f\"\"\"\n",
    "               CREATE TABLE articles (\n",
    "                   tenant_id UUID NOT NULL, -- this makes the table tenant-aware\n",
    "                   id integer,\n",
    "                   url TEXT,\n",
    "                   title TEXT,\n",
    "                   content TEXT,\n",
    "                   title_vector VECTOR(1536),\n",
    "                   content_vector VECTOR(1536),\n",
    "                   vector_id INTEGER,\n",
    "                  PRIMARY KEY (tenant_id, id)\n",
    "               );\"\"\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to database\n",
    "\n",
    "Next, we'll use batch inserts to load the data to the table. First, we'll load the data into a Pandas data frame and from there's we'll insert it to the table.\n",
    "We are using a transaction\n",
    "This will take a few minutes, so a good time to grab coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000 rows\n",
      "Inserted 2000 rows\n",
      "Inserted 3000 rows\n",
      "Inserted 4000 rows\n",
      "Inserted 5000 rows\n",
      "Inserted 6000 rows\n",
      "Inserted 7000 rows\n",
      "Inserted 8000 rows\n",
      "Inserted 9000 rows\n",
      "Inserted 10000 rows\n",
      "Inserted 11000 rows\n",
      "Inserted 12000 rows\n",
      "Inserted 13000 rows\n",
      "Inserted 14000 rows\n",
      "Inserted 15000 rows\n",
      "Inserted 16000 rows\n",
      "Inserted 17000 rows\n",
      "Inserted 18000 rows\n",
      "Inserted 19000 rows\n",
      "Inserted 20000 rows\n",
      "Inserted 21000 rows\n",
      "Inserted 22000 rows\n",
      "Inserted 23000 rows\n",
      "Inserted 24000 rows\n",
      "Inserted 25000 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2.extras as extras\n",
    "\n",
    "# load the data\n",
    "data = pd.read_csv(\"data/vector_database_wikipedia_articles_embedded.csv\")\n",
    "data[\"tenant_id\"] = tenant_id # add column with tenant_id of the tenant we created\n",
    "data = data.rename(columns={'text': 'content'}) # rename column to match the table\n",
    "\n",
    "# insert the data in batches\n",
    "batch_size = 1000\n",
    "connection = psycopg2.connect(os.getenv(\"DATABASE_URL\"))\n",
    "cursor = connection.cursor()\n",
    "for i in range(0, len(data), batch_size):\n",
    "    batch = data.iloc[i:i+batch_size]\n",
    "    cols = ','.join(list(data.columns))\n",
    "    tuples = [tuple(x) for x in batch.to_numpy()] \n",
    "    query = \"INSERT INTO articles(%s) VALUES %%s\" % (cols)\n",
    "    try:\n",
    "        extras.execute_values(cursor, query, tuples)\n",
    "        print(f\"Inserted {i + len(batch)} rows\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        connection.rollback()\n",
    "        break\n",
    "# commit after all inserts are done\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search the data\n",
    "\n",
    "Now that we have the documents and embeddings, we can use this to search for wikipedia articles relevant to a topic. This is the fun part!\n",
    "\n",
    "We'll start by defining a function that given a text query will find top-N relevant documents. It starts by generating an embedding for the query using OpenAI. Since the embeddings were generated with `text-embedding-3-small` model, we must use the same model for searching. \n",
    "Then it we'll use SQL to find near-by vectors in the database. We'll use the familiar cosine distance method from `pg_vector`.\n",
    "\n",
    "Since the dataset has two embeddings - one for the title and one for the content of the articles, we'll also have a column_name parameter, which will let us choose which embedding to search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from psycopg2 import sql\n",
    "\n",
    "def find_similar_articles(query, vector_name, top_n=10):\n",
    "    # get the vector for the query\n",
    "    embedded_query = openai.embeddings.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-3-small\",\n",
    "    ).data[0].embedding\n",
    "    \n",
    "    formatted_embedding = \"'[\" + \",\".join(map(str, embedded_query)) + \"]'\"\n",
    "    # Connect to the virtual tenant database\n",
    "    connection = psycopg2.connect(os.environ['DATABASE_URL'])\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute('set nile.tenant_id = %s', (tenant_id,))\n",
    "    \n",
    "    sql_query = sql.SQL(\n",
    "          \"\"\"SELECT id, title, {vector_name} <=> {embedded_query}::VECTOR(1536) as cosine_distance \n",
    "            -- if you want to try another distance: embedding <-> $2 as euclidean_distance, (embedding <#> $2)* -1 as inner_product\n",
    "            FROM articles\n",
    "            ORDER BY ({vector_name} <=>  {embedded_query})\n",
    "            LIMIT ({top_n});\"\"\")\n",
    "\n",
    "    # find the most similar articles\n",
    "    try:\n",
    "        cursor.execute(sql_query.format(\n",
    "            vector_name=sql.Identifier(vector_name),\n",
    "            embedded_query=sql.SQL(formatted_embedding),\n",
    "            top_n=sql.Literal(top_n)))\n",
    "        return cursor.fetchall()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        connection.rollback()\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this method, we can start querying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. European Article Number (Score: 0.996)\n",
      "2. File extension (Score: 0.998)\n",
      "3. Bel√©m (Score: 1.0)\n",
      "4. Computer number format (Score: 1.0)\n",
      "5. Budai (Score: 1.001)\n",
      "6. 100 (number) (Score: 1.002)\n",
      "7. Creed (Score: 1.002)\n",
      "8. Carbon steel (Score: 1.003)\n",
      "9. Valencia (autonomous community) (Score: 1.004)\n",
      "10. Enlightenment (Score: 1.004)\n"
     ]
    }
   ],
   "source": [
    "query_results = find_similar_articles(\"Albert Einstein\", \"title_vector\")\n",
    "for i, article in enumerate(query_results):\n",
    "    print(f\"{i + 1}. {article[1]} (Score: {round(float(article[2]), 3)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Turbofolk (Score: 0.959)\n",
      "2. General Dynamics F-16 Fighting Falcon (Score: 0.96)\n",
      "3. Cessna 172 (Score: 0.961)\n",
      "4. Darin (singer) (Score: 0.961)\n",
      "5. North American F-86 Sabre (Score: 0.963)\n",
      "6. Boeing 767 (Score: 0.964)\n",
      "7. Bohemianism (Score: 0.964)\n",
      "8. Benny Goodman (Score: 0.964)\n",
      "9. Sublime (Score: 0.964)\n",
      "10. Eagles (band) (Score: 0.964)\n"
     ]
    }
   ],
   "source": [
    "query_results = find_similar_articles(\"Albert Einstein\", \"content_vector\")\n",
    "for i, article in enumerate(query_results):\n",
    "    print(f\"{i + 1}. {article[1]} (Score: {round(float(article[2]), 3)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
